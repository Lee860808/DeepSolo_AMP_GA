[04/29 01:37:56] detectron2 INFO: Rank of current process: 0. World size: 1
[04/29 01:37:56] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/kylee/DeepSolo_LangPriors/detectron2/detectron2
Compiler                         GCC 9.5
CUDA compiler                    CUDA 11.3
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.12.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 3060 (arch=8.6)
Driver version                   550.120
CUDA_HOME                        /usr
Pillow                           9.0.1
torchvision                      0.13.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[04/29 01:37:56] detectron2 INFO: Command line arguments: Namespace(config_file='configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50152', opts=[])
[04/29 01:37:56] detectron2 INFO: Contents of args.config_file=configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml:
_BASE_: "../Base_det.yaml"

MODEL:
  WEIGHTS: "output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth"
  TRANSFORMER:
    INFERENCE_TH_TEST: 0.3

DATASETS:
  TRAIN: ("ic15_train",)
  TEST: ("ic15_test",)

INPUT:
  MIN_SIZE_TRAIN: (640, 700, 760, 820, 880, 940, 1000)
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MAX_SIZE_TEST: 4000
  CROP:
    ENABLED: False
  ROTATE: False

SOLVER:
  AMP:
    ENABLED: True  # Enable Automatic Mixed Precision
  IMS_PER_BATCH: 2
  BASE_LR: 1e-5
  LR_BACKBONE: 1e-6
  WARMUP_ITERS: 250
  STEPS: (15000, 25000)  # no step
  MAX_ITER: 30000
  GAMMA: 0.1
  CHECKPOINT_PERIOD: 1000

TEST:
  EVAL_PERIOD: 1000
  # 1 - Generic, 2 - Weak, 3 - Strong (for icdar2015)
  LEXICON_TYPE: 3

OUTPUT_DIR: "output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15"

[04/29 01:37:56] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ic15_test
  TRAIN:
  - ic15_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    CROP_INSTANCE: false
    ENABLED: false
    SIZE:
    - 0.1
    - 0.1
    TYPE: relative_range
  FORMAT: RGB
  HFLIP_TRAIN: false
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 4000
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MIN_SIZE_TRAIN:
  - 640
  - 700
  - 760
  - 820
  - 880
  - 940
  - 1000
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  ROTATE: false
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    ANTI_ALIAS: false
    FREEZE_AT: 2
    NAME: build_resnet_backbone
  BASIS_MODULE:
    ANN_SET: coco
    COMMON_STRIDE: 8
    CONVS_DIM: 128
    IN_FEATURES:
    - p3
    - p4
    - p5
    LOSS_ON: false
    LOSS_WEIGHT: 0.3
    NAME: ProtoNet
    NORM: SyncBN
    NUM_BASES: 4
    NUM_CLASSES: 80
    NUM_CONVS: 3
  BATEXT:
    CANONICAL_SIZE: 96
    CONV_DIM: 256
    CUSTOM_DICT: ''
    IN_FEATURES:
    - p2
    - p3
    - p4
    NUM_CHARS: 25
    NUM_CONV: 2
    POOLER_RESOLUTION:
    - 8
    - 32
    POOLER_SCALES:
    - 0.25
    - 0.125
    - 0.0625
    RECOGNITION_LOSS: ctc
    RECOGNIZER: attn
    SAMPLING_RATIO: 1
    USE_AET: false
    USE_COORDCONV: false
    VOC_SIZE: 96
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LINGUISTIC_PRIOR:
    EMBED_MODEL_NAME: google/canine-c
    GENERIC_DICT_PATH: datasets/generic_90k_words.txt
    SOFT_TARGET_THRESHOLD: 0.85
    TEMPERATURE: 1.0
  LOAD_PROPOSALS: false
  MASK_ON: false
  META_ARCHITECTURE: TransformerPureDetector
  MOBILENET: false
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_INTERVAL: 1
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  SWIN:
    DROP_PATH_RATE: 0.2
    TYPE: tiny
  TOP_MODULE:
    DIM: 16
    NAME: conv
  TRANSFORMER:
    AUX_LOSS: true
    BOUNDARY_HEAD: true
    CUSTOM_DICT: ''
    DEC_LAYERS: 6
    DEC_N_POINTS: 4
    DIM_FEEDFORWARD: 1024
    DROPOUT: 0.0
    ENABLED: true
    ENC_LAYERS: 6
    ENC_N_POINTS: 4
    HIDDEN_DIM: 256
    INFERENCE_TH_TEST: 0.3
    LOSS:
      AUX_LOSS: true
      BEZIER_CLASS_WEIGHT: 1.0
      BEZIER_COORD_WEIGHT: 1.0
      BEZIER_SAMPLE_POINTS: 25
      BOUNDARY_WEIGHT: 0.5
      FOCAL_ALPHA: 0.25
      FOCAL_GAMMA: 2.0
      POINT_CLASS_WEIGHT: 1.0
      POINT_COORD_WEIGHT: 1.0
      POINT_TEXT_WEIGHT: 0.5
    NHEADS: 8
    NUM_FEATURE_LEVELS: 4
    NUM_POINTS: 25
    NUM_QUERIES: 100
    POSITION_EMBEDDING_SCALE: 6.283185307179586
    TEMPERATURE: 10000
    VOC_SIZE: 37
  ViTAEv2:
    DROP_PATH_RATE: 0.2
    TYPE: vitaev2_s
  WEIGHTS: output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth
OUTPUT_DIR: output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15
SEED: 42
SOLVER:
  AMP:
    ENABLED: true
  BASE_LR: 1.0e-05
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.1
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 2
  LR_BACKBONE: 1.0e-06
  LR_BACKBONE_NAMES:
  - backbone.0
  LR_LINEAR_PROJ_MULT: 1.0
  LR_LINEAR_PROJ_NAMES:
  - reference_points
  - sampling_offsets
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 30000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 15000
  - 25000
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 250
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  LEXICON_TYPE: 3
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/29 01:37:56] detectron2 INFO: Full config saved to output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/config.yaml
[04/29 01:38:00] d2.engine.defaults INFO: Model:
TransformerPureDetector(
  (detection_transformer): DETECTION_TRANSFORMER(
    (backbone): Joiner(
      (0): MaskedBackbone(
        (backbone): ResNet(
          (stem): BasicStem(
            (conv1): Conv2d(
              3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
          )
          (res2): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv1): Conv2d(
                64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
          )
          (res3): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv1): Conv2d(
                256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
          )
          (res4): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
              (conv1): Conv2d(
                512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (4): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (5): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
          )
          (res5): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
              (conv1): Conv2d(
                1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
          )
        )
      )
      (1): PositionalEncoding2D()
    )
    (point_embed): Embedding(2500, 256)
    (transformer): DeformableTransformer(
      (encoder): DeformableTransformerEncoder(
        (layers): ModuleList(
          (0): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (decoder): DeformableCompositeTransformerDecoder(
        (layers): ModuleList(
          (0): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ctrl_point_coord): ModuleList(
          (0): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (1): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (2): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (3): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (4): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (5): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
        )
      )
      (enc_output): Linear(in_features=256, out_features=256, bias=True)
      (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (bezier_coord_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=8, bias=True)
        )
      )
      (bezier_class_embed): Linear(in_features=256, out_features=1, bias=True)
    )
    (input_proj): ModuleList(
      (0): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (1): Sequential(
        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (2): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (3): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (bezier_proposal_coord): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=8, bias=True)
      )
    )
    (bezier_proposal_class): Linear(in_features=256, out_features=1, bias=True)
    (ctrl_point_coord): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
    )
    (ctrl_point_class): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (ctrl_point_text): ModuleList(
      (0): Linear(in_features=256, out_features=38, bias=True)
      (1): Linear(in_features=256, out_features=38, bias=True)
      (2): Linear(in_features=256, out_features=38, bias=True)
      (3): Linear(in_features=256, out_features=38, bias=True)
      (4): Linear(in_features=256, out_features=38, bias=True)
      (5): Linear(in_features=256, out_features=38, bias=True)
    )
    (boundary_offset): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (bezier_sampler): BezierSampler()
    (enc_matcher): BezierHungarianMatcher(
      (bezier_sampler): BezierSampler()
    )
    (dec_matcher): CtrlPointHungarianMatcher()
  )
)
[04/29 01:38:00] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 700, 760, 820, 880, 940, 1000), max_size=2000, sample_style='choice'), RandomFlip()]
[04/29 01:38:00] adet.data.dataset_mapper INFO: Rebuilding the augmentations. The previous augmentations will be overridden.
[04/29 01:38:00] adet.data.detection_utils INFO: Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 700, 760, 820, 880, 940, 1000), max_size=2000, sample_style='choice'), RandomContrast(intensity_min=0.3, intensity_max=1.7), RandomBrightness(intensity_min=0.3, intensity_max=1.7), RandomLighting(scale=1.1394267984578836), RandomSaturation(intensity_min=0.3, intensity_max=1.7)]
[04/29 01:38:00] adet.data.datasets.text INFO: Loaded 1000 images in COCO format from datasets/ic15/train_37voc.json
[04/29 01:38:01] d2.data.build INFO: Removed 0 images with no usable annotations. 979 images left.
[04/29 01:38:01] d2.data.build INFO: Distribution of instances among all 1 categories:
[36m|  category  | #instances   |
|:----------:|:-------------|
|    text    | 4441         |
|            |              |[0m
[04/29 01:38:01] d2.data.build INFO: Using training sampler TrainingSampler
[04/29 01:38:01] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[04/29 01:38:01] d2.data.common INFO: Serializing 979 elements to byte tensors and concatenating them all ...
[04/29 01:38:01] d2.data.common INFO: Serialized dataset takes 6.51 MiB
[04/29 01:38:01] d2.data.build INFO: Making batched data loader with batch_size=2
[04/29 01:38:01] d2.engine.defaults INFO: Model:
TransformerPureDetector(
  (detection_transformer): DETECTION_TRANSFORMER(
    (backbone): Joiner(
      (0): MaskedBackbone(
        (backbone): ResNet(
          (stem): BasicStem(
            (conv1): Conv2d(
              3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
          )
          (res2): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv1): Conv2d(
                64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
          )
          (res3): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv1): Conv2d(
                256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
          )
          (res4): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
              (conv1): Conv2d(
                512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (4): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (5): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
          )
          (res5): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
              (conv1): Conv2d(
                1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
          )
        )
      )
      (1): PositionalEncoding2D()
    )
    (point_embed): Embedding(2500, 256)
    (transformer): DeformableTransformer(
      (encoder): DeformableTransformerEncoder(
        (layers): ModuleList(
          (0): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (decoder): DeformableCompositeTransformerDecoder(
        (layers): ModuleList(
          (0): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ctrl_point_coord): ModuleList(
          (0): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (1): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (2): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (3): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (4): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (5): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
        )
      )
      (enc_output): Linear(in_features=256, out_features=256, bias=True)
      (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (bezier_coord_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=8, bias=True)
        )
      )
      (bezier_class_embed): Linear(in_features=256, out_features=1, bias=True)
    )
    (input_proj): ModuleList(
      (0): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (1): Sequential(
        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (2): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (3): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (bezier_proposal_coord): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=8, bias=True)
      )
    )
    (bezier_proposal_class): Linear(in_features=256, out_features=1, bias=True)
    (ctrl_point_coord): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
    )
    (ctrl_point_class): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (ctrl_point_text): ModuleList(
      (0): Linear(in_features=256, out_features=38, bias=True)
      (1): Linear(in_features=256, out_features=38, bias=True)
      (2): Linear(in_features=256, out_features=38, bias=True)
      (3): Linear(in_features=256, out_features=38, bias=True)
      (4): Linear(in_features=256, out_features=38, bias=True)
      (5): Linear(in_features=256, out_features=38, bias=True)
    )
    (boundary_offset): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (bezier_sampler): BezierSampler()
    (enc_matcher): BezierHungarianMatcher(
      (bezier_sampler): BezierSampler()
    )
    (dec_matcher): CtrlPointHungarianMatcher()
  )
)
[04/29 01:38:01] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 700, 760, 820, 880, 940, 1000), max_size=2000, sample_style='choice'), RandomFlip()]
[04/29 01:38:01] adet.data.dataset_mapper INFO: Rebuilding the augmentations. The previous augmentations will be overridden.
[04/29 01:38:01] adet.data.detection_utils INFO: Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 700, 760, 820, 880, 940, 1000), max_size=2000, sample_style='choice'), RandomContrast(intensity_min=0.3, intensity_max=1.7), RandomBrightness(intensity_min=0.3, intensity_max=1.7), RandomLighting(scale=0.5250107552226669), RandomSaturation(intensity_min=0.3, intensity_max=1.7)]
[04/29 01:38:01] adet.data.datasets.text INFO: Loaded 1000 images in COCO format from datasets/ic15/train_37voc.json
[04/29 01:38:01] d2.data.build INFO: Removed 0 images with no usable annotations. 979 images left.
[04/29 01:38:01] d2.data.build INFO: Using training sampler TrainingSampler
[04/29 01:38:01] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[04/29 01:38:01] d2.data.common INFO: Serializing 979 elements to byte tensors and concatenating them all ...
[04/29 01:38:01] d2.data.common INFO: Serialized dataset takes 6.51 MiB
[04/29 01:38:01] d2.data.build INFO: Making batched data loader with batch_size=2
[04/29 01:38:01] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth ...
[04/29 01:38:01] fvcore.common.checkpoint INFO: [Checkpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth ...
[04/29 01:38:02] adet.trainer INFO: Starting training from iteration 0
[04/29 01:41:16] d2.utils.events INFO:  eta: 3 days, 7:42:33  iter: 19  loss_ce: 0.59  loss_texts: 1.031  loss_ctrl_points: 0.3547  loss_bd_points: 0.3924  loss_ce_0: 0.5896  loss_texts_0: 1.147  loss_ctrl_points_0: 0.3951  loss_bd_points_0: 0.4598  loss_ce_1: 0.6411  loss_texts_1: 1.222  loss_ctrl_points_1: 0.3768  loss_bd_points_1: 0.4449  loss_ce_2: 0.6241  loss_texts_2: 1.027  loss_ctrl_points_2: 0.3597  loss_bd_points_2: 0.4147  loss_ce_3: 0.6063  loss_texts_3: 1.008  loss_ctrl_points_3: 0.3587  loss_bd_points_3: 0.4  loss_ce_4: 0.5921  loss_texts_4: 1.095  loss_ctrl_points_4: 0.3619  loss_bd_points_4: 0.4061  loss_ce_enc: 0.1528  loss_bezier_enc: 0.1128  total_loss: 15.25    time: 9.7144  last_time: 9.4313  data_time: 0.0070  last_data_time: 0.0032   lr: 1.5285e-06  max_mem: 7893M
[04/29 01:41:16] d2.utils.events INFO:  eta: 3 days, 7:32:14  iter: 19  loss_ce: 0.59  loss_texts: 1.031  loss_ctrl_points: 0.3547  loss_bd_points: 0.3924  loss_ce_0: 0.5896  loss_texts_0: 1.147  loss_ctrl_points_0: 0.3951  loss_bd_points_0: 0.4598  loss_ce_1: 0.6411  loss_texts_1: 1.222  loss_ctrl_points_1: 0.3768  loss_bd_points_1: 0.4449  loss_ce_2: 0.6241  loss_texts_2: 1.027  loss_ctrl_points_2: 0.3597  loss_bd_points_2: 0.4147  loss_ce_3: 0.6063  loss_texts_3: 1.008  loss_ctrl_points_3: 0.3587  loss_bd_points_3: 0.4  loss_ce_4: 0.5921  loss_texts_4: 1.095  loss_ctrl_points_4: 0.3619  loss_bd_points_4: 0.4061  loss_ce_enc: 0.1528  loss_bezier_enc: 0.1128  total_loss: 15.25    time: 9.7093  last_time: 9.5302  data_time: 0.0070  last_data_time: 0.0032   lr: 1.5684e-06  max_mem: 7893M
[04/29 02:10:49] detectron2 INFO: Rank of current process: 0. World size: 1
[04/29 02:10:50] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/kylee/DeepSolo_LangPriors/detectron2/detectron2
Compiler                         GCC 9.5
CUDA compiler                    CUDA 11.3
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.12.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 3060 (arch=8.6)
Driver version                   550.120
CUDA_HOME                        /usr
Pillow                           9.0.1
torchvision                      0.13.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[04/29 02:10:50] detectron2 INFO: Command line arguments: Namespace(config_file='configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50152', opts=[])
[04/29 02:10:50] detectron2 INFO: Contents of args.config_file=configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml:
_BASE_: "../Base_det.yaml"

MODEL:
  WEIGHTS: "output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth"
  TRANSFORMER:
    INFERENCE_TH_TEST: 0.3

DATASETS:
  TRAIN: ("ic15_train",)
  TEST: ("ic15_test",)

INPUT:
  MIN_SIZE_TRAIN: (640, 700, 760, 820, 880, 940, 1000)
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MAX_SIZE_TEST: 4000
  CROP:
    ENABLED: False
  ROTATE: False

SOLVER:
  AMP:
    ENABLED: True  # Enable Automatic Mixed Precision
  IMS_PER_BATCH: 2
  BASE_LR: 1e-5
  LR_BACKBONE: 1e-6
  WARMUP_ITERS: 250
  STEPS: (15000, 25000)  # no step
  MAX_ITER: 30000
  GAMMA: 0.1
  CHECKPOINT_PERIOD: 1000

TEST:
  EVAL_PERIOD: 1000
  # 1 - Generic, 2 - Weak, 3 - Strong (for icdar2015)
  LEXICON_TYPE: 3

OUTPUT_DIR: "output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15"

[04/29 02:10:50] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ic15_test
  TRAIN:
  - ic15_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    CROP_INSTANCE: false
    ENABLED: false
    SIZE:
    - 0.1
    - 0.1
    TYPE: relative_range
  FORMAT: RGB
  HFLIP_TRAIN: false
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 4000
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MIN_SIZE_TRAIN:
  - 640
  - 700
  - 760
  - 820
  - 880
  - 940
  - 1000
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  ROTATE: false
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    ANTI_ALIAS: false
    FREEZE_AT: 2
    NAME: build_resnet_backbone
  BASIS_MODULE:
    ANN_SET: coco
    COMMON_STRIDE: 8
    CONVS_DIM: 128
    IN_FEATURES:
    - p3
    - p4
    - p5
    LOSS_ON: false
    LOSS_WEIGHT: 0.3
    NAME: ProtoNet
    NORM: SyncBN
    NUM_BASES: 4
    NUM_CLASSES: 80
    NUM_CONVS: 3
  BATEXT:
    CANONICAL_SIZE: 96
    CONV_DIM: 256
    CUSTOM_DICT: ''
    IN_FEATURES:
    - p2
    - p3
    - p4
    NUM_CHARS: 25
    NUM_CONV: 2
    POOLER_RESOLUTION:
    - 8
    - 32
    POOLER_SCALES:
    - 0.25
    - 0.125
    - 0.0625
    RECOGNITION_LOSS: ctc
    RECOGNIZER: attn
    SAMPLING_RATIO: 1
    USE_AET: false
    USE_COORDCONV: false
    VOC_SIZE: 96
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LINGUISTIC_PRIOR:
    EMBED_MODEL_NAME: google/canine-c
    GENERIC_DICT_PATH: datasets/generic_90k_words.txt
    SOFT_TARGET_THRESHOLD: 0.85
    TEMPERATURE: 1.0
  LOAD_PROPOSALS: false
  MASK_ON: false
  META_ARCHITECTURE: TransformerPureDetector
  MOBILENET: false
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_INTERVAL: 1
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  SWIN:
    DROP_PATH_RATE: 0.2
    TYPE: tiny
  TOP_MODULE:
    DIM: 16
    NAME: conv
  TRANSFORMER:
    AUX_LOSS: true
    BOUNDARY_HEAD: true
    CUSTOM_DICT: ''
    DEC_LAYERS: 6
    DEC_N_POINTS: 4
    DIM_FEEDFORWARD: 1024
    DROPOUT: 0.0
    ENABLED: true
    ENC_LAYERS: 6
    ENC_N_POINTS: 4
    HIDDEN_DIM: 256
    INFERENCE_TH_TEST: 0.3
    LOSS:
      AUX_LOSS: true
      BEZIER_CLASS_WEIGHT: 1.0
      BEZIER_COORD_WEIGHT: 1.0
      BEZIER_SAMPLE_POINTS: 25
      BOUNDARY_WEIGHT: 0.5
      FOCAL_ALPHA: 0.25
      FOCAL_GAMMA: 2.0
      POINT_CLASS_WEIGHT: 1.0
      POINT_COORD_WEIGHT: 1.0
      POINT_TEXT_WEIGHT: 0.5
    NHEADS: 8
    NUM_FEATURE_LEVELS: 4
    NUM_POINTS: 25
    NUM_QUERIES: 100
    POSITION_EMBEDDING_SCALE: 6.283185307179586
    TEMPERATURE: 10000
    VOC_SIZE: 37
  ViTAEv2:
    DROP_PATH_RATE: 0.2
    TYPE: vitaev2_s
  WEIGHTS: output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth
OUTPUT_DIR: output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15
SEED: 42
SOLVER:
  AMP:
    ENABLED: true
  BASE_LR: 1.0e-05
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.1
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 2
  LR_BACKBONE: 1.0e-06
  LR_BACKBONE_NAMES:
  - backbone.0
  LR_LINEAR_PROJ_MULT: 1.0
  LR_LINEAR_PROJ_NAMES:
  - reference_points
  - sampling_offsets
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 30000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 15000
  - 25000
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 250
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  LEXICON_TYPE: 3
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/29 02:10:50] detectron2 INFO: Full config saved to output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/config.yaml
[04/29 02:10:55] d2.engine.defaults INFO: Model:
TransformerPureDetector(
  (detection_transformer): DETECTION_TRANSFORMER(
    (backbone): Joiner(
      (0): MaskedBackbone(
        (backbone): ResNet(
          (stem): BasicStem(
            (conv1): Conv2d(
              3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
          )
          (res2): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv1): Conv2d(
                64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
          )
          (res3): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv1): Conv2d(
                256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
          )
          (res4): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
              (conv1): Conv2d(
                512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (4): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (5): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
          )
          (res5): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
              (conv1): Conv2d(
                1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
          )
        )
      )
      (1): PositionalEncoding2D()
    )
    (point_embed): Embedding(2500, 256)
    (transformer): DeformableTransformer(
      (encoder): DeformableTransformerEncoder(
        (layers): ModuleList(
          (0): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (decoder): DeformableCompositeTransformerDecoder(
        (layers): ModuleList(
          (0): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ctrl_point_coord): ModuleList(
          (0): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (1): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (2): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (3): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (4): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (5): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
        )
      )
      (enc_output): Linear(in_features=256, out_features=256, bias=True)
      (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (bezier_coord_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=8, bias=True)
        )
      )
      (bezier_class_embed): Linear(in_features=256, out_features=1, bias=True)
    )
    (input_proj): ModuleList(
      (0): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (1): Sequential(
        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (2): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (3): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (bezier_proposal_coord): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=8, bias=True)
      )
    )
    (bezier_proposal_class): Linear(in_features=256, out_features=1, bias=True)
    (ctrl_point_coord): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
    )
    (ctrl_point_class): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (ctrl_point_text): ModuleList(
      (0): Linear(in_features=256, out_features=38, bias=True)
      (1): Linear(in_features=256, out_features=38, bias=True)
      (2): Linear(in_features=256, out_features=38, bias=True)
      (3): Linear(in_features=256, out_features=38, bias=True)
      (4): Linear(in_features=256, out_features=38, bias=True)
      (5): Linear(in_features=256, out_features=38, bias=True)
    )
    (boundary_offset): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (bezier_sampler): BezierSampler()
    (enc_matcher): BezierHungarianMatcher(
      (bezier_sampler): BezierSampler()
    )
    (dec_matcher): CtrlPointHungarianMatcher()
  )
)
[04/29 02:10:55] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 700, 760, 820, 880, 940, 1000), max_size=2000, sample_style='choice'), RandomFlip()]
[04/29 02:10:55] adet.data.dataset_mapper INFO: Rebuilding the augmentations. The previous augmentations will be overridden.
[04/29 02:10:55] adet.data.detection_utils INFO: Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 700, 760, 820, 880, 940, 1000), max_size=2000, sample_style='choice'), RandomContrast(intensity_min=0.3, intensity_max=1.7), RandomBrightness(intensity_min=0.3, intensity_max=1.7), RandomLighting(scale=1.1394267984578836), RandomSaturation(intensity_min=0.3, intensity_max=1.7)]
[04/29 02:10:55] adet.data.datasets.text INFO: Loaded 1000 images in COCO format from datasets/ic15/train_37voc.json
[04/29 02:10:55] d2.data.build INFO: Removed 0 images with no usable annotations. 979 images left.
[04/29 02:10:55] d2.data.build INFO: Distribution of instances among all 1 categories:
[36m|  category  | #instances   |
|:----------:|:-------------|
|    text    | 4441         |
|            |              |[0m
[04/29 02:10:55] d2.data.build INFO: Using training sampler TrainingSampler
[04/29 02:10:55] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[04/29 02:10:55] d2.data.common INFO: Serializing 979 elements to byte tensors and concatenating them all ...
[04/29 02:10:55] d2.data.common INFO: Serialized dataset takes 6.51 MiB
[04/29 02:10:55] d2.data.build INFO: Making batched data loader with batch_size=2
[04/29 02:10:56] d2.engine.defaults INFO: Model:
TransformerPureDetector(
  (detection_transformer): DETECTION_TRANSFORMER(
    (backbone): Joiner(
      (0): MaskedBackbone(
        (backbone): ResNet(
          (stem): BasicStem(
            (conv1): Conv2d(
              3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
          )
          (res2): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv1): Conv2d(
                64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
          )
          (res3): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv1): Conv2d(
                256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
          )
          (res4): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
              (conv1): Conv2d(
                512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (4): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (5): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
          )
          (res5): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
              (conv1): Conv2d(
                1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
          )
        )
      )
      (1): PositionalEncoding2D()
    )
    (point_embed): Embedding(2500, 256)
    (transformer): DeformableTransformer(
      (encoder): DeformableTransformerEncoder(
        (layers): ModuleList(
          (0): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (decoder): DeformableCompositeTransformerDecoder(
        (layers): ModuleList(
          (0): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ctrl_point_coord): ModuleList(
          (0): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (1): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (2): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (3): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (4): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (5): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
        )
      )
      (enc_output): Linear(in_features=256, out_features=256, bias=True)
      (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (bezier_coord_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=8, bias=True)
        )
      )
      (bezier_class_embed): Linear(in_features=256, out_features=1, bias=True)
    )
    (input_proj): ModuleList(
      (0): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (1): Sequential(
        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (2): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (3): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (bezier_proposal_coord): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=8, bias=True)
      )
    )
    (bezier_proposal_class): Linear(in_features=256, out_features=1, bias=True)
    (ctrl_point_coord): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
    )
    (ctrl_point_class): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (ctrl_point_text): ModuleList(
      (0): Linear(in_features=256, out_features=38, bias=True)
      (1): Linear(in_features=256, out_features=38, bias=True)
      (2): Linear(in_features=256, out_features=38, bias=True)
      (3): Linear(in_features=256, out_features=38, bias=True)
      (4): Linear(in_features=256, out_features=38, bias=True)
      (5): Linear(in_features=256, out_features=38, bias=True)
    )
    (boundary_offset): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (bezier_sampler): BezierSampler()
    (enc_matcher): BezierHungarianMatcher(
      (bezier_sampler): BezierSampler()
    )
    (dec_matcher): CtrlPointHungarianMatcher()
  )
)
[04/29 02:10:56] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 700, 760, 820, 880, 940, 1000), max_size=2000, sample_style='choice'), RandomFlip()]
[04/29 02:10:56] adet.data.dataset_mapper INFO: Rebuilding the augmentations. The previous augmentations will be overridden.
[04/29 02:10:56] adet.data.detection_utils INFO: Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 700, 760, 820, 880, 940, 1000), max_size=2000, sample_style='choice'), RandomContrast(intensity_min=0.3, intensity_max=1.7), RandomBrightness(intensity_min=0.3, intensity_max=1.7), RandomLighting(scale=0.5250107552226669), RandomSaturation(intensity_min=0.3, intensity_max=1.7)]
[04/29 02:10:56] adet.data.datasets.text INFO: Loaded 1000 images in COCO format from datasets/ic15/train_37voc.json
[04/29 02:10:56] d2.data.build INFO: Removed 0 images with no usable annotations. 979 images left.
[04/29 02:10:56] d2.data.build INFO: Using training sampler TrainingSampler
[04/29 02:10:56] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[04/29 02:10:56] d2.data.common INFO: Serializing 979 elements to byte tensors and concatenating them all ...
[04/29 02:10:56] d2.data.common INFO: Serialized dataset takes 6.51 MiB
[04/29 02:10:56] d2.data.build INFO: Making batched data loader with batch_size=2
[04/29 02:10:56] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth ...
[04/29 02:10:56] fvcore.common.checkpoint INFO: [Checkpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth ...
[04/29 02:10:56] adet.trainer INFO: Starting training from iteration 0
[04/29 02:19:53] detectron2 INFO: Rank of current process: 0. World size: 1
[04/29 02:19:53] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/kylee/DeepSolo_LangPriors/detectron2/detectron2
Compiler                         GCC 9.5
CUDA compiler                    CUDA 11.3
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.12.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 3060 (arch=8.6)
Driver version                   550.120
CUDA_HOME                        /usr
Pillow                           9.0.1
torchvision                      0.13.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[04/29 02:19:53] detectron2 INFO: Command line arguments: Namespace(config_file='configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50152', opts=[])
[04/29 02:19:53] detectron2 INFO: Contents of args.config_file=configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml:
_BASE_: "../Base_det.yaml"

MODEL:
  WEIGHTS: "output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth"
  TRANSFORMER:
    INFERENCE_TH_TEST: 0.3

DATASETS:
  TRAIN: ("ic15_train",)
  TEST: ("ic15_test",)

INPUT:
  MIN_SIZE_TRAIN: (640, 700, 760, 820, 880, 940, 1000)
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MAX_SIZE_TEST: 4000
  CROP:
    ENABLED: False
  ROTATE: False

SOLVER:
  AMP:
    ENABLED: True  # Enable Automatic Mixed Precision
  IMS_PER_BATCH: 2
  BASE_LR: 1e-5
  LR_BACKBONE: 1e-6
  WARMUP_ITERS: 250
  STEPS: (15000, 25000)  # no step
  MAX_ITER: 30000
  GAMMA: 0.1
  CHECKPOINT_PERIOD: 1000

TEST:
  EVAL_PERIOD: 1000
  # 1 - Generic, 2 - Weak, 3 - Strong (for icdar2015)
  LEXICON_TYPE: 3

OUTPUT_DIR: "output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15"

[04/29 02:19:53] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ic15_test
  TRAIN:
  - ic15_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    CROP_INSTANCE: false
    ENABLED: false
    SIZE:
    - 0.1
    - 0.1
    TYPE: relative_range
  FORMAT: RGB
  HFLIP_TRAIN: false
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 4000
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MIN_SIZE_TRAIN:
  - 640
  - 700
  - 760
  - 820
  - 880
  - 940
  - 1000
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  ROTATE: false
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    ANTI_ALIAS: false
    FREEZE_AT: 2
    NAME: build_resnet_backbone
  BASIS_MODULE:
    ANN_SET: coco
    COMMON_STRIDE: 8
    CONVS_DIM: 128
    IN_FEATURES:
    - p3
    - p4
    - p5
    LOSS_ON: false
    LOSS_WEIGHT: 0.3
    NAME: ProtoNet
    NORM: SyncBN
    NUM_BASES: 4
    NUM_CLASSES: 80
    NUM_CONVS: 3
  BATEXT:
    CANONICAL_SIZE: 96
    CONV_DIM: 256
    CUSTOM_DICT: ''
    IN_FEATURES:
    - p2
    - p3
    - p4
    NUM_CHARS: 25
    NUM_CONV: 2
    POOLER_RESOLUTION:
    - 8
    - 32
    POOLER_SCALES:
    - 0.25
    - 0.125
    - 0.0625
    RECOGNITION_LOSS: ctc
    RECOGNIZER: attn
    SAMPLING_RATIO: 1
    USE_AET: false
    USE_COORDCONV: false
    VOC_SIZE: 96
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: false
  META_ARCHITECTURE: TransformerPureDetector
  MOBILENET: false
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_INTERVAL: 1
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  SWIN:
    DROP_PATH_RATE: 0.2
    TYPE: tiny
  TOP_MODULE:
    DIM: 16
    NAME: conv
  TRANSFORMER:
    AUX_LOSS: true
    BOUNDARY_HEAD: true
    CUSTOM_DICT: ''
    DEC_LAYERS: 6
    DEC_N_POINTS: 4
    DIM_FEEDFORWARD: 1024
    DROPOUT: 0.0
    ENABLED: true
    ENC_LAYERS: 6
    ENC_N_POINTS: 4
    HIDDEN_DIM: 256
    INFERENCE_TH_TEST: 0.3
    LOSS:
      AUX_LOSS: true
      BEZIER_CLASS_WEIGHT: 1.0
      BEZIER_COORD_WEIGHT: 1.0
      BEZIER_SAMPLE_POINTS: 25
      BOUNDARY_WEIGHT: 0.5
      FOCAL_ALPHA: 0.25
      FOCAL_GAMMA: 2.0
      POINT_CLASS_WEIGHT: 1.0
      POINT_COORD_WEIGHT: 1.0
      POINT_TEXT_WEIGHT: 0.5
    NHEADS: 8
    NUM_FEATURE_LEVELS: 4
    NUM_POINTS: 25
    NUM_QUERIES: 100
    POSITION_EMBEDDING_SCALE: 6.283185307179586
    TEMPERATURE: 10000
    VOC_SIZE: 37
  ViTAEv2:
    DROP_PATH_RATE: 0.2
    TYPE: vitaev2_s
  WEIGHTS: output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth
OUTPUT_DIR: output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15
SEED: 42
SOLVER:
  AMP:
    ENABLED: true
  BASE_LR: 1.0e-05
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.1
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 2
  LR_BACKBONE: 1.0e-06
  LR_BACKBONE_NAMES:
  - backbone.0
  LR_LINEAR_PROJ_MULT: 1.0
  LR_LINEAR_PROJ_NAMES:
  - reference_points
  - sampling_offsets
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 30000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 15000
  - 25000
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 250
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  LEXICON_TYPE: 3
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/29 02:19:53] detectron2 INFO: Full config saved to output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/config.yaml
[04/29 02:19:55] d2.engine.defaults INFO: Model:
TransformerPureDetector(
  (detection_transformer): DETECTION_TRANSFORMER(
    (backbone): Joiner(
      (0): MaskedBackbone(
        (backbone): ResNet(
          (stem): BasicStem(
            (conv1): Conv2d(
              3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
          )
          (res2): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv1): Conv2d(
                64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
          )
          (res3): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv1): Conv2d(
                256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
          )
          (res4): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
              (conv1): Conv2d(
                512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (4): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (5): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
          )
          (res5): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
              (conv1): Conv2d(
                1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
          )
        )
      )
      (1): PositionalEncoding2D()
    )
    (point_embed): Embedding(2500, 256)
    (transformer): DeformableTransformer(
      (encoder): DeformableTransformerEncoder(
        (layers): ModuleList(
          (0): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (decoder): DeformableCompositeTransformerDecoder(
        (layers): ModuleList(
          (0): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ctrl_point_coord): ModuleList(
          (0): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (1): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (2): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (3): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (4): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (5): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
        )
      )
      (enc_output): Linear(in_features=256, out_features=256, bias=True)
      (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (bezier_coord_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=8, bias=True)
        )
      )
      (bezier_class_embed): Linear(in_features=256, out_features=1, bias=True)
    )
    (input_proj): ModuleList(
      (0): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (1): Sequential(
        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (2): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (3): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (bezier_proposal_coord): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=8, bias=True)
      )
    )
    (bezier_proposal_class): Linear(in_features=256, out_features=1, bias=True)
    (ctrl_point_coord): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
    )
    (ctrl_point_class): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (ctrl_point_text): ModuleList(
      (0): Linear(in_features=256, out_features=38, bias=True)
      (1): Linear(in_features=256, out_features=38, bias=True)
      (2): Linear(in_features=256, out_features=38, bias=True)
      (3): Linear(in_features=256, out_features=38, bias=True)
      (4): Linear(in_features=256, out_features=38, bias=True)
      (5): Linear(in_features=256, out_features=38, bias=True)
    )
    (boundary_offset): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (enc_matcher): BezierHungarianMatcher(
      (bezier_sampler): BezierSampler()
    )
    (dec_matcher): CtrlPointHungarianMatcher()
    (bezier_sampler): BezierSampler()
  )
)
[04/29 02:19:55] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 700, 760, 820, 880, 940, 1000), max_size=2000, sample_style='choice'), RandomFlip()]
[04/29 02:19:55] adet.data.dataset_mapper INFO: Rebuilding the augmentations. The previous augmentations will be overridden.
[04/29 02:19:55] adet.data.detection_utils INFO: Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 700, 760, 820, 880, 940, 1000), max_size=2000, sample_style='choice'), RandomContrast(intensity_min=0.3, intensity_max=1.7), RandomBrightness(intensity_min=0.3, intensity_max=1.7), RandomLighting(scale=1.1394267984578836), RandomSaturation(intensity_min=0.3, intensity_max=1.7)]
[04/29 02:19:55] adet.data.datasets.text INFO: Loaded 1000 images in COCO format from datasets/ic15/train_37voc.json
[04/29 02:19:55] d2.data.build INFO: Removed 0 images with no usable annotations. 979 images left.
[04/29 02:19:55] d2.data.build INFO: Distribution of instances among all 1 categories:
[36m|  category  | #instances   |
|:----------:|:-------------|
|    text    | 4441         |
|            |              |[0m
[04/29 02:19:55] d2.data.build INFO: Using training sampler TrainingSampler
[04/29 02:19:55] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[04/29 02:19:55] d2.data.common INFO: Serializing 979 elements to byte tensors and concatenating them all ...
[04/29 02:19:55] d2.data.common INFO: Serialized dataset takes 6.51 MiB
[04/29 02:19:55] d2.data.build INFO: Making batched data loader with batch_size=2
[04/29 02:19:55] d2.engine.defaults INFO: Model:
TransformerPureDetector(
  (detection_transformer): DETECTION_TRANSFORMER(
    (backbone): Joiner(
      (0): MaskedBackbone(
        (backbone): ResNet(
          (stem): BasicStem(
            (conv1): Conv2d(
              3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
          )
          (res2): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv1): Conv2d(
                64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
          )
          (res3): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv1): Conv2d(
                256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
          )
          (res4): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
              (conv1): Conv2d(
                512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (4): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (5): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
          )
          (res5): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
              (conv1): Conv2d(
                1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
          )
        )
      )
      (1): PositionalEncoding2D()
    )
    (point_embed): Embedding(2500, 256)
    (transformer): DeformableTransformer(
      (encoder): DeformableTransformerEncoder(
        (layers): ModuleList(
          (0): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (decoder): DeformableCompositeTransformerDecoder(
        (layers): ModuleList(
          (0): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ctrl_point_coord): ModuleList(
          (0): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (1): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (2): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (3): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (4): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (5): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
        )
      )
      (enc_output): Linear(in_features=256, out_features=256, bias=True)
      (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (bezier_coord_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=8, bias=True)
        )
      )
      (bezier_class_embed): Linear(in_features=256, out_features=1, bias=True)
    )
    (input_proj): ModuleList(
      (0): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (1): Sequential(
        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (2): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (3): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (bezier_proposal_coord): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=8, bias=True)
      )
    )
    (bezier_proposal_class): Linear(in_features=256, out_features=1, bias=True)
    (ctrl_point_coord): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
    )
    (ctrl_point_class): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (ctrl_point_text): ModuleList(
      (0): Linear(in_features=256, out_features=38, bias=True)
      (1): Linear(in_features=256, out_features=38, bias=True)
      (2): Linear(in_features=256, out_features=38, bias=True)
      (3): Linear(in_features=256, out_features=38, bias=True)
      (4): Linear(in_features=256, out_features=38, bias=True)
      (5): Linear(in_features=256, out_features=38, bias=True)
    )
    (boundary_offset): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (enc_matcher): BezierHungarianMatcher(
      (bezier_sampler): BezierSampler()
    )
    (dec_matcher): CtrlPointHungarianMatcher()
    (bezier_sampler): BezierSampler()
  )
)
[04/29 02:19:55] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 700, 760, 820, 880, 940, 1000), max_size=2000, sample_style='choice'), RandomFlip()]
[04/29 02:19:55] adet.data.dataset_mapper INFO: Rebuilding the augmentations. The previous augmentations will be overridden.
[04/29 02:19:55] adet.data.detection_utils INFO: Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 700, 760, 820, 880, 940, 1000), max_size=2000, sample_style='choice'), RandomContrast(intensity_min=0.3, intensity_max=1.7), RandomBrightness(intensity_min=0.3, intensity_max=1.7), RandomLighting(scale=0.5250107552226669), RandomSaturation(intensity_min=0.3, intensity_max=1.7)]
[04/29 02:19:55] adet.data.datasets.text INFO: Loaded 1000 images in COCO format from datasets/ic15/train_37voc.json
[04/29 02:19:55] d2.data.build INFO: Removed 0 images with no usable annotations. 979 images left.
[04/29 02:19:55] d2.data.build INFO: Using training sampler TrainingSampler
[04/29 02:19:55] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[04/29 02:19:55] d2.data.common INFO: Serializing 979 elements to byte tensors and concatenating them all ...
[04/29 02:19:55] d2.data.common INFO: Serialized dataset takes 6.51 MiB
[04/29 02:19:55] d2.data.build INFO: Making batched data loader with batch_size=2
[04/29 02:19:55] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth ...
[04/29 02:19:55] fvcore.common.checkpoint INFO: [Checkpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth ...
[04/29 02:19:55] adet.trainer INFO: Starting training from iteration 0
[04/29 02:32:49] detectron2 INFO: Rank of current process: 0. World size: 1
[04/29 02:32:50] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/kylee/DeepSolo_LangPriors/detectron2/detectron2
Compiler                         GCC 9.5
CUDA compiler                    CUDA 11.3
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.12.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 3060 (arch=8.6)
Driver version                   550.120
CUDA_HOME                        /usr
Pillow                           9.0.1
torchvision                      0.13.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[04/29 02:32:50] detectron2 INFO: Command line arguments: Namespace(config_file='configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50152', opts=[])
[04/29 02:32:50] detectron2 INFO: Contents of args.config_file=configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml:
_BASE_: "../Base_det.yaml"

MODEL:
  WEIGHTS: "output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth"
  TRANSFORMER:
    INFERENCE_TH_TEST: 0.3

DATASETS:
  TRAIN: ("ic15_train",)
  TEST: ("ic15_test",)

INPUT:
  MIN_SIZE_TRAIN: (640, 700, 760, 820, 880, 940, 1000)
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MAX_SIZE_TEST: 4000
  CROP:
    ENABLED: False
  ROTATE: False

SOLVER:
  AMP:
    ENABLED: True  # Enable Automatic Mixed Precision
  IMS_PER_BATCH: 2
  BASE_LR: 1e-5
  LR_BACKBONE: 1e-6
  WARMUP_ITERS: 250
  STEPS: (15000, 25000)  # no step
  MAX_ITER: 30000
  GAMMA: 0.1
  CHECKPOINT_PERIOD: 1000

TEST:
  EVAL_PERIOD: 1000
  # 1 - Generic, 2 - Weak, 3 - Strong (for icdar2015)
  LEXICON_TYPE: 3

OUTPUT_DIR: "output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15"

[04/29 02:32:50] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ic15_test
  TRAIN:
  - ic15_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    CROP_INSTANCE: false
    ENABLED: false
    SIZE:
    - 0.1
    - 0.1
    TYPE: relative_range
  FORMAT: RGB
  HFLIP_TRAIN: false
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 4000
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MIN_SIZE_TRAIN:
  - 640
  - 700
  - 760
  - 820
  - 880
  - 940
  - 1000
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  ROTATE: false
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    ANTI_ALIAS: false
    FREEZE_AT: 2
    NAME: build_resnet_backbone
  BASIS_MODULE:
    ANN_SET: coco
    COMMON_STRIDE: 8
    CONVS_DIM: 128
    IN_FEATURES:
    - p3
    - p4
    - p5
    LOSS_ON: false
    LOSS_WEIGHT: 0.3
    NAME: ProtoNet
    NORM: SyncBN
    NUM_BASES: 4
    NUM_CLASSES: 80
    NUM_CONVS: 3
  BATEXT:
    CANONICAL_SIZE: 96
    CONV_DIM: 256
    CUSTOM_DICT: ''
    IN_FEATURES:
    - p2
    - p3
    - p4
    NUM_CHARS: 25
    NUM_CONV: 2
    POOLER_RESOLUTION:
    - 8
    - 32
    POOLER_SCALES:
    - 0.25
    - 0.125
    - 0.0625
    RECOGNITION_LOSS: ctc
    RECOGNIZER: attn
    SAMPLING_RATIO: 1
    USE_AET: false
    USE_COORDCONV: false
    VOC_SIZE: 96
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: false
  META_ARCHITECTURE: TransformerPureDetector
  MOBILENET: false
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_INTERVAL: 1
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  SWIN:
    DROP_PATH_RATE: 0.2
    TYPE: tiny
  TOP_MODULE:
    DIM: 16
    NAME: conv
  TRANSFORMER:
    AUX_LOSS: true
    BOUNDARY_HEAD: true
    CUSTOM_DICT: ''
    DEC_LAYERS: 6
    DEC_N_POINTS: 4
    DIM_FEEDFORWARD: 1024
    DROPOUT: 0.0
    ENABLED: true
    ENC_LAYERS: 6
    ENC_N_POINTS: 4
    HIDDEN_DIM: 256
    INFERENCE_TH_TEST: 0.3
    LOSS:
      AUX_LOSS: true
      BEZIER_CLASS_WEIGHT: 1.0
      BEZIER_COORD_WEIGHT: 1.0
      BEZIER_SAMPLE_POINTS: 25
      BOUNDARY_WEIGHT: 0.5
      FOCAL_ALPHA: 0.25
      FOCAL_GAMMA: 2.0
      POINT_CLASS_WEIGHT: 1.0
      POINT_COORD_WEIGHT: 1.0
      POINT_TEXT_WEIGHT: 0.5
    NHEADS: 8
    NUM_FEATURE_LEVELS: 4
    NUM_POINTS: 25
    NUM_QUERIES: 100
    POSITION_EMBEDDING_SCALE: 6.283185307179586
    TEMPERATURE: 10000
    VOC_SIZE: 37
  ViTAEv2:
    DROP_PATH_RATE: 0.2
    TYPE: vitaev2_s
  WEIGHTS: output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth
OUTPUT_DIR: output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15
SEED: 42
SOLVER:
  AMP:
    ENABLED: true
  BASE_LR: 1.0e-05
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.1
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 2
  LR_BACKBONE: 1.0e-06
  LR_BACKBONE_NAMES:
  - backbone.0
  LR_LINEAR_PROJ_MULT: 1.0
  LR_LINEAR_PROJ_NAMES:
  - reference_points
  - sampling_offsets
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 30000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 15000
  - 25000
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 250
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  LEXICON_TYPE: 3
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/29 02:32:50] detectron2 INFO: Full config saved to output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/config.yaml
[04/29 02:32:51] d2.engine.defaults INFO: Model:
TransformerPureDetector(
  (detection_transformer): DETECTION_TRANSFORMER(
    (backbone): Joiner(
      (0): MaskedBackbone(
        (backbone): ResNet(
          (stem): BasicStem(
            (conv1): Conv2d(
              3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
          )
          (res2): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv1): Conv2d(
                64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
          )
          (res3): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv1): Conv2d(
                256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
          )
          (res4): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
              (conv1): Conv2d(
                512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (4): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (5): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
          )
          (res5): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
              (conv1): Conv2d(
                1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
          )
        )
      )
      (1): PositionalEncoding2D()
    )
    (point_embed): Embedding(2500, 256)
    (transformer): DeformableTransformer(
      (encoder): DeformableTransformerEncoder(
        (layers): ModuleList(
          (0): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (decoder): DeformableCompositeTransformerDecoder(
        (layers): ModuleList(
          (0): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ctrl_point_coord): ModuleList(
          (0): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (1): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (2): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (3): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (4): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (5): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
        )
      )
      (enc_output): Linear(in_features=256, out_features=256, bias=True)
      (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (bezier_coord_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=8, bias=True)
        )
      )
      (bezier_class_embed): Linear(in_features=256, out_features=1, bias=True)
    )
    (input_proj): ModuleList(
      (0): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (1): Sequential(
        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (2): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (3): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (bezier_proposal_coord): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=8, bias=True)
      )
    )
    (bezier_proposal_class): Linear(in_features=256, out_features=1, bias=True)
    (ctrl_point_coord): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
    )
    (ctrl_point_class): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (ctrl_point_text): ModuleList(
      (0): Linear(in_features=256, out_features=38, bias=True)
      (1): Linear(in_features=256, out_features=38, bias=True)
      (2): Linear(in_features=256, out_features=38, bias=True)
      (3): Linear(in_features=256, out_features=38, bias=True)
      (4): Linear(in_features=256, out_features=38, bias=True)
      (5): Linear(in_features=256, out_features=38, bias=True)
    )
    (boundary_offset): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (enc_matcher): BezierHungarianMatcher(
      (bezier_sampler): BezierSampler()
    )
    (dec_matcher): CtrlPointHungarianMatcher()
    (bezier_sampler): BezierSampler()
  )
)
[04/29 02:32:51] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 700, 760, 820, 880, 940, 1000), max_size=2000, sample_style='choice'), RandomFlip()]
[04/29 02:32:51] adet.data.dataset_mapper INFO: Rebuilding the augmentations. The previous augmentations will be overridden.
[04/29 02:32:51] adet.data.detection_utils INFO: Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 700, 760, 820, 880, 940, 1000), max_size=2000, sample_style='choice'), RandomContrast(intensity_min=0.3, intensity_max=1.7), RandomBrightness(intensity_min=0.3, intensity_max=1.7), RandomLighting(scale=1.1394267984578836), RandomSaturation(intensity_min=0.3, intensity_max=1.7)]
[04/29 02:32:51] adet.data.datasets.text INFO: Loaded 1000 images in COCO format from datasets/ic15/train_37voc.json
[04/29 02:32:51] d2.data.build INFO: Removed 0 images with no usable annotations. 979 images left.
[04/29 02:32:51] d2.data.build INFO: Distribution of instances among all 1 categories:
[36m|  category  | #instances   |
|:----------:|:-------------|
|    text    | 4441         |
|            |              |[0m
[04/29 02:32:51] d2.data.build INFO: Using training sampler TrainingSampler
[04/29 02:32:51] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[04/29 02:32:51] d2.data.common INFO: Serializing 979 elements to byte tensors and concatenating them all ...
[04/29 02:32:51] d2.data.common INFO: Serialized dataset takes 6.51 MiB
[04/29 02:32:51] d2.data.build INFO: Making batched data loader with batch_size=2
[04/29 02:32:52] d2.engine.defaults INFO: Model:
TransformerPureDetector(
  (detection_transformer): DETECTION_TRANSFORMER(
    (backbone): Joiner(
      (0): MaskedBackbone(
        (backbone): ResNet(
          (stem): BasicStem(
            (conv1): Conv2d(
              3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
          )
          (res2): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv1): Conv2d(
                64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
          )
          (res3): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv1): Conv2d(
                256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
          )
          (res4): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
              (conv1): Conv2d(
                512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (4): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (5): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
          )
          (res5): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
              (conv1): Conv2d(
                1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
          )
        )
      )
      (1): PositionalEncoding2D()
    )
    (point_embed): Embedding(2500, 256)
    (transformer): DeformableTransformer(
      (encoder): DeformableTransformerEncoder(
        (layers): ModuleList(
          (0): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (decoder): DeformableCompositeTransformerDecoder(
        (layers): ModuleList(
          (0): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ctrl_point_coord): ModuleList(
          (0): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (1): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (2): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (3): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (4): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (5): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
        )
      )
      (enc_output): Linear(in_features=256, out_features=256, bias=True)
      (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (bezier_coord_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=8, bias=True)
        )
      )
      (bezier_class_embed): Linear(in_features=256, out_features=1, bias=True)
    )
    (input_proj): ModuleList(
      (0): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (1): Sequential(
        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (2): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (3): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (bezier_proposal_coord): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=8, bias=True)
      )
    )
    (bezier_proposal_class): Linear(in_features=256, out_features=1, bias=True)
    (ctrl_point_coord): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
    )
    (ctrl_point_class): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (ctrl_point_text): ModuleList(
      (0): Linear(in_features=256, out_features=38, bias=True)
      (1): Linear(in_features=256, out_features=38, bias=True)
      (2): Linear(in_features=256, out_features=38, bias=True)
      (3): Linear(in_features=256, out_features=38, bias=True)
      (4): Linear(in_features=256, out_features=38, bias=True)
      (5): Linear(in_features=256, out_features=38, bias=True)
    )
    (boundary_offset): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (enc_matcher): BezierHungarianMatcher(
      (bezier_sampler): BezierSampler()
    )
    (dec_matcher): CtrlPointHungarianMatcher()
    (bezier_sampler): BezierSampler()
  )
)
[04/29 02:32:52] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 700, 760, 820, 880, 940, 1000), max_size=2000, sample_style='choice'), RandomFlip()]
[04/29 02:32:52] adet.data.dataset_mapper INFO: Rebuilding the augmentations. The previous augmentations will be overridden.
[04/29 02:32:52] adet.data.detection_utils INFO: Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 700, 760, 820, 880, 940, 1000), max_size=2000, sample_style='choice'), RandomContrast(intensity_min=0.3, intensity_max=1.7), RandomBrightness(intensity_min=0.3, intensity_max=1.7), RandomLighting(scale=0.5250107552226669), RandomSaturation(intensity_min=0.3, intensity_max=1.7)]
[04/29 02:32:52] adet.data.datasets.text INFO: Loaded 1000 images in COCO format from datasets/ic15/train_37voc.json
[04/29 02:32:52] d2.data.build INFO: Removed 0 images with no usable annotations. 979 images left.
[04/29 02:32:52] d2.data.build INFO: Using training sampler TrainingSampler
[04/29 02:32:52] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[04/29 02:32:52] d2.data.common INFO: Serializing 979 elements to byte tensors and concatenating them all ...
[04/29 02:32:52] d2.data.common INFO: Serialized dataset takes 6.51 MiB
[04/29 02:32:52] d2.data.build INFO: Making batched data loader with batch_size=2
[04/29 02:32:52] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth ...
[04/29 02:32:52] fvcore.common.checkpoint INFO: [Checkpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth ...
[04/29 02:32:52] adet.trainer INFO: Starting training from iteration 0
[04/29 02:35:14] d2.utils.events INFO:  eta: 2 days, 12:22:08  iter: 19  loss_ce: 0.1484  loss_texts: 0.2085  loss_ctrl_points: 0.1134  loss_bd_points: 0.1527  loss_ce_0: 0.1713  loss_texts_0: 0.3088  loss_ctrl_points_0: 0.1185  loss_bd_points_0: 0.1675  loss_ce_1: 0.157  loss_texts_1: 0.2489  loss_ctrl_points_1: 0.1187  loss_bd_points_1: 0.1634  loss_ce_2: 0.1519  loss_texts_2: 0.234  loss_ctrl_points_2: 0.1172  loss_bd_points_2: 0.162  loss_ce_3: 0.1475  loss_texts_3: 0.2156  loss_ctrl_points_3: 0.1174  loss_bd_points_3: 0.1572  loss_ce_4: 0.1464  loss_texts_4: 0.2079  loss_ctrl_points_4: 0.1138  loss_bd_points_4: 0.1527  loss_ce_enc: 0.1626  loss_bezier_enc: 0.1128  total_loss: 4.458    time: 7.0078  last_time: 6.1511  data_time: 0.0073  last_data_time: 0.0032   lr: 1.5285e-06  max_mem: 7245M
[04/29 02:35:14] d2.utils.events INFO:  eta: 2 days, 12:03:23  iter: 19  loss_ce: 0.1484  loss_texts: 0.2085  loss_ctrl_points: 0.1134  loss_bd_points: 0.1527  loss_ce_0: 0.1713  loss_texts_0: 0.3088  loss_ctrl_points_0: 0.1185  loss_bd_points_0: 0.1675  loss_ce_1: 0.157  loss_texts_1: 0.2489  loss_ctrl_points_1: 0.1187  loss_bd_points_1: 0.1634  loss_ce_2: 0.1519  loss_texts_2: 0.234  loss_ctrl_points_2: 0.1172  loss_bd_points_2: 0.162  loss_ce_3: 0.1475  loss_texts_3: 0.2156  loss_ctrl_points_3: 0.1174  loss_bd_points_3: 0.1572  loss_ce_4: 0.1464  loss_texts_4: 0.2079  loss_ctrl_points_4: 0.1138  loss_bd_points_4: 0.1527  loss_ce_enc: 0.1626  loss_bezier_enc: 0.1128  total_loss: 4.458    time: 7.0017  last_time: 6.7897  data_time: 0.0073  last_data_time: 0.0032   lr: 1.5684e-06  max_mem: 7245M
[04/29 02:37:46] d2.utils.events INFO:  eta: 2 days, 12:40:20  iter: 39  loss_ce: 0.1612  loss_texts: 0.2215  loss_ctrl_points: 0.1365  loss_bd_points: 0.1932  loss_ce_0: 0.1991  loss_texts_0: 0.3283  loss_ctrl_points_0: 0.1361  loss_bd_points_0: 0.2055  loss_ce_1: 0.1746  loss_texts_1: 0.2733  loss_ctrl_points_1: 0.1401  loss_bd_points_1: 0.2036  loss_ce_2: 0.1694  loss_texts_2: 0.2335  loss_ctrl_points_2: 0.1395  loss_bd_points_2: 0.203  loss_ce_3: 0.1669  loss_texts_3: 0.2274  loss_ctrl_points_3: 0.1361  loss_bd_points_3: 0.1932  loss_ce_4: 0.1629  loss_texts_4: 0.2242  loss_ctrl_points_4: 0.1371  loss_bd_points_4: 0.1934  loss_ce_enc: 0.1927  loss_bezier_enc: 0.1137  total_loss: 4.887    time: 7.3094  last_time: 7.9418  data_time: 0.0035  last_data_time: 0.0036   lr: 3.1269e-06  max_mem: 7245M
[04/29 02:37:46] d2.utils.events INFO:  eta: 2 days, 12:53:35  iter: 39  loss_ce: 0.1612  loss_texts: 0.2215  loss_ctrl_points: 0.1365  loss_bd_points: 0.1932  loss_ce_0: 0.1991  loss_texts_0: 0.3283  loss_ctrl_points_0: 0.1361  loss_bd_points_0: 0.2055  loss_ce_1: 0.1746  loss_texts_1: 0.2733  loss_ctrl_points_1: 0.1401  loss_bd_points_1: 0.2036  loss_ce_2: 0.1694  loss_texts_2: 0.2335  loss_ctrl_points_2: 0.1395  loss_bd_points_2: 0.203  loss_ce_3: 0.1669  loss_texts_3: 0.2274  loss_ctrl_points_3: 0.1361  loss_bd_points_3: 0.1932  loss_ce_4: 0.1629  loss_texts_4: 0.2242  loss_ctrl_points_4: 0.1371  loss_bd_points_4: 0.1934  loss_ce_enc: 0.1927  loss_bezier_enc: 0.1137  total_loss: 4.887    time: 7.3193  last_time: 8.0634  data_time: 0.0035  last_data_time: 0.0036   lr: 3.1668e-06  max_mem: 7245M
[04/29 02:40:07] d2.utils.events INFO:  eta: 2 days, 12:17:30  iter: 59  loss_ce: 0.1559  loss_texts: 0.1757  loss_ctrl_points: 0.1123  loss_bd_points: 0.15  loss_ce_0: 0.1839  loss_texts_0: 0.2959  loss_ctrl_points_0: 0.1167  loss_bd_points_0: 0.162  loss_ce_1: 0.1758  loss_texts_1: 0.2378  loss_ctrl_points_1: 0.1112  loss_bd_points_1: 0.157  loss_ce_2: 0.1637  loss_texts_2: 0.2093  loss_ctrl_points_2: 0.1135  loss_bd_points_2: 0.1552  loss_ce_3: 0.1592  loss_texts_3: 0.1854  loss_ctrl_points_3: 0.1122  loss_bd_points_3: 0.1534  loss_ce_4: 0.1548  loss_texts_4: 0.1756  loss_ctrl_points_4: 0.1131  loss_bd_points_4: 0.1485  loss_ce_enc: 0.1716  loss_bezier_enc: 0.1096  total_loss: 4.432    time: 7.2146  last_time: 7.8532  data_time: 0.0034  last_data_time: 0.0034   lr: 4.7253e-06  max_mem: 7245M
[04/29 02:40:07] d2.utils.events INFO:  eta: 2 days, 12:19:37  iter: 59  loss_ce: 0.1559  loss_texts: 0.1757  loss_ctrl_points: 0.1123  loss_bd_points: 0.15  loss_ce_0: 0.1839  loss_texts_0: 0.2959  loss_ctrl_points_0: 0.1167  loss_bd_points_0: 0.162  loss_ce_1: 0.1758  loss_texts_1: 0.2378  loss_ctrl_points_1: 0.1112  loss_bd_points_1: 0.157  loss_ce_2: 0.1637  loss_texts_2: 0.2093  loss_ctrl_points_2: 0.1135  loss_bd_points_2: 0.1552  loss_ce_3: 0.1592  loss_texts_3: 0.1854  loss_ctrl_points_3: 0.1122  loss_bd_points_3: 0.1534  loss_ce_4: 0.1548  loss_texts_4: 0.1756  loss_ctrl_points_4: 0.1131  loss_bd_points_4: 0.1485  loss_ce_enc: 0.1716  loss_bezier_enc: 0.1096  total_loss: 4.432    time: 7.2207  last_time: 7.9135  data_time: 0.0034  last_data_time: 0.0034   lr: 4.7652e-06  max_mem: 7245M
[04/29 02:43:01] d2.utils.events INFO:  eta: 2 days, 13:24:04  iter: 79  loss_ce: 0.1569  loss_texts: 0.2264  loss_ctrl_points: 0.1188  loss_bd_points: 0.1561  loss_ce_0: 0.178  loss_texts_0: 0.3103  loss_ctrl_points_0: 0.1265  loss_bd_points_0: 0.1549  loss_ce_1: 0.1689  loss_texts_1: 0.2781  loss_ctrl_points_1: 0.1206  loss_bd_points_1: 0.15  loss_ce_2: 0.1686  loss_texts_2: 0.2508  loss_ctrl_points_2: 0.1155  loss_bd_points_2: 0.1502  loss_ce_3: 0.1644  loss_texts_3: 0.246  loss_ctrl_points_3: 0.1203  loss_bd_points_3: 0.157  loss_ce_4: 0.1603  loss_texts_4: 0.2268  loss_ctrl_points_4: 0.119  loss_bd_points_4: 0.1567  loss_ce_enc: 0.1682  loss_bezier_enc: 0.1066  total_loss: 4.459    time: 7.5901  last_time: 8.4703  data_time: 0.0037  last_data_time: 0.0038   lr: 6.3237e-06  max_mem: 7245M
[04/29 02:43:01] d2.utils.events INFO:  eta: 2 days, 13:30:10  iter: 79  loss_ce: 0.1569  loss_texts: 0.2264  loss_ctrl_points: 0.1188  loss_bd_points: 0.1561  loss_ce_0: 0.178  loss_texts_0: 0.3103  loss_ctrl_points_0: 0.1265  loss_bd_points_0: 0.1549  loss_ce_1: 0.1689  loss_texts_1: 0.2781  loss_ctrl_points_1: 0.1206  loss_bd_points_1: 0.15  loss_ce_2: 0.1686  loss_texts_2: 0.2508  loss_ctrl_points_2: 0.1155  loss_bd_points_2: 0.1502  loss_ce_3: 0.1644  loss_texts_3: 0.246  loss_ctrl_points_3: 0.1203  loss_bd_points_3: 0.157  loss_ce_4: 0.1603  loss_texts_4: 0.2268  loss_ctrl_points_4: 0.119  loss_bd_points_4: 0.1567  loss_ce_enc: 0.1682  loss_bezier_enc: 0.1066  total_loss: 4.459    time: 7.5962  last_time: 8.5305  data_time: 0.0037  last_data_time: 0.0038   lr: 6.3636e-06  max_mem: 7245M
[04/29 02:45:51] d2.utils.events INFO:  eta: 2 days, 15:23:33  iter: 99  loss_ce: 0.1536  loss_texts: 0.1709  loss_ctrl_points: 0.1006  loss_bd_points: 0.1431  loss_ce_0: 0.1742  loss_texts_0: 0.2681  loss_ctrl_points_0: 0.1066  loss_bd_points_0: 0.1517  loss_ce_1: 0.1668  loss_texts_1: 0.2214  loss_ctrl_points_1: 0.1043  loss_bd_points_1: 0.1445  loss_ce_2: 0.1637  loss_texts_2: 0.1884  loss_ctrl_points_2: 0.1032  loss_bd_points_2: 0.1447  loss_ce_3: 0.1598  loss_texts_3: 0.1762  loss_ctrl_points_3: 0.1022  loss_bd_points_3: 0.1427  loss_ce_4: 0.1554  loss_texts_4: 0.169  loss_ctrl_points_4: 0.102  loss_bd_points_4: 0.144  loss_ce_enc: 0.1608  loss_bezier_enc: 0.105  total_loss: 3.903    time: 7.7748  last_time: 7.6947  data_time: 0.0037  last_data_time: 0.0040   lr: 7.9221e-06  max_mem: 7275M
[04/29 02:45:51] d2.utils.events INFO:  eta: 2 days, 15:32:50  iter: 99  loss_ce: 0.1536  loss_texts: 0.1709  loss_ctrl_points: 0.1006  loss_bd_points: 0.1431  loss_ce_0: 0.1742  loss_texts_0: 0.2681  loss_ctrl_points_0: 0.1066  loss_bd_points_0: 0.1517  loss_ce_1: 0.1668  loss_texts_1: 0.2214  loss_ctrl_points_1: 0.1043  loss_bd_points_1: 0.1445  loss_ce_2: 0.1637  loss_texts_2: 0.1884  loss_ctrl_points_2: 0.1032  loss_bd_points_2: 0.1447  loss_ce_3: 0.1598  loss_texts_3: 0.1762  loss_ctrl_points_3: 0.1022  loss_bd_points_3: 0.1427  loss_ce_4: 0.1554  loss_texts_4: 0.169  loss_ctrl_points_4: 0.102  loss_bd_points_4: 0.144  loss_ce_enc: 0.1608  loss_bezier_enc: 0.105  total_loss: 3.903    time: 7.7747  last_time: 7.7595  data_time: 0.0037  last_data_time: 0.0040   lr: 7.962e-06  max_mem: 7275M
[04/29 02:48:44] d2.utils.events INFO:  eta: 2 days, 17:41:37  iter: 119  loss_ce: 0.1526  loss_texts: 0.2007  loss_ctrl_points: 0.1197  loss_bd_points: 0.1652  loss_ce_0: 0.1738  loss_texts_0: 0.3059  loss_ctrl_points_0: 0.1162  loss_bd_points_0: 0.1545  loss_ce_1: 0.1719  loss_texts_1: 0.2559  loss_ctrl_points_1: 0.1222  loss_bd_points_1: 0.17  loss_ce_2: 0.1592  loss_texts_2: 0.2241  loss_ctrl_points_2: 0.1243  loss_bd_points_2: 0.1842  loss_ce_3: 0.1556  loss_texts_3: 0.206  loss_ctrl_points_3: 0.1208  loss_bd_points_3: 0.1716  loss_ce_4: 0.1534  loss_texts_4: 0.2012  loss_ctrl_points_4: 0.1201  loss_bd_points_4: 0.1652  loss_ce_enc: 0.1698  loss_bezier_enc: 0.1105  total_loss: 4.508    time: 7.9250  last_time: 8.8220  data_time: 0.0037  last_data_time: 0.0037   lr: 9.5205e-06  max_mem: 7275M
[04/29 02:48:45] d2.utils.events INFO:  eta: 2 days, 17:44:24  iter: 119  loss_ce: 0.1526  loss_texts: 0.2007  loss_ctrl_points: 0.1197  loss_bd_points: 0.1652  loss_ce_0: 0.1738  loss_texts_0: 0.3059  loss_ctrl_points_0: 0.1162  loss_bd_points_0: 0.1545  loss_ce_1: 0.1719  loss_texts_1: 0.2559  loss_ctrl_points_1: 0.1222  loss_bd_points_1: 0.17  loss_ce_2: 0.1592  loss_texts_2: 0.2241  loss_ctrl_points_2: 0.1243  loss_bd_points_2: 0.1842  loss_ce_3: 0.1556  loss_texts_3: 0.206  loss_ctrl_points_3: 0.1208  loss_bd_points_3: 0.1716  loss_ce_4: 0.1534  loss_texts_4: 0.2012  loss_ctrl_points_4: 0.1201  loss_bd_points_4: 0.1652  loss_ce_enc: 0.1698  loss_bezier_enc: 0.1105  total_loss: 4.508    time: 7.9292  last_time: 8.9152  data_time: 0.0037  last_data_time: 0.0037   lr: 9.5604e-06  max_mem: 7275M
[04/29 02:51:37] d2.utils.events INFO:  eta: 2 days, 20:01:25  iter: 139  loss_ce: 0.1567  loss_texts: 0.1846  loss_ctrl_points: 0.1071  loss_bd_points: 0.1399  loss_ce_0: 0.1782  loss_texts_0: 0.277  loss_ctrl_points_0: 0.1104  loss_bd_points_0: 0.1526  loss_ce_1: 0.1742  loss_texts_1: 0.2338  loss_ctrl_points_1: 0.1105  loss_bd_points_1: 0.1404  loss_ce_2: 0.1673  loss_texts_2: 0.206  loss_ctrl_points_2: 0.1081  loss_bd_points_2: 0.141  loss_ce_3: 0.1664  loss_texts_3: 0.1893  loss_ctrl_points_3: 0.1134  loss_bd_points_3: 0.142  loss_ce_4: 0.1639  loss_texts_4: 0.1804  loss_ctrl_points_4: 0.1034  loss_bd_points_4: 0.1379  loss_ce_enc: 0.178  loss_bezier_enc: 0.1007  total_loss: 4.198    time: 8.0287  last_time: 8.8563  data_time: 0.0039  last_data_time: 0.0038   lr: 1e-05  max_mem: 7275M
[04/29 02:51:37] d2.utils.events INFO:  eta: 2 days, 20:03:53  iter: 139  loss_ce: 0.1567  loss_texts: 0.1846  loss_ctrl_points: 0.1071  loss_bd_points: 0.1399  loss_ce_0: 0.1782  loss_texts_0: 0.277  loss_ctrl_points_0: 0.1104  loss_bd_points_0: 0.1526  loss_ce_1: 0.1742  loss_texts_1: 0.2338  loss_ctrl_points_1: 0.1105  loss_bd_points_1: 0.1404  loss_ce_2: 0.1673  loss_texts_2: 0.206  loss_ctrl_points_2: 0.1081  loss_bd_points_2: 0.141  loss_ce_3: 0.1664  loss_texts_3: 0.1893  loss_ctrl_points_3: 0.1134  loss_bd_points_3: 0.142  loss_ce_4: 0.1639  loss_texts_4: 0.1804  loss_ctrl_points_4: 0.1034  loss_bd_points_4: 0.1379  loss_ce_enc: 0.178  loss_bezier_enc: 0.1007  total_loss: 4.198    time: 8.0319  last_time: 8.9201  data_time: 0.0039  last_data_time: 0.0038   lr: 1e-05  max_mem: 7275M
[04/29 02:54:23] d2.utils.events INFO:  eta: 2 days, 20:03:39  iter: 159  loss_ce: 0.1602  loss_texts: 0.213  loss_ctrl_points: 0.1075  loss_bd_points: 0.1387  loss_ce_0: 0.1758  loss_texts_0: 0.3028  loss_ctrl_points_0: 0.111  loss_bd_points_0: 0.1625  loss_ce_1: 0.1686  loss_texts_1: 0.2736  loss_ctrl_points_1: 0.1088  loss_bd_points_1: 0.1541  loss_ce_2: 0.1678  loss_texts_2: 0.24  loss_ctrl_points_2: 0.1075  loss_bd_points_2: 0.1499  loss_ce_3: 0.1699  loss_texts_3: 0.2242  loss_ctrl_points_3: 0.112  loss_bd_points_3: 0.1498  loss_ce_4: 0.1655  loss_texts_4: 0.216  loss_ctrl_points_4: 0.111  loss_bd_points_4: 0.152  loss_ce_enc: 0.1633  loss_bezier_enc: 0.1075  total_loss: 4.432    time: 8.0663  last_time: 7.6266  data_time: 0.0037  last_data_time: 0.0034   lr: 1e-05  max_mem: 7275M
[04/29 02:54:23] d2.utils.events INFO:  eta: 2 days, 20:03:38  iter: 159  loss_ce: 0.1602  loss_texts: 0.213  loss_ctrl_points: 0.1075  loss_bd_points: 0.1387  loss_ce_0: 0.1758  loss_texts_0: 0.3028  loss_ctrl_points_0: 0.111  loss_bd_points_0: 0.1625  loss_ce_1: 0.1686  loss_texts_1: 0.2736  loss_ctrl_points_1: 0.1088  loss_bd_points_1: 0.1541  loss_ce_2: 0.1678  loss_texts_2: 0.24  loss_ctrl_points_2: 0.1075  loss_bd_points_2: 0.1499  loss_ce_3: 0.1699  loss_texts_3: 0.2242  loss_ctrl_points_3: 0.112  loss_bd_points_3: 0.1498  loss_ce_4: 0.1655  loss_texts_4: 0.216  loss_ctrl_points_4: 0.111  loss_bd_points_4: 0.152  loss_ce_enc: 0.1633  loss_bezier_enc: 0.1075  total_loss: 4.432    time: 8.0652  last_time: 7.6886  data_time: 0.0037  last_data_time: 0.0034   lr: 1e-05  max_mem: 7275M
[04/29 02:57:04] d2.utils.events INFO:  eta: 2 days, 19:55:43  iter: 179  loss_ce: 0.1543  loss_texts: 0.1959  loss_ctrl_points: 0.1105  loss_bd_points: 0.1459  loss_ce_0: 0.1782  loss_texts_0: 0.314  loss_ctrl_points_0: 0.1229  loss_bd_points_0: 0.1524  loss_ce_1: 0.1674  loss_texts_1: 0.2631  loss_ctrl_points_1: 0.1125  loss_bd_points_1: 0.1516  loss_ce_2: 0.1592  loss_texts_2: 0.2181  loss_ctrl_points_2: 0.1116  loss_bd_points_2: 0.148  loss_ce_3: 0.1601  loss_texts_3: 0.2048  loss_ctrl_points_3: 0.1103  loss_bd_points_3: 0.1475  loss_ce_4: 0.1543  loss_texts_4: 0.1975  loss_ctrl_points_4: 0.1089  loss_bd_points_4: 0.1449  loss_ce_enc: 0.1678  loss_bezier_enc: 0.1068  total_loss: 4.258    time: 8.0635  last_time: 6.9200  data_time: 0.0036  last_data_time: 0.0035   lr: 1e-05  max_mem: 7275M
[04/29 02:57:04] d2.utils.events INFO:  eta: 2 days, 19:50:51  iter: 179  loss_ce: 0.1543  loss_texts: 0.1959  loss_ctrl_points: 0.1105  loss_bd_points: 0.1459  loss_ce_0: 0.1782  loss_texts_0: 0.314  loss_ctrl_points_0: 0.1229  loss_bd_points_0: 0.1524  loss_ce_1: 0.1674  loss_texts_1: 0.2631  loss_ctrl_points_1: 0.1125  loss_bd_points_1: 0.1516  loss_ce_2: 0.1592  loss_texts_2: 0.2181  loss_ctrl_points_2: 0.1116  loss_bd_points_2: 0.148  loss_ce_3: 0.1601  loss_texts_3: 0.2048  loss_ctrl_points_3: 0.1103  loss_bd_points_3: 0.1475  loss_ce_4: 0.1543  loss_texts_4: 0.1975  loss_ctrl_points_4: 0.1089  loss_bd_points_4: 0.1449  loss_ce_enc: 0.1678  loss_bezier_enc: 0.1068  total_loss: 4.258    time: 8.0605  last_time: 6.9668  data_time: 0.0036  last_data_time: 0.0035   lr: 1e-05  max_mem: 7275M
[04/29 02:59:42] d2.utils.events INFO:  eta: 2 days, 18:50:19  iter: 199  loss_ce: 0.1566  loss_texts: 0.1838  loss_ctrl_points: 0.1196  loss_bd_points: 0.1736  loss_ce_0: 0.1725  loss_texts_0: 0.2978  loss_ctrl_points_0: 0.1337  loss_bd_points_0: 0.1769  loss_ce_1: 0.1682  loss_texts_1: 0.2545  loss_ctrl_points_1: 0.1275  loss_bd_points_1: 0.1733  loss_ce_2: 0.1649  loss_texts_2: 0.2088  loss_ctrl_points_2: 0.1202  loss_bd_points_2: 0.1761  loss_ce_3: 0.1624  loss_texts_3: 0.1952  loss_ctrl_points_3: 0.1244  loss_bd_points_3: 0.1757  loss_ce_4: 0.1595  loss_texts_4: 0.1925  loss_ctrl_points_4: 0.1147  loss_bd_points_4: 0.1663  loss_ce_enc: 0.1689  loss_bezier_enc: 0.1053  total_loss: 4.224    time: 8.0454  last_time: 7.8767  data_time: 0.0037  last_data_time: 0.0036   lr: 1e-05  max_mem: 7275M
[04/29 02:59:42] d2.utils.events INFO:  eta: 2 days, 18:50:01  iter: 199  loss_ce: 0.1566  loss_texts: 0.1838  loss_ctrl_points: 0.1196  loss_bd_points: 0.1736  loss_ce_0: 0.1725  loss_texts_0: 0.2978  loss_ctrl_points_0: 0.1337  loss_bd_points_0: 0.1769  loss_ce_1: 0.1682  loss_texts_1: 0.2545  loss_ctrl_points_1: 0.1275  loss_bd_points_1: 0.1733  loss_ce_2: 0.1649  loss_texts_2: 0.2088  loss_ctrl_points_2: 0.1202  loss_bd_points_2: 0.1761  loss_ce_3: 0.1624  loss_texts_3: 0.1952  loss_ctrl_points_3: 0.1244  loss_bd_points_3: 0.1757  loss_ce_4: 0.1595  loss_texts_4: 0.1925  loss_ctrl_points_4: 0.1147  loss_bd_points_4: 0.1663  loss_ce_enc: 0.1689  loss_bezier_enc: 0.1053  total_loss: 4.224    time: 8.0451  last_time: 7.9436  data_time: 0.0037  last_data_time: 0.0036   lr: 1e-05  max_mem: 7275M
[04/29 03:02:19] d2.utils.events INFO:  eta: 2 days, 18:27:59  iter: 219  loss_ce: 0.1577  loss_texts: 0.2072  loss_ctrl_points: 0.1134  loss_bd_points: 0.1462  loss_ce_0: 0.182  loss_texts_0: 0.3162  loss_ctrl_points_0: 0.1089  loss_bd_points_0: 0.1403  loss_ce_1: 0.1717  loss_texts_1: 0.2736  loss_ctrl_points_1: 0.1098  loss_bd_points_1: 0.145  loss_ce_2: 0.1636  loss_texts_2: 0.2331  loss_ctrl_points_2: 0.1151  loss_bd_points_2: 0.1493  loss_ce_3: 0.16  loss_texts_3: 0.2164  loss_ctrl_points_3: 0.1148  loss_bd_points_3: 0.1468  loss_ce_4: 0.1604  loss_texts_4: 0.21  loss_ctrl_points_4: 0.1145  loss_bd_points_4: 0.1467  loss_ce_enc: 0.1696  loss_bezier_enc: 0.1052  total_loss: 4.427    time: 8.0259  last_time: 7.9566  data_time: 0.0036  last_data_time: 0.0034   lr: 1e-05  max_mem: 7275M
[04/29 03:02:19] d2.utils.events INFO:  eta: 2 days, 18:27:51  iter: 219  loss_ce: 0.1577  loss_texts: 0.2072  loss_ctrl_points: 0.1134  loss_bd_points: 0.1462  loss_ce_0: 0.182  loss_texts_0: 0.3162  loss_ctrl_points_0: 0.1089  loss_bd_points_0: 0.1403  loss_ce_1: 0.1717  loss_texts_1: 0.2736  loss_ctrl_points_1: 0.1098  loss_bd_points_1: 0.145  loss_ce_2: 0.1636  loss_texts_2: 0.2331  loss_ctrl_points_2: 0.1151  loss_bd_points_2: 0.1493  loss_ce_3: 0.16  loss_texts_3: 0.2164  loss_ctrl_points_3: 0.1148  loss_bd_points_3: 0.1468  loss_ce_4: 0.1604  loss_texts_4: 0.21  loss_ctrl_points_4: 0.1145  loss_bd_points_4: 0.1467  loss_ce_enc: 0.1696  loss_bezier_enc: 0.1052  total_loss: 4.427    time: 8.0259  last_time: 8.0079  data_time: 0.0036  last_data_time: 0.0034   lr: 1e-05  max_mem: 7275M
[04/29 03:04:49] d2.utils.events INFO:  eta: 2 days, 17:47:58  iter: 239  loss_ce: 0.1636  loss_texts: 0.2154  loss_ctrl_points: 0.1266  loss_bd_points: 0.169  loss_ce_0: 0.1896  loss_texts_0: 0.3192  loss_ctrl_points_0: 0.1266  loss_bd_points_0: 0.1752  loss_ce_1: 0.1808  loss_texts_1: 0.2631  loss_ctrl_points_1: 0.132  loss_bd_points_1: 0.1751  loss_ce_2: 0.1734  loss_texts_2: 0.2345  loss_ctrl_points_2: 0.1338  loss_bd_points_2: 0.1744  loss_ce_3: 0.1712  loss_texts_3: 0.2157  loss_ctrl_points_3: 0.1303  loss_bd_points_3: 0.1764  loss_ce_4: 0.1659  loss_texts_4: 0.2148  loss_ctrl_points_4: 0.1282  loss_bd_points_4: 0.1682  loss_ce_enc: 0.1853  loss_bezier_enc: 0.1018  total_loss: 4.642    time: 7.9821  last_time: 6.8355  data_time: 0.0036  last_data_time: 0.0035   lr: 1e-05  max_mem: 7275M
[04/29 03:04:49] d2.utils.events INFO:  eta: 2 days, 17:47:18  iter: 239  loss_ce: 0.1636  loss_texts: 0.2154  loss_ctrl_points: 0.1266  loss_bd_points: 0.169  loss_ce_0: 0.1896  loss_texts_0: 0.3192  loss_ctrl_points_0: 0.1266  loss_bd_points_0: 0.1752  loss_ce_1: 0.1808  loss_texts_1: 0.2631  loss_ctrl_points_1: 0.132  loss_bd_points_1: 0.1751  loss_ce_2: 0.1734  loss_texts_2: 0.2345  loss_ctrl_points_2: 0.1338  loss_bd_points_2: 0.1744  loss_ce_3: 0.1712  loss_texts_3: 0.2157  loss_ctrl_points_3: 0.1303  loss_bd_points_3: 0.1764  loss_ce_4: 0.1659  loss_texts_4: 0.2148  loss_ctrl_points_4: 0.1282  loss_bd_points_4: 0.1682  loss_ce_enc: 0.1853  loss_bezier_enc: 0.1018  total_loss: 4.642    time: 7.9799  last_time: 6.9226  data_time: 0.0036  last_data_time: 0.0035   lr: 1e-05  max_mem: 7275M
[04/29 03:07:18] d2.utils.events INFO:  eta: 2 days, 17:29:10  iter: 259  loss_ce: 0.1472  loss_texts: 0.1955  loss_ctrl_points: 0.1225  loss_bd_points: 0.1774  loss_ce_0: 0.171  loss_texts_0: 0.2951  loss_ctrl_points_0: 0.1221  loss_bd_points_0: 0.1765  loss_ce_1: 0.1594  loss_texts_1: 0.2509  loss_ctrl_points_1: 0.1261  loss_bd_points_1: 0.1796  loss_ce_2: 0.154  loss_texts_2: 0.2218  loss_ctrl_points_2: 0.1219  loss_bd_points_2: 0.1738  loss_ce_3: 0.1546  loss_texts_3: 0.1932  loss_ctrl_points_3: 0.1249  loss_bd_points_3: 0.1743  loss_ce_4: 0.1507  loss_texts_4: 0.1958  loss_ctrl_points_4: 0.1224  loss_bd_points_4: 0.1741  loss_ce_enc: 0.1682  loss_bezier_enc: 0.1049  total_loss: 4.636    time: 7.9357  last_time: 9.6012  data_time: 0.0037  last_data_time: 0.0041   lr: 1e-05  max_mem: 7275M
[04/29 03:07:18] d2.utils.events INFO:  eta: 2 days, 17:29:21  iter: 259  loss_ce: 0.1472  loss_texts: 0.1955  loss_ctrl_points: 0.1225  loss_bd_points: 0.1774  loss_ce_0: 0.171  loss_texts_0: 0.2951  loss_ctrl_points_0: 0.1221  loss_bd_points_0: 0.1765  loss_ce_1: 0.1594  loss_texts_1: 0.2509  loss_ctrl_points_1: 0.1261  loss_bd_points_1: 0.1796  loss_ce_2: 0.154  loss_texts_2: 0.2218  loss_ctrl_points_2: 0.1219  loss_bd_points_2: 0.1738  loss_ce_3: 0.1546  loss_texts_3: 0.1932  loss_ctrl_points_3: 0.1249  loss_bd_points_3: 0.1743  loss_ce_4: 0.1507  loss_texts_4: 0.1958  loss_ctrl_points_4: 0.1224  loss_bd_points_4: 0.1741  loss_ce_enc: 0.1682  loss_bezier_enc: 0.1049  total_loss: 4.636    time: 7.9390  last_time: 9.6530  data_time: 0.0037  last_data_time: 0.0041   lr: 1e-05  max_mem: 7275M
[04/29 03:10:07] d2.utils.events INFO:  eta: 2 days, 17:51:17  iter: 279  loss_ce: 0.161  loss_texts: 0.2293  loss_ctrl_points: 0.1083  loss_bd_points: 0.1397  loss_ce_0: 0.1894  loss_texts_0: 0.34  loss_ctrl_points_0: 0.1215  loss_bd_points_0: 0.1485  loss_ce_1: 0.1758  loss_texts_1: 0.2983  loss_ctrl_points_1: 0.1175  loss_bd_points_1: 0.1426  loss_ce_2: 0.1697  loss_texts_2: 0.2686  loss_ctrl_points_2: 0.1191  loss_bd_points_2: 0.1484  loss_ce_3: 0.1691  loss_texts_3: 0.2457  loss_ctrl_points_3: 0.1132  loss_bd_points_3: 0.1423  loss_ce_4: 0.1674  loss_texts_4: 0.2302  loss_ctrl_points_4: 0.1143  loss_bd_points_4: 0.1422  loss_ce_enc: 0.1846  loss_bezier_enc: 0.1036  total_loss: 4.639    time: 7.9742  last_time: 8.4759  data_time: 0.0039  last_data_time: 0.0039   lr: 1e-05  max_mem: 7275M
[04/29 03:10:07] d2.utils.events INFO:  eta: 2 days, 17:58:55  iter: 279  loss_ce: 0.161  loss_texts: 0.2293  loss_ctrl_points: 0.1083  loss_bd_points: 0.1397  loss_ce_0: 0.1894  loss_texts_0: 0.34  loss_ctrl_points_0: 0.1215  loss_bd_points_0: 0.1485  loss_ce_1: 0.1758  loss_texts_1: 0.2983  loss_ctrl_points_1: 0.1175  loss_bd_points_1: 0.1426  loss_ce_2: 0.1697  loss_texts_2: 0.2686  loss_ctrl_points_2: 0.1191  loss_bd_points_2: 0.1484  loss_ce_3: 0.1691  loss_texts_3: 0.2457  loss_ctrl_points_3: 0.1132  loss_bd_points_3: 0.1423  loss_ce_4: 0.1674  loss_texts_4: 0.2302  loss_ctrl_points_4: 0.1143  loss_bd_points_4: 0.1422  loss_ce_enc: 0.1846  loss_bezier_enc: 0.1036  total_loss: 4.639    time: 7.9752  last_time: 8.5353  data_time: 0.0039  last_data_time: 0.0039   lr: 1e-05  max_mem: 7275M
[04/29 03:12:58] d2.utils.events INFO:  eta: 2 days, 18:30:14  iter: 299  loss_ce: 0.1574  loss_texts: 0.2022  loss_ctrl_points: 0.1263  loss_bd_points: 0.1775  loss_ce_0: 0.176  loss_texts_0: 0.3122  loss_ctrl_points_0: 0.124  loss_bd_points_0: 0.1706  loss_ce_1: 0.1696  loss_texts_1: 0.2632  loss_ctrl_points_1: 0.1287  loss_bd_points_1: 0.1791  loss_ce_2: 0.1641  loss_texts_2: 0.2348  loss_ctrl_points_2: 0.1277  loss_bd_points_2: 0.1778  loss_ce_3: 0.1655  loss_texts_3: 0.2129  loss_ctrl_points_3: 0.1281  loss_bd_points_3: 0.1775  loss_ce_4: 0.1636  loss_texts_4: 0.2021  loss_ctrl_points_4: 0.1279  loss_bd_points_4: 0.1801  loss_ce_enc: 0.1671  loss_bezier_enc: 0.1057  total_loss: 4.486    time: 8.0128  last_time: 8.8727  data_time: 0.0038  last_data_time: 0.0040   lr: 1e-05  max_mem: 7275M
[04/29 03:12:58] d2.utils.events INFO:  eta: 2 days, 18:30:18  iter: 299  loss_ce: 0.1574  loss_texts: 0.2022  loss_ctrl_points: 0.1263  loss_bd_points: 0.1775  loss_ce_0: 0.176  loss_texts_0: 0.3122  loss_ctrl_points_0: 0.124  loss_bd_points_0: 0.1706  loss_ce_1: 0.1696  loss_texts_1: 0.2632  loss_ctrl_points_1: 0.1287  loss_bd_points_1: 0.1791  loss_ce_2: 0.1641  loss_texts_2: 0.2348  loss_ctrl_points_2: 0.1277  loss_bd_points_2: 0.1778  loss_ce_3: 0.1655  loss_texts_3: 0.2129  loss_ctrl_points_3: 0.1281  loss_bd_points_3: 0.1775  loss_ce_4: 0.1636  loss_texts_4: 0.2021  loss_ctrl_points_4: 0.1279  loss_bd_points_4: 0.1801  loss_ce_enc: 0.1671  loss_bezier_enc: 0.1057  total_loss: 4.486    time: 8.0143  last_time: 8.9252  data_time: 0.0038  last_data_time: 0.0040   lr: 1e-05  max_mem: 7275M
[04/29 03:15:55] d2.utils.events INFO:  eta: 2 days, 19:04:41  iter: 319  loss_ce: 0.1557  loss_texts: 0.2205  loss_ctrl_points: 0.1  loss_bd_points: 0.1399  loss_ce_0: 0.183  loss_texts_0: 0.3482  loss_ctrl_points_0: 0.1055  loss_bd_points_0: 0.1607  loss_ce_1: 0.1761  loss_texts_1: 0.2672  loss_ctrl_points_1: 0.09783  loss_bd_points_1: 0.1431  loss_ce_2: 0.1688  loss_texts_2: 0.237  loss_ctrl_points_2: 0.1015  loss_bd_points_2: 0.1447  loss_ce_3: 0.1648  loss_texts_3: 0.217  loss_ctrl_points_3: 0.1009  loss_bd_points_3: 0.1438  loss_ce_4: 0.1608  loss_texts_4: 0.2218  loss_ctrl_points_4: 0.1039  loss_bd_points_4: 0.1393  loss_ce_enc: 0.1693  loss_bezier_enc: 0.1046  total_loss: 4.309    time: 8.0628  last_time: 9.7244  data_time: 0.0039  last_data_time: 0.0042   lr: 1e-05  max_mem: 7275M
[04/29 03:15:55] d2.utils.events INFO:  eta: 2 days, 19:06:55  iter: 319  loss_ce: 0.1557  loss_texts: 0.2205  loss_ctrl_points: 0.1  loss_bd_points: 0.1399  loss_ce_0: 0.183  loss_texts_0: 0.3482  loss_ctrl_points_0: 0.1055  loss_bd_points_0: 0.1607  loss_ce_1: 0.1761  loss_texts_1: 0.2672  loss_ctrl_points_1: 0.09783  loss_bd_points_1: 0.1431  loss_ce_2: 0.1688  loss_texts_2: 0.237  loss_ctrl_points_2: 0.1015  loss_bd_points_2: 0.1447  loss_ce_3: 0.1648  loss_texts_3: 0.217  loss_ctrl_points_3: 0.1009  loss_bd_points_3: 0.1438  loss_ce_4: 0.1608  loss_texts_4: 0.2218  loss_ctrl_points_4: 0.1039  loss_bd_points_4: 0.1393  loss_ce_enc: 0.1693  loss_bezier_enc: 0.1046  total_loss: 4.309    time: 8.0655  last_time: 9.7858  data_time: 0.0039  last_data_time: 0.0042   lr: 1e-05  max_mem: 7275M
[04/29 03:18:54] d2.utils.events INFO:  eta: 2 days, 19:25:10  iter: 339  loss_ce: 0.1525  loss_texts: 0.2187  loss_ctrl_points: 0.148  loss_bd_points: 0.1893  loss_ce_0: 0.1897  loss_texts_0: 0.3336  loss_ctrl_points_0: 0.1423  loss_bd_points_0: 0.1929  loss_ce_1: 0.1688  loss_texts_1: 0.2814  loss_ctrl_points_1: 0.1458  loss_bd_points_1: 0.1861  loss_ce_2: 0.1595  loss_texts_2: 0.2414  loss_ctrl_points_2: 0.1406  loss_bd_points_2: 0.1772  loss_ce_3: 0.154  loss_texts_3: 0.2322  loss_ctrl_points_3: 0.1439  loss_bd_points_3: 0.176  loss_ce_4: 0.1542  loss_texts_4: 0.2235  loss_ctrl_points_4: 0.1482  loss_bd_points_4: 0.1887  loss_ce_enc: 0.1745  loss_bezier_enc: 0.1138  total_loss: 4.819    time: 8.1149  last_time: 10.8082  data_time: 0.0038  last_data_time: 0.0042   lr: 1e-05  max_mem: 7275M
[04/29 03:18:54] d2.utils.events INFO:  eta: 2 days, 19:26:20  iter: 339  loss_ce: 0.1525  loss_texts: 0.2187  loss_ctrl_points: 0.148  loss_bd_points: 0.1893  loss_ce_0: 0.1897  loss_texts_0: 0.3336  loss_ctrl_points_0: 0.1423  loss_bd_points_0: 0.1929  loss_ce_1: 0.1688  loss_texts_1: 0.2814  loss_ctrl_points_1: 0.1458  loss_bd_points_1: 0.1861  loss_ce_2: 0.1595  loss_texts_2: 0.2414  loss_ctrl_points_2: 0.1406  loss_bd_points_2: 0.1772  loss_ce_3: 0.154  loss_texts_3: 0.2322  loss_ctrl_points_3: 0.1439  loss_bd_points_3: 0.176  loss_ce_4: 0.1542  loss_texts_4: 0.2235  loss_ctrl_points_4: 0.1482  loss_bd_points_4: 0.1887  loss_ce_enc: 0.1745  loss_bezier_enc: 0.1138  total_loss: 4.819    time: 8.1190  last_time: 10.8720  data_time: 0.0038  last_data_time: 0.0042   lr: 1e-05  max_mem: 7275M
[04/29 03:21:45] d2.utils.events INFO:  eta: 2 days, 19:24:46  iter: 359  loss_ce: 0.164  loss_texts: 0.1863  loss_ctrl_points: 0.1096  loss_bd_points: 0.1476  loss_ce_0: 0.178  loss_texts_0: 0.2791  loss_ctrl_points_0: 0.1097  loss_bd_points_0: 0.1499  loss_ce_1: 0.1794  loss_texts_1: 0.2343  loss_ctrl_points_1: 0.1103  loss_bd_points_1: 0.1432  loss_ce_2: 0.167  loss_texts_2: 0.2085  loss_ctrl_points_2: 0.1104  loss_bd_points_2: 0.1456  loss_ce_3: 0.1696  loss_texts_3: 0.1969  loss_ctrl_points_3: 0.1119  loss_bd_points_3: 0.1468  loss_ce_4: 0.1649  loss_texts_4: 0.1862  loss_ctrl_points_4: 0.1096  loss_bd_points_4: 0.1464  loss_ce_enc: 0.1645  loss_bezier_enc: 0.1055  total_loss: 4.051    time: 8.1431  last_time: 7.0255  data_time: 0.0039  last_data_time: 0.0039   lr: 1e-05  max_mem: 7275M
[04/29 03:21:45] d2.utils.events INFO:  eta: 2 days, 19:23:36  iter: 359  loss_ce: 0.164  loss_texts: 0.1863  loss_ctrl_points: 0.1096  loss_bd_points: 0.1476  loss_ce_0: 0.178  loss_texts_0: 0.2791  loss_ctrl_points_0: 0.1097  loss_bd_points_0: 0.1499  loss_ce_1: 0.1794  loss_texts_1: 0.2343  loss_ctrl_points_1: 0.1103  loss_bd_points_1: 0.1432  loss_ce_2: 0.167  loss_texts_2: 0.2085  loss_ctrl_points_2: 0.1104  loss_bd_points_2: 0.1456  loss_ce_3: 0.1696  loss_texts_3: 0.1969  loss_ctrl_points_3: 0.1119  loss_bd_points_3: 0.1468  loss_ce_4: 0.1649  loss_texts_4: 0.1862  loss_ctrl_points_4: 0.1096  loss_bd_points_4: 0.1464  loss_ce_enc: 0.1645  loss_bezier_enc: 0.1055  total_loss: 4.051    time: 8.1416  last_time: 7.0885  data_time: 0.0039  last_data_time: 0.0039   lr: 1e-05  max_mem: 7275M
[04/29 03:24:21] d2.utils.events INFO:  eta: 2 days, 19:05:23  iter: 379  loss_ce: 0.1544  loss_texts: 0.2296  loss_ctrl_points: 0.1195  loss_bd_points: 0.1622  loss_ce_0: 0.1842  loss_texts_0: 0.3336  loss_ctrl_points_0: 0.1199  loss_bd_points_0: 0.1708  loss_ce_1: 0.1709  loss_texts_1: 0.3015  loss_ctrl_points_1: 0.1136  loss_bd_points_1: 0.1541  loss_ce_2: 0.1657  loss_texts_2: 0.2669  loss_ctrl_points_2: 0.1146  loss_bd_points_2: 0.1573  loss_ce_3: 0.162  loss_texts_3: 0.2468  loss_ctrl_points_3: 0.1206  loss_bd_points_3: 0.1647  loss_ce_4: 0.1569  loss_texts_4: 0.2374  loss_ctrl_points_4: 0.12  loss_bd_points_4: 0.1632  loss_ce_enc: 0.1752  loss_bezier_enc: 0.1052  total_loss: 4.694    time: 8.1238  last_time: 8.2816  data_time: 0.0036  last_data_time: 0.0039   lr: 1e-05  max_mem: 7275M
[04/29 03:24:21] d2.utils.events INFO:  eta: 2 days, 19:06:47  iter: 379  loss_ce: 0.1544  loss_texts: 0.2296  loss_ctrl_points: 0.1195  loss_bd_points: 0.1622  loss_ce_0: 0.1842  loss_texts_0: 0.3336  loss_ctrl_points_0: 0.1199  loss_bd_points_0: 0.1708  loss_ce_1: 0.1709  loss_texts_1: 0.3015  loss_ctrl_points_1: 0.1136  loss_bd_points_1: 0.1541  loss_ce_2: 0.1657  loss_texts_2: 0.2669  loss_ctrl_points_2: 0.1146  loss_bd_points_2: 0.1573  loss_ce_3: 0.162  loss_texts_3: 0.2468  loss_ctrl_points_3: 0.1206  loss_bd_points_3: 0.1647  loss_ce_4: 0.1569  loss_texts_4: 0.2374  loss_ctrl_points_4: 0.12  loss_bd_points_4: 0.1632  loss_ce_enc: 0.1752  loss_bezier_enc: 0.1052  total_loss: 4.694    time: 8.1241  last_time: 8.3552  data_time: 0.0036  last_data_time: 0.0039   lr: 1e-05  max_mem: 7275M
[04/29 03:26:59] d2.utils.events INFO:  eta: 2 days, 19:01:21  iter: 399  loss_ce: 0.1465  loss_texts: 0.1994  loss_ctrl_points: 0.1236  loss_bd_points: 0.1559  loss_ce_0: 0.1709  loss_texts_0: 0.2912  loss_ctrl_points_0: 0.1165  loss_bd_points_0: 0.1525  loss_ce_1: 0.1754  loss_texts_1: 0.2423  loss_ctrl_points_1: 0.1193  loss_bd_points_1: 0.1436  loss_ce_2: 0.1593  loss_texts_2: 0.2126  loss_ctrl_points_2: 0.1199  loss_bd_points_2: 0.1457  loss_ce_3: 0.1502  loss_texts_3: 0.2085  loss_ctrl_points_3: 0.1181  loss_bd_points_3: 0.1509  loss_ce_4: 0.1492  loss_texts_4: 0.1998  loss_ctrl_points_4: 0.1181  loss_bd_points_4: 0.1546  loss_ce_enc: 0.1574  loss_bezier_enc: 0.1081  total_loss: 4.299    time: 8.1134  last_time: 7.2485  data_time: 0.0036  last_data_time: 0.0032   lr: 1e-05  max_mem: 7275M
[04/29 03:26:59] d2.utils.events INFO:  eta: 2 days, 18:59:58  iter: 399  loss_ce: 0.1465  loss_texts: 0.1994  loss_ctrl_points: 0.1236  loss_bd_points: 0.1559  loss_ce_0: 0.1709  loss_texts_0: 0.2912  loss_ctrl_points_0: 0.1165  loss_bd_points_0: 0.1525  loss_ce_1: 0.1754  loss_texts_1: 0.2423  loss_ctrl_points_1: 0.1193  loss_bd_points_1: 0.1436  loss_ce_2: 0.1593  loss_texts_2: 0.2126  loss_ctrl_points_2: 0.1199  loss_bd_points_2: 0.1457  loss_ce_3: 0.1502  loss_texts_3: 0.2085  loss_ctrl_points_3: 0.1181  loss_bd_points_3: 0.1509  loss_ce_4: 0.1492  loss_texts_4: 0.1998  loss_ctrl_points_4: 0.1181  loss_bd_points_4: 0.1546  loss_ce_enc: 0.1574  loss_bezier_enc: 0.1081  total_loss: 4.299    time: 8.1124  last_time: 7.2950  data_time: 0.0036  last_data_time: 0.0032   lr: 1e-05  max_mem: 7275M
[04/29 03:29:24] d2.utils.events INFO:  eta: 2 days, 18:28:26  iter: 419  loss_ce: 0.1529  loss_texts: 0.1965  loss_ctrl_points: 0.1177  loss_bd_points: 0.1618  loss_ce_0: 0.1873  loss_texts_0: 0.2937  loss_ctrl_points_0: 0.1139  loss_bd_points_0: 0.1549  loss_ce_1: 0.1773  loss_texts_1: 0.2456  loss_ctrl_points_1: 0.114  loss_bd_points_1: 0.1587  loss_ce_2: 0.168  loss_texts_2: 0.2183  loss_ctrl_points_2: 0.1181  loss_bd_points_2: 0.1603  loss_ce_3: 0.1636  loss_texts_3: 0.2088  loss_ctrl_points_3: 0.1192  loss_bd_points_3: 0.1601  loss_ce_4: 0.1554  loss_texts_4: 0.2001  loss_ctrl_points_4: 0.1186  loss_bd_points_4: 0.1617  loss_ce_enc: 0.169  loss_bezier_enc: 0.1088  total_loss: 4.472    time: 8.0716  last_time: 6.7844  data_time: 0.0037  last_data_time: 0.0034   lr: 1e-05  max_mem: 7275M
[04/29 03:29:24] d2.utils.events INFO:  eta: 2 days, 18:24:42  iter: 419  loss_ce: 0.1529  loss_texts: 0.1965  loss_ctrl_points: 0.1177  loss_bd_points: 0.1618  loss_ce_0: 0.1873  loss_texts_0: 0.2937  loss_ctrl_points_0: 0.1139  loss_bd_points_0: 0.1549  loss_ce_1: 0.1773  loss_texts_1: 0.2456  loss_ctrl_points_1: 0.114  loss_bd_points_1: 0.1587  loss_ce_2: 0.168  loss_texts_2: 0.2183  loss_ctrl_points_2: 0.1181  loss_bd_points_2: 0.1603  loss_ce_3: 0.1636  loss_texts_3: 0.2088  loss_ctrl_points_3: 0.1192  loss_bd_points_3: 0.1601  loss_ce_4: 0.1554  loss_texts_4: 0.2001  loss_ctrl_points_4: 0.1186  loss_bd_points_4: 0.1617  loss_ce_enc: 0.169  loss_bezier_enc: 0.1088  total_loss: 4.472    time: 8.0702  last_time: 6.8440  data_time: 0.0037  last_data_time: 0.0034   lr: 1e-05  max_mem: 7275M
[04/29 03:31:38] d2.utils.events INFO:  eta: 2 days, 18:06:17  iter: 439  loss_ce: 0.1485  loss_texts: 0.1828  loss_ctrl_points: 0.1255  loss_bd_points: 0.1655  loss_ce_0: 0.1926  loss_texts_0: 0.3045  loss_ctrl_points_0: 0.1135  loss_bd_points_0: 0.1727  loss_ce_1: 0.1771  loss_texts_1: 0.2495  loss_ctrl_points_1: 0.125  loss_bd_points_1: 0.1731  loss_ce_2: 0.1614  loss_texts_2: 0.2171  loss_ctrl_points_2: 0.1276  loss_bd_points_2: 0.177  loss_ce_3: 0.156  loss_texts_3: 0.2013  loss_ctrl_points_3: 0.1267  loss_bd_points_3: 0.1719  loss_ce_4: 0.1515  loss_texts_4: 0.1855  loss_ctrl_points_4: 0.1261  loss_bd_points_4: 0.1684  loss_ce_enc: 0.1785  loss_bezier_enc: 0.1134  total_loss: 4.333    time: 8.0086  last_time: 7.3920  data_time: 0.0036  last_data_time: 0.0037   lr: 1e-05  max_mem: 7275M
[04/29 03:31:38] d2.utils.events INFO:  eta: 2 days, 18:06:04  iter: 439  loss_ce: 0.1485  loss_texts: 0.1828  loss_ctrl_points: 0.1255  loss_bd_points: 0.1655  loss_ce_0: 0.1926  loss_texts_0: 0.3045  loss_ctrl_points_0: 0.1135  loss_bd_points_0: 0.1727  loss_ce_1: 0.1771  loss_texts_1: 0.2495  loss_ctrl_points_1: 0.125  loss_bd_points_1: 0.1731  loss_ce_2: 0.1614  loss_texts_2: 0.2171  loss_ctrl_points_2: 0.1276  loss_bd_points_2: 0.177  loss_ce_3: 0.156  loss_texts_3: 0.2013  loss_ctrl_points_3: 0.1267  loss_bd_points_3: 0.1719  loss_ce_4: 0.1515  loss_texts_4: 0.1855  loss_ctrl_points_4: 0.1261  loss_bd_points_4: 0.1684  loss_ce_enc: 0.1785  loss_bezier_enc: 0.1134  total_loss: 4.333    time: 8.0079  last_time: 7.4351  data_time: 0.0036  last_data_time: 0.0037   lr: 1e-05  max_mem: 7275M
[04/29 03:33:57] d2.utils.events INFO:  eta: 2 days, 17:26:29  iter: 459  loss_ce: 0.1495  loss_texts: 0.1896  loss_ctrl_points: 0.1031  loss_bd_points: 0.1318  loss_ce_0: 0.1724  loss_texts_0: 0.2921  loss_ctrl_points_0: 0.1069  loss_bd_points_0: 0.1428  loss_ce_1: 0.1647  loss_texts_1: 0.2486  loss_ctrl_points_1: 0.1073  loss_bd_points_1: 0.1358  loss_ce_2: 0.1594  loss_texts_2: 0.2124  loss_ctrl_points_2: 0.1063  loss_bd_points_2: 0.1353  loss_ce_3: 0.1558  loss_texts_3: 0.1984  loss_ctrl_points_3: 0.1039  loss_bd_points_3: 0.1335  loss_ce_4: 0.1555  loss_texts_4: 0.1901  loss_ctrl_points_4: 0.1035  loss_bd_points_4: 0.1319  loss_ce_enc: 0.1578  loss_bezier_enc: 0.1093  total_loss: 4.06    time: 7.9638  last_time: 6.9382  data_time: 0.0035  last_data_time: 0.0032   lr: 1e-05  max_mem: 7275M
[04/29 03:33:57] d2.utils.events INFO:  eta: 2 days, 17:23:28  iter: 459  loss_ce: 0.1495  loss_texts: 0.1896  loss_ctrl_points: 0.1031  loss_bd_points: 0.1318  loss_ce_0: 0.1724  loss_texts_0: 0.2921  loss_ctrl_points_0: 0.1069  loss_bd_points_0: 0.1428  loss_ce_1: 0.1647  loss_texts_1: 0.2486  loss_ctrl_points_1: 0.1073  loss_bd_points_1: 0.1358  loss_ce_2: 0.1594  loss_texts_2: 0.2124  loss_ctrl_points_2: 0.1063  loss_bd_points_2: 0.1353  loss_ce_3: 0.1558  loss_texts_3: 0.1984  loss_ctrl_points_3: 0.1039  loss_bd_points_3: 0.1335  loss_ce_4: 0.1555  loss_texts_4: 0.1901  loss_ctrl_points_4: 0.1035  loss_bd_points_4: 0.1319  loss_ce_enc: 0.1578  loss_bezier_enc: 0.1093  total_loss: 4.06    time: 7.9628  last_time: 7.0467  data_time: 0.0035  last_data_time: 0.0032   lr: 1e-05  max_mem: 7275M
[04/29 03:36:18] d2.utils.events INFO:  eta: 2 days, 16:59:51  iter: 479  loss_ce: 0.1579  loss_texts: 0.1891  loss_ctrl_points: 0.1327  loss_bd_points: 0.18  loss_ce_0: 0.1859  loss_texts_0: 0.3012  loss_ctrl_points_0: 0.1331  loss_bd_points_0: 0.1825  loss_ce_1: 0.1665  loss_texts_1: 0.2442  loss_ctrl_points_1: 0.1292  loss_bd_points_1: 0.1866  loss_ce_2: 0.1584  loss_texts_2: 0.2109  loss_ctrl_points_2: 0.1305  loss_bd_points_2: 0.196  loss_ce_3: 0.1599  loss_texts_3: 0.2004  loss_ctrl_points_3: 0.1278  loss_bd_points_3: 0.1856  loss_ce_4: 0.157  loss_texts_4: 0.1906  loss_ctrl_points_4: 0.1336  loss_bd_points_4: 0.1844  loss_ce_enc: 0.1602  loss_bezier_enc: 0.115  total_loss: 4.623    time: 7.9238  last_time: 7.0317  data_time: 0.0035  last_data_time: 0.0036   lr: 1e-05  max_mem: 7275M
[04/29 03:36:18] d2.utils.events INFO:  eta: 2 days, 16:59:45  iter: 479  loss_ce: 0.1579  loss_texts: 0.1891  loss_ctrl_points: 0.1327  loss_bd_points: 0.18  loss_ce_0: 0.1859  loss_texts_0: 0.3012  loss_ctrl_points_0: 0.1331  loss_bd_points_0: 0.1825  loss_ce_1: 0.1665  loss_texts_1: 0.2442  loss_ctrl_points_1: 0.1292  loss_bd_points_1: 0.1866  loss_ce_2: 0.1584  loss_texts_2: 0.2109  loss_ctrl_points_2: 0.1305  loss_bd_points_2: 0.196  loss_ce_3: 0.1599  loss_texts_3: 0.2004  loss_ctrl_points_3: 0.1278  loss_bd_points_3: 0.1856  loss_ce_4: 0.157  loss_texts_4: 0.1906  loss_ctrl_points_4: 0.1336  loss_bd_points_4: 0.1844  loss_ce_enc: 0.1602  loss_bezier_enc: 0.115  total_loss: 4.623    time: 7.9229  last_time: 7.0828  data_time: 0.0035  last_data_time: 0.0036   lr: 1e-05  max_mem: 7275M
[04/29 03:38:37] d2.utils.events INFO:  eta: 2 days, 16:24:15  iter: 499  loss_ce: 0.1523  loss_texts: 0.2075  loss_ctrl_points: 0.1089  loss_bd_points: 0.1438  loss_ce_0: 0.1665  loss_texts_0: 0.3203  loss_ctrl_points_0: 0.1144  loss_bd_points_0: 0.15  loss_ce_1: 0.1677  loss_texts_1: 0.2685  loss_ctrl_points_1: 0.12  loss_bd_points_1: 0.1506  loss_ce_2: 0.1552  loss_texts_2: 0.2309  loss_ctrl_points_2: 0.1192  loss_bd_points_2: 0.1495  loss_ce_3: 0.1593  loss_texts_3: 0.2121  loss_ctrl_points_3: 0.1197  loss_bd_points_3: 0.1485  loss_ce_4: 0.1577  loss_texts_4: 0.2078  loss_ctrl_points_4: 0.1156  loss_bd_points_4: 0.1454  loss_ce_enc: 0.162  loss_bezier_enc: 0.1014  total_loss: 4.368    time: 7.8863  last_time: 6.8220  data_time: 0.0036  last_data_time: 0.0037   lr: 1e-05  max_mem: 7275M
[04/29 03:38:38] d2.utils.events INFO:  eta: 2 days, 16:24:09  iter: 499  loss_ce: 0.1523  loss_texts: 0.2075  loss_ctrl_points: 0.1089  loss_bd_points: 0.1438  loss_ce_0: 0.1665  loss_texts_0: 0.3203  loss_ctrl_points_0: 0.1144  loss_bd_points_0: 0.15  loss_ce_1: 0.1677  loss_texts_1: 0.2685  loss_ctrl_points_1: 0.12  loss_bd_points_1: 0.1506  loss_ce_2: 0.1552  loss_texts_2: 0.2309  loss_ctrl_points_2: 0.1192  loss_bd_points_2: 0.1495  loss_ce_3: 0.1593  loss_texts_3: 0.2121  loss_ctrl_points_3: 0.1197  loss_bd_points_3: 0.1485  loss_ce_4: 0.1577  loss_texts_4: 0.2078  loss_ctrl_points_4: 0.1156  loss_bd_points_4: 0.1454  loss_ce_enc: 0.162  loss_bezier_enc: 0.1014  total_loss: 4.368    time: 7.8853  last_time: 6.8766  data_time: 0.0036  last_data_time: 0.0037   lr: 1e-05  max_mem: 7275M
[04/29 03:40:56] d2.utils.events INFO:  eta: 2 days, 16:18:12  iter: 519  loss_ce: 0.149  loss_texts: 0.1843  loss_ctrl_points: 0.1208  loss_bd_points: 0.1678  loss_ce_0: 0.1777  loss_texts_0: 0.3069  loss_ctrl_points_0: 0.1173  loss_bd_points_0: 0.1684  loss_ce_1: 0.1649  loss_texts_1: 0.2489  loss_ctrl_points_1: 0.1203  loss_bd_points_1: 0.1673  loss_ce_2: 0.1626  loss_texts_2: 0.2174  loss_ctrl_points_2: 0.12  loss_bd_points_2: 0.1614  loss_ce_3: 0.1549  loss_texts_3: 0.1887  loss_ctrl_points_3: 0.1212  loss_bd_points_3: 0.1687  loss_ce_4: 0.151  loss_texts_4: 0.1832  loss_ctrl_points_4: 0.1206  loss_bd_points_4: 0.1661  loss_ce_enc: 0.1659  loss_bezier_enc: 0.1081  total_loss: 4.401    time: 7.8491  last_time: 7.3044  data_time: 0.0036  last_data_time: 0.0036   lr: 1e-05  max_mem: 7275M
[04/29 03:40:56] d2.utils.events INFO:  eta: 2 days, 16:18:12  iter: 519  loss_ce: 0.149  loss_texts: 0.1843  loss_ctrl_points: 0.1208  loss_bd_points: 0.1678  loss_ce_0: 0.1777  loss_texts_0: 0.3069  loss_ctrl_points_0: 0.1173  loss_bd_points_0: 0.1684  loss_ce_1: 0.1649  loss_texts_1: 0.2489  loss_ctrl_points_1: 0.1203  loss_bd_points_1: 0.1673  loss_ce_2: 0.1626  loss_texts_2: 0.2174  loss_ctrl_points_2: 0.12  loss_bd_points_2: 0.1614  loss_ce_3: 0.1549  loss_texts_3: 0.1887  loss_ctrl_points_3: 0.1212  loss_bd_points_3: 0.1687  loss_ce_4: 0.151  loss_texts_4: 0.1832  loss_ctrl_points_4: 0.1206  loss_bd_points_4: 0.1661  loss_ce_enc: 0.1659  loss_bezier_enc: 0.1081  total_loss: 4.401    time: 7.8487  last_time: 7.3610  data_time: 0.0036  last_data_time: 0.0036   lr: 1e-05  max_mem: 7275M
[04/29 03:43:12] d2.utils.events INFO:  eta: 2 days, 15:50:43  iter: 539  loss_ce: 0.1481  loss_texts: 0.1992  loss_ctrl_points: 0.1174  loss_bd_points: 0.1548  loss_ce_0: 0.1794  loss_texts_0: 0.3184  loss_ctrl_points_0: 0.1186  loss_bd_points_0: 0.1666  loss_ce_1: 0.1614  loss_texts_1: 0.2629  loss_ctrl_points_1: 0.1206  loss_bd_points_1: 0.1651  loss_ce_2: 0.1502  loss_texts_2: 0.2276  loss_ctrl_points_2: 0.1196  loss_bd_points_2: 0.1583  loss_ce_3: 0.1512  loss_texts_3: 0.2087  loss_ctrl_points_3: 0.1203  loss_bd_points_3: 0.1561  loss_ce_4: 0.1516  loss_texts_4: 0.1997  loss_ctrl_points_4: 0.1181  loss_bd_points_4: 0.1567  loss_ce_enc: 0.1654  loss_bezier_enc: 0.1061  total_loss: 4.335    time: 7.8094  last_time: 7.5112  data_time: 0.0035  last_data_time: 0.0035   lr: 1e-05  max_mem: 7275M
[04/29 03:43:12] d2.utils.events INFO:  eta: 2 days, 15:46:27  iter: 539  loss_ce: 0.1481  loss_texts: 0.1992  loss_ctrl_points: 0.1174  loss_bd_points: 0.1548  loss_ce_0: 0.1794  loss_texts_0: 0.3184  loss_ctrl_points_0: 0.1186  loss_bd_points_0: 0.1666  loss_ce_1: 0.1614  loss_texts_1: 0.2629  loss_ctrl_points_1: 0.1206  loss_bd_points_1: 0.1651  loss_ce_2: 0.1502  loss_texts_2: 0.2276  loss_ctrl_points_2: 0.1196  loss_bd_points_2: 0.1583  loss_ce_3: 0.1512  loss_texts_3: 0.2087  loss_ctrl_points_3: 0.1203  loss_bd_points_3: 0.1561  loss_ce_4: 0.1516  loss_texts_4: 0.1997  loss_ctrl_points_4: 0.1181  loss_bd_points_4: 0.1567  loss_ce_enc: 0.1654  loss_bezier_enc: 0.1061  total_loss: 4.335    time: 7.8092  last_time: 7.5656  data_time: 0.0035  last_data_time: 0.0035   lr: 1e-05  max_mem: 7275M
[04/29 03:45:26] d2.utils.events INFO:  eta: 2 days, 15:39:34  iter: 559  loss_ce: 0.1488  loss_texts: 0.2108  loss_ctrl_points: 0.1196  loss_bd_points: 0.1551  loss_ce_0: 0.181  loss_texts_0: 0.3108  loss_ctrl_points_0: 0.1234  loss_bd_points_0: 0.1558  loss_ce_1: 0.1669  loss_texts_1: 0.2585  loss_ctrl_points_1: 0.121  loss_bd_points_1: 0.1544  loss_ce_2: 0.16  loss_texts_2: 0.2301  loss_ctrl_points_2: 0.1186  loss_bd_points_2: 0.1574  loss_ce_3: 0.1567  loss_texts_3: 0.2215  loss_ctrl_points_3: 0.1186  loss_bd_points_3: 0.1539  loss_ce_4: 0.1511  loss_texts_4: 0.215  loss_ctrl_points_4: 0.1217  loss_bd_points_4: 0.1563  loss_ce_enc: 0.1632  loss_bezier_enc: 0.1145  total_loss: 4.513    time: 7.7708  last_time: 6.6262  data_time: 0.0035  last_data_time: 0.0033   lr: 1e-05  max_mem: 7275M
[04/29 03:45:27] d2.utils.events INFO:  eta: 2 days, 15:37:50  iter: 559  loss_ce: 0.1488  loss_texts: 0.2108  loss_ctrl_points: 0.1196  loss_bd_points: 0.1551  loss_ce_0: 0.181  loss_texts_0: 0.3108  loss_ctrl_points_0: 0.1234  loss_bd_points_0: 0.1558  loss_ce_1: 0.1669  loss_texts_1: 0.2585  loss_ctrl_points_1: 0.121  loss_bd_points_1: 0.1544  loss_ce_2: 0.16  loss_texts_2: 0.2301  loss_ctrl_points_2: 0.1186  loss_bd_points_2: 0.1574  loss_ce_3: 0.1567  loss_texts_3: 0.2215  loss_ctrl_points_3: 0.1186  loss_bd_points_3: 0.1539  loss_ce_4: 0.1511  loss_texts_4: 0.215  loss_ctrl_points_4: 0.1217  loss_bd_points_4: 0.1563  loss_ce_enc: 0.1632  loss_bezier_enc: 0.1145  total_loss: 4.513    time: 7.7699  last_time: 6.7034  data_time: 0.0035  last_data_time: 0.0033   lr: 1e-05  max_mem: 7275M
[04/29 03:47:42] d2.utils.events INFO:  eta: 2 days, 14:40:29  iter: 579  loss_ce: 0.1558  loss_texts: 0.2118  loss_ctrl_points: 0.1187  loss_bd_points: 0.1607  loss_ce_0: 0.1809  loss_texts_0: 0.3267  loss_ctrl_points_0: 0.1135  loss_bd_points_0: 0.1716  loss_ce_1: 0.1692  loss_texts_1: 0.2655  loss_ctrl_points_1: 0.1208  loss_bd_points_1: 0.1674  loss_ce_2: 0.1602  loss_texts_2: 0.2307  loss_ctrl_points_2: 0.121  loss_bd_points_2: 0.1658  loss_ce_3: 0.1624  loss_texts_3: 0.2222  loss_ctrl_points_3: 0.1231  loss_bd_points_3: 0.163  loss_ce_4: 0.1598  loss_texts_4: 0.2126  loss_ctrl_points_4: 0.1218  loss_bd_points_4: 0.1587  loss_ce_enc: 0.1655  loss_bezier_enc: 0.1056  total_loss: 4.433    time: 7.7372  last_time: 6.4986  data_time: 0.0034  last_data_time: 0.0034   lr: 1e-05  max_mem: 7275M
[04/29 03:47:43] d2.utils.events INFO:  eta: 2 days, 14:40:06  iter: 579  loss_ce: 0.1558  loss_texts: 0.2118  loss_ctrl_points: 0.1187  loss_bd_points: 0.1607  loss_ce_0: 0.1809  loss_texts_0: 0.3267  loss_ctrl_points_0: 0.1135  loss_bd_points_0: 0.1716  loss_ce_1: 0.1692  loss_texts_1: 0.2655  loss_ctrl_points_1: 0.1208  loss_bd_points_1: 0.1674  loss_ce_2: 0.1602  loss_texts_2: 0.2307  loss_ctrl_points_2: 0.121  loss_bd_points_2: 0.1658  loss_ce_3: 0.1624  loss_texts_3: 0.2222  loss_ctrl_points_3: 0.1231  loss_bd_points_3: 0.163  loss_ce_4: 0.1598  loss_texts_4: 0.2126  loss_ctrl_points_4: 0.1218  loss_bd_points_4: 0.1587  loss_ce_enc: 0.1655  loss_bezier_enc: 0.1056  total_loss: 4.433    time: 7.7361  last_time: 6.5411  data_time: 0.0034  last_data_time: 0.0034   lr: 1e-05  max_mem: 7275M
[04/29 03:50:04] d2.utils.events INFO:  eta: 2 days, 14:03:45  iter: 599  loss_ce: 0.1452  loss_texts: 0.1903  loss_ctrl_points: 0.108  loss_bd_points: 0.1445  loss_ce_0: 0.1764  loss_texts_0: 0.2934  loss_ctrl_points_0: 0.111  loss_bd_points_0: 0.1486  loss_ce_1: 0.1633  loss_texts_1: 0.2428  loss_ctrl_points_1: 0.1088  loss_bd_points_1: 0.1466  loss_ce_2: 0.1597  loss_texts_2: 0.2011  loss_ctrl_points_2: 0.1134  loss_bd_points_2: 0.1488  loss_ce_3: 0.1538  loss_texts_3: 0.1918  loss_ctrl_points_3: 0.1121  loss_bd_points_3: 0.1477  loss_ce_4: 0.1496  loss_texts_4: 0.1913  loss_ctrl_points_4: 0.1084  loss_bd_points_4: 0.1444  loss_ce_enc: 0.1624  loss_bezier_enc: 0.104  total_loss: 4.353    time: 7.7133  last_time: 7.4726  data_time: 0.0035  last_data_time: 0.0036   lr: 1e-05  max_mem: 7275M
[04/29 03:50:04] d2.utils.events INFO:  eta: 2 days, 14:02:49  iter: 599  loss_ce: 0.1452  loss_texts: 0.1903  loss_ctrl_points: 0.108  loss_bd_points: 0.1445  loss_ce_0: 0.1764  loss_texts_0: 0.2934  loss_ctrl_points_0: 0.111  loss_bd_points_0: 0.1486  loss_ce_1: 0.1633  loss_texts_1: 0.2428  loss_ctrl_points_1: 0.1088  loss_bd_points_1: 0.1466  loss_ce_2: 0.1597  loss_texts_2: 0.2011  loss_ctrl_points_2: 0.1134  loss_bd_points_2: 0.1488  loss_ce_3: 0.1538  loss_texts_3: 0.1918  loss_ctrl_points_3: 0.1121  loss_bd_points_3: 0.1477  loss_ce_4: 0.1496  loss_texts_4: 0.1913  loss_ctrl_points_4: 0.1084  loss_bd_points_4: 0.1444  loss_ce_enc: 0.1624  loss_bezier_enc: 0.104  total_loss: 4.353    time: 7.7131  last_time: 7.5124  data_time: 0.0035  last_data_time: 0.0036   lr: 1e-05  max_mem: 7275M
[04/29 03:52:20] d2.utils.events INFO:  eta: 2 days, 13:09:37  iter: 619  loss_ce: 0.1721  loss_texts: 0.2518  loss_ctrl_points: 0.1344  loss_bd_points: 0.1767  loss_ce_0: 0.1944  loss_texts_0: 0.3526  loss_ctrl_points_0: 0.141  loss_bd_points_0: 0.1753  loss_ce_1: 0.1791  loss_texts_1: 0.3125  loss_ctrl_points_1: 0.1424  loss_bd_points_1: 0.1806  loss_ce_2: 0.1766  loss_texts_2: 0.2871  loss_ctrl_points_2: 0.1436  loss_bd_points_2: 0.1832  loss_ce_3: 0.181  loss_texts_3: 0.2707  loss_ctrl_points_3: 0.1377  loss_bd_points_3: 0.1798  loss_ce_4: 0.1755  loss_texts_4: 0.2565  loss_ctrl_points_4: 0.1327  loss_bd_points_4: 0.1762  loss_ce_enc: 0.1787  loss_bezier_enc: 0.1157  total_loss: 5.144    time: 7.6847  last_time: 6.5608  data_time: 0.0036  last_data_time: 0.0033   lr: 1e-05  max_mem: 7275M
[04/29 03:52:20] d2.utils.events INFO:  eta: 2 days, 13:08:49  iter: 619  loss_ce: 0.1721  loss_texts: 0.2518  loss_ctrl_points: 0.1344  loss_bd_points: 0.1767  loss_ce_0: 0.1944  loss_texts_0: 0.3526  loss_ctrl_points_0: 0.141  loss_bd_points_0: 0.1753  loss_ce_1: 0.1791  loss_texts_1: 0.3125  loss_ctrl_points_1: 0.1424  loss_bd_points_1: 0.1806  loss_ce_2: 0.1766  loss_texts_2: 0.2871  loss_ctrl_points_2: 0.1436  loss_bd_points_2: 0.1832  loss_ce_3: 0.181  loss_texts_3: 0.2707  loss_ctrl_points_3: 0.1377  loss_bd_points_3: 0.1798  loss_ce_4: 0.1755  loss_texts_4: 0.2565  loss_ctrl_points_4: 0.1327  loss_bd_points_4: 0.1762  loss_ce_enc: 0.1787  loss_bezier_enc: 0.1157  total_loss: 5.144    time: 7.6838  last_time: 6.5960  data_time: 0.0036  last_data_time: 0.0033   lr: 1e-05  max_mem: 7275M
[04/29 03:54:40] d2.utils.events INFO:  eta: 2 days, 12:13:25  iter: 639  loss_ce: 0.1648  loss_texts: 0.2319  loss_ctrl_points: 0.1237  loss_bd_points: 0.1684  loss_ce_0: 0.1869  loss_texts_0: 0.327  loss_ctrl_points_0: 0.1238  loss_bd_points_0: 0.1762  loss_ce_1: 0.1795  loss_texts_1: 0.2928  loss_ctrl_points_1: 0.1239  loss_bd_points_1: 0.1694  loss_ce_2: 0.1736  loss_texts_2: 0.2599  loss_ctrl_points_2: 0.1239  loss_bd_points_2: 0.1702  loss_ce_3: 0.1669  loss_texts_3: 0.2437  loss_ctrl_points_3: 0.1233  loss_bd_points_3: 0.1702  loss_ce_4: 0.1661  loss_texts_4: 0.2333  loss_ctrl_points_4: 0.1236  loss_bd_points_4: 0.1707  loss_ce_enc: 0.1747  loss_bezier_enc: 0.1112  total_loss: 4.315    time: 7.6630  last_time: 6.5432  data_time: 0.0034  last_data_time: 0.0034   lr: 1e-05  max_mem: 7275M
[04/29 03:54:40] d2.utils.events INFO:  eta: 2 days, 12:12:29  iter: 639  loss_ce: 0.1648  loss_texts: 0.2319  loss_ctrl_points: 0.1237  loss_bd_points: 0.1684  loss_ce_0: 0.1869  loss_texts_0: 0.327  loss_ctrl_points_0: 0.1238  loss_bd_points_0: 0.1762  loss_ce_1: 0.1795  loss_texts_1: 0.2928  loss_ctrl_points_1: 0.1239  loss_bd_points_1: 0.1694  loss_ce_2: 0.1736  loss_texts_2: 0.2599  loss_ctrl_points_2: 0.1239  loss_bd_points_2: 0.1702  loss_ce_3: 0.1669  loss_texts_3: 0.2437  loss_ctrl_points_3: 0.1233  loss_bd_points_3: 0.1702  loss_ce_4: 0.1661  loss_texts_4: 0.2333  loss_ctrl_points_4: 0.1236  loss_bd_points_4: 0.1707  loss_ce_enc: 0.1747  loss_bezier_enc: 0.1112  total_loss: 4.315    time: 7.6622  last_time: 6.5870  data_time: 0.0034  last_data_time: 0.0034   lr: 1e-05  max_mem: 7275M
[04/29 03:56:59] d2.utils.events INFO:  eta: 2 days, 11:43:51  iter: 659  loss_ce: 0.1466  loss_texts: 0.1903  loss_ctrl_points: 0.1141  loss_bd_points: 0.1407  loss_ce_0: 0.175  loss_texts_0: 0.2946  loss_ctrl_points_0: 0.123  loss_bd_points_0: 0.1641  loss_ce_1: 0.1672  loss_texts_1: 0.2525  loss_ctrl_points_1: 0.1181  loss_bd_points_1: 0.1589  loss_ce_2: 0.154  loss_texts_2: 0.2148  loss_ctrl_points_2: 0.1177  loss_bd_points_2: 0.1597  loss_ce_3: 0.1524  loss_texts_3: 0.1946  loss_ctrl_points_3: 0.1159  loss_bd_points_3: 0.1569  loss_ce_4: 0.1479  loss_texts_4: 0.1934  loss_ctrl_points_4: 0.1133  loss_bd_points_4: 0.1405  loss_ce_enc: 0.1646  loss_bezier_enc: 0.1128  total_loss: 4.289    time: 7.6411  last_time: 7.4678  data_time: 0.0035  last_data_time: 0.0036   lr: 1e-05  max_mem: 7275M
[04/29 03:56:59] d2.utils.events INFO:  eta: 2 days, 11:43:51  iter: 659  loss_ce: 0.1466  loss_texts: 0.1903  loss_ctrl_points: 0.1141  loss_bd_points: 0.1407  loss_ce_0: 0.175  loss_texts_0: 0.2946  loss_ctrl_points_0: 0.123  loss_bd_points_0: 0.1641  loss_ce_1: 0.1672  loss_texts_1: 0.2525  loss_ctrl_points_1: 0.1181  loss_bd_points_1: 0.1589  loss_ce_2: 0.154  loss_texts_2: 0.2148  loss_ctrl_points_2: 0.1177  loss_bd_points_2: 0.1597  loss_ce_3: 0.1524  loss_texts_3: 0.1946  loss_ctrl_points_3: 0.1159  loss_bd_points_3: 0.1569  loss_ce_4: 0.1479  loss_texts_4: 0.1934  loss_ctrl_points_4: 0.1133  loss_bd_points_4: 0.1405  loss_ce_enc: 0.1646  loss_bezier_enc: 0.1128  total_loss: 4.289    time: 7.6410  last_time: 7.5192  data_time: 0.0035  last_data_time: 0.0036   lr: 1e-05  max_mem: 7275M
[04/29 03:59:17] d2.utils.events INFO:  eta: 2 days, 11:25:30  iter: 679  loss_ce: 0.1679  loss_texts: 0.2204  loss_ctrl_points: 0.143  loss_bd_points: 0.1883  loss_ce_0: 0.1962  loss_texts_0: 0.3343  loss_ctrl_points_0: 0.1375  loss_bd_points_0: 0.1983  loss_ce_1: 0.187  loss_texts_1: 0.2737  loss_ctrl_points_1: 0.1449  loss_bd_points_1: 0.1941  loss_ce_2: 0.1797  loss_texts_2: 0.2404  loss_ctrl_points_2: 0.1471  loss_bd_points_2: 0.1868  loss_ce_3: 0.1751  loss_texts_3: 0.236  loss_ctrl_points_3: 0.1419  loss_bd_points_3: 0.1894  loss_ce_4: 0.1715  loss_texts_4: 0.2277  loss_ctrl_points_4: 0.1423  loss_bd_points_4: 0.1892  loss_ce_enc: 0.1901  loss_bezier_enc: 0.1074  total_loss: 5.1    time: 7.6206  last_time: 5.8266  data_time: 0.0035  last_data_time: 0.0034   lr: 1e-05  max_mem: 7275M
[04/29 03:59:17] d2.utils.events INFO:  eta: 2 days, 11:25:30  iter: 679  loss_ce: 0.1679  loss_texts: 0.2204  loss_ctrl_points: 0.143  loss_bd_points: 0.1883  loss_ce_0: 0.1962  loss_texts_0: 0.3343  loss_ctrl_points_0: 0.1375  loss_bd_points_0: 0.1983  loss_ce_1: 0.187  loss_texts_1: 0.2737  loss_ctrl_points_1: 0.1449  loss_bd_points_1: 0.1941  loss_ce_2: 0.1797  loss_texts_2: 0.2404  loss_ctrl_points_2: 0.1471  loss_bd_points_2: 0.1868  loss_ce_3: 0.1751  loss_texts_3: 0.236  loss_ctrl_points_3: 0.1419  loss_bd_points_3: 0.1894  loss_ce_4: 0.1715  loss_texts_4: 0.2277  loss_ctrl_points_4: 0.1423  loss_bd_points_4: 0.1892  loss_ce_enc: 0.1901  loss_bezier_enc: 0.1074  total_loss: 5.1    time: 7.6193  last_time: 5.8829  data_time: 0.0035  last_data_time: 0.0034   lr: 1e-05  max_mem: 7275M
[04/29 04:01:36] d2.utils.events INFO:  eta: 2 days, 10:57:58  iter: 699  loss_ce: 0.1458  loss_texts: 0.2255  loss_ctrl_points: 0.1124  loss_bd_points: 0.1545  loss_ce_0: 0.1847  loss_texts_0: 0.3284  loss_ctrl_points_0: 0.1101  loss_bd_points_0: 0.1612  loss_ce_1: 0.1736  loss_texts_1: 0.2771  loss_ctrl_points_1: 0.1147  loss_bd_points_1: 0.1576  loss_ce_2: 0.1596  loss_texts_2: 0.2544  loss_ctrl_points_2: 0.112  loss_bd_points_2: 0.157  loss_ce_3: 0.1532  loss_texts_3: 0.228  loss_ctrl_points_3: 0.1118  loss_bd_points_3: 0.1556  loss_ce_4: 0.1487  loss_texts_4: 0.2149  loss_ctrl_points_4: 0.1121  loss_bd_points_4: 0.1549  loss_ce_enc: 0.1655  loss_bezier_enc: 0.1087  total_loss: 4.423    time: 7.6006  last_time: 6.7294  data_time: 0.0036  last_data_time: 0.0035   lr: 1e-05  max_mem: 7275M
[04/29 04:01:36] d2.utils.events INFO:  eta: 2 days, 10:57:41  iter: 699  loss_ce: 0.1458  loss_texts: 0.2255  loss_ctrl_points: 0.1124  loss_bd_points: 0.1545  loss_ce_0: 0.1847  loss_texts_0: 0.3284  loss_ctrl_points_0: 0.1101  loss_bd_points_0: 0.1612  loss_ce_1: 0.1736  loss_texts_1: 0.2771  loss_ctrl_points_1: 0.1147  loss_bd_points_1: 0.1576  loss_ce_2: 0.1596  loss_texts_2: 0.2544  loss_ctrl_points_2: 0.112  loss_bd_points_2: 0.157  loss_ce_3: 0.1532  loss_texts_3: 0.228  loss_ctrl_points_3: 0.1118  loss_bd_points_3: 0.1556  loss_ce_4: 0.1487  loss_texts_4: 0.2149  loss_ctrl_points_4: 0.1121  loss_bd_points_4: 0.1549  loss_ce_enc: 0.1655  loss_bezier_enc: 0.1087  total_loss: 4.423    time: 7.6000  last_time: 6.7749  data_time: 0.0036  last_data_time: 0.0035   lr: 1e-05  max_mem: 7275M
[04/29 04:03:53] d2.utils.events INFO:  eta: 2 days, 10:30:32  iter: 719  loss_ce: 0.1506  loss_texts: 0.1931  loss_ctrl_points: 0.1119  loss_bd_points: 0.1578  loss_ce_0: 0.1777  loss_texts_0: 0.3077  loss_ctrl_points_0: 0.1234  loss_bd_points_0: 0.1588  loss_ce_1: 0.1741  loss_texts_1: 0.2488  loss_ctrl_points_1: 0.1098  loss_bd_points_1: 0.1484  loss_ce_2: 0.1565  loss_texts_2: 0.2127  loss_ctrl_points_2: 0.1115  loss_bd_points_2: 0.1589  loss_ce_3: 0.1602  loss_texts_3: 0.2039  loss_ctrl_points_3: 0.1144  loss_bd_points_3: 0.1609  loss_ce_4: 0.1532  loss_texts_4: 0.1908  loss_ctrl_points_4: 0.1132  loss_bd_points_4: 0.1582  loss_ce_enc: 0.1681  loss_bezier_enc: 0.1094  total_loss: 4.345    time: 7.5796  last_time: 6.7033  data_time: 0.0035  last_data_time: 0.0034   lr: 1e-05  max_mem: 7275M
[04/29 04:03:53] d2.utils.events INFO:  eta: 2 days, 10:30:03  iter: 719  loss_ce: 0.1506  loss_texts: 0.1931  loss_ctrl_points: 0.1119  loss_bd_points: 0.1578  loss_ce_0: 0.1777  loss_texts_0: 0.3077  loss_ctrl_points_0: 0.1234  loss_bd_points_0: 0.1588  loss_ce_1: 0.1741  loss_texts_1: 0.2488  loss_ctrl_points_1: 0.1098  loss_bd_points_1: 0.1484  loss_ce_2: 0.1565  loss_texts_2: 0.2127  loss_ctrl_points_2: 0.1115  loss_bd_points_2: 0.1589  loss_ce_3: 0.1602  loss_texts_3: 0.2039  loss_ctrl_points_3: 0.1144  loss_bd_points_3: 0.1609  loss_ce_4: 0.1532  loss_texts_4: 0.1908  loss_ctrl_points_4: 0.1132  loss_bd_points_4: 0.1582  loss_ce_enc: 0.1681  loss_bezier_enc: 0.1094  total_loss: 4.345    time: 7.5790  last_time: 6.7543  data_time: 0.0035  last_data_time: 0.0034   lr: 1e-05  max_mem: 7275M
[04/29 04:06:12] d2.utils.events INFO:  eta: 2 days, 10:14:41  iter: 739  loss_ce: 0.1566  loss_texts: 0.2257  loss_ctrl_points: 0.101  loss_bd_points: 0.134  loss_ce_0: 0.1841  loss_texts_0: 0.316  loss_ctrl_points_0: 0.1069  loss_bd_points_0: 0.1381  loss_ce_1: 0.1749  loss_texts_1: 0.2726  loss_ctrl_points_1: 0.1051  loss_bd_points_1: 0.1368  loss_ce_2: 0.1698  loss_texts_2: 0.2475  loss_ctrl_points_2: 0.1061  loss_bd_points_2: 0.1403  loss_ce_3: 0.165  loss_texts_3: 0.2346  loss_ctrl_points_3: 0.1046  loss_bd_points_3: 0.1361  loss_ce_4: 0.1575  loss_texts_4: 0.2214  loss_ctrl_points_4: 0.1022  loss_bd_points_4: 0.1301  loss_ce_enc: 0.1712  loss_bezier_enc: 0.1027  total_loss: 4.275    time: 7.5617  last_time: 6.6079  data_time: 0.0035  last_data_time: 0.0037   lr: 1e-05  max_mem: 7275M
[04/29 04:06:12] d2.utils.events INFO:  eta: 2 days, 10:14:41  iter: 739  loss_ce: 0.1566  loss_texts: 0.2257  loss_ctrl_points: 0.101  loss_bd_points: 0.134  loss_ce_0: 0.1841  loss_texts_0: 0.316  loss_ctrl_points_0: 0.1069  loss_bd_points_0: 0.1381  loss_ce_1: 0.1749  loss_texts_1: 0.2726  loss_ctrl_points_1: 0.1051  loss_bd_points_1: 0.1368  loss_ce_2: 0.1698  loss_texts_2: 0.2475  loss_ctrl_points_2: 0.1061  loss_bd_points_2: 0.1403  loss_ce_3: 0.165  loss_texts_3: 0.2346  loss_ctrl_points_3: 0.1046  loss_bd_points_3: 0.1361  loss_ce_4: 0.1575  loss_texts_4: 0.2214  loss_ctrl_points_4: 0.1022  loss_bd_points_4: 0.1301  loss_ce_enc: 0.1712  loss_bezier_enc: 0.1027  total_loss: 4.275    time: 7.5611  last_time: 6.6577  data_time: 0.0035  last_data_time: 0.0037   lr: 1e-05  max_mem: 7275M
[04/29 04:08:30] d2.utils.events INFO:  eta: 2 days, 9:53:57  iter: 759  loss_ce: 0.1575  loss_texts: 0.2035  loss_ctrl_points: 0.106  loss_bd_points: 0.1496  loss_ce_0: 0.1944  loss_texts_0: 0.2963  loss_ctrl_points_0: 0.1164  loss_bd_points_0: 0.1594  loss_ce_1: 0.1699  loss_texts_1: 0.2572  loss_ctrl_points_1: 0.1128  loss_bd_points_1: 0.1515  loss_ce_2: 0.1663  loss_texts_2: 0.2333  loss_ctrl_points_2: 0.1091  loss_bd_points_2: 0.1491  loss_ce_3: 0.1647  loss_texts_3: 0.2111  loss_ctrl_points_3: 0.1077  loss_bd_points_3: 0.1511  loss_ce_4: 0.1596  loss_texts_4: 0.2019  loss_ctrl_points_4: 0.1066  loss_bd_points_4: 0.1498  loss_ce_enc: 0.1706  loss_bezier_enc: 0.1071  total_loss: 4.171    time: 7.5449  last_time: 7.0538  data_time: 0.0035  last_data_time: 0.0035   lr: 1e-05  max_mem: 7275M
[04/29 04:08:30] d2.utils.events INFO:  eta: 2 days, 9:53:50  iter: 759  loss_ce: 0.1575  loss_texts: 0.2035  loss_ctrl_points: 0.106  loss_bd_points: 0.1496  loss_ce_0: 0.1944  loss_texts_0: 0.2963  loss_ctrl_points_0: 0.1164  loss_bd_points_0: 0.1594  loss_ce_1: 0.1699  loss_texts_1: 0.2572  loss_ctrl_points_1: 0.1128  loss_bd_points_1: 0.1515  loss_ce_2: 0.1663  loss_texts_2: 0.2333  loss_ctrl_points_2: 0.1091  loss_bd_points_2: 0.1491  loss_ce_3: 0.1647  loss_texts_3: 0.2111  loss_ctrl_points_3: 0.1077  loss_bd_points_3: 0.1511  loss_ce_4: 0.1596  loss_texts_4: 0.2019  loss_ctrl_points_4: 0.1066  loss_bd_points_4: 0.1498  loss_ce_enc: 0.1706  loss_bezier_enc: 0.1071  total_loss: 4.171    time: 7.5446  last_time: 7.1117  data_time: 0.0035  last_data_time: 0.0035   lr: 1e-05  max_mem: 7275M
[04/29 04:10:47] d2.utils.events INFO:  eta: 2 days, 9:22:40  iter: 779  loss_ce: 0.1596  loss_texts: 0.191  loss_ctrl_points: 0.1269  loss_bd_points: 0.1508  loss_ce_0: 0.1828  loss_texts_0: 0.3033  loss_ctrl_points_0: 0.1256  loss_bd_points_0: 0.1624  loss_ce_1: 0.1723  loss_texts_1: 0.2511  loss_ctrl_points_1: 0.1276  loss_bd_points_1: 0.1578  loss_ce_2: 0.1673  loss_texts_2: 0.2175  loss_ctrl_points_2: 0.1286  loss_bd_points_2: 0.1571  loss_ce_3: 0.1658  loss_texts_3: 0.2052  loss_ctrl_points_3: 0.1271  loss_bd_points_3: 0.1546  loss_ce_4: 0.1587  loss_texts_4: 0.1928  loss_ctrl_points_4: 0.1275  loss_bd_points_4: 0.1534  loss_ce_enc: 0.1676  loss_bezier_enc: 0.1132  total_loss: 4.61    time: 7.5268  last_time: 6.6398  data_time: 0.0036  last_data_time: 0.0040   lr: 1e-05  max_mem: 7275M
[04/29 04:10:47] d2.utils.events INFO:  eta: 2 days, 9:20:41  iter: 779  loss_ce: 0.1596  loss_texts: 0.191  loss_ctrl_points: 0.1269  loss_bd_points: 0.1508  loss_ce_0: 0.1828  loss_texts_0: 0.3033  loss_ctrl_points_0: 0.1256  loss_bd_points_0: 0.1624  loss_ce_1: 0.1723  loss_texts_1: 0.2511  loss_ctrl_points_1: 0.1276  loss_bd_points_1: 0.1578  loss_ce_2: 0.1673  loss_texts_2: 0.2175  loss_ctrl_points_2: 0.1286  loss_bd_points_2: 0.1571  loss_ce_3: 0.1658  loss_texts_3: 0.2052  loss_ctrl_points_3: 0.1271  loss_bd_points_3: 0.1546  loss_ce_4: 0.1587  loss_texts_4: 0.1928  loss_ctrl_points_4: 0.1275  loss_bd_points_4: 0.1534  loss_ce_enc: 0.1676  loss_bezier_enc: 0.1132  total_loss: 4.61    time: 7.5263  last_time: 6.6996  data_time: 0.0036  last_data_time: 0.0040   lr: 1e-05  max_mem: 7275M
[04/29 04:13:02] d2.utils.events INFO:  eta: 2 days, 9:04:43  iter: 799  loss_ce: 0.1583  loss_texts: 0.207  loss_ctrl_points: 0.1186  loss_bd_points: 0.1473  loss_ce_0: 0.1861  loss_texts_0: 0.3063  loss_ctrl_points_0: 0.1199  loss_bd_points_0: 0.1566  loss_ce_1: 0.1777  loss_texts_1: 0.266  loss_ctrl_points_1: 0.1158  loss_bd_points_1: 0.155  loss_ce_2: 0.1694  loss_texts_2: 0.2303  loss_ctrl_points_2: 0.1181  loss_bd_points_2: 0.1546  loss_ce_3: 0.1661  loss_texts_3: 0.2177  loss_ctrl_points_3: 0.1247  loss_bd_points_3: 0.155  loss_ce_4: 0.1603  loss_texts_4: 0.2109  loss_ctrl_points_4: 0.1195  loss_bd_points_4: 0.1491  loss_ce_enc: 0.1748  loss_bezier_enc: 0.1052  total_loss: 4.258    time: 7.5069  last_time: 6.9898  data_time: 0.0035  last_data_time: 0.0036   lr: 1e-05  max_mem: 7275M
[04/29 04:13:02] d2.utils.events INFO:  eta: 2 days, 9:04:03  iter: 799  loss_ce: 0.1583  loss_texts: 0.207  loss_ctrl_points: 0.1186  loss_bd_points: 0.1473  loss_ce_0: 0.1861  loss_texts_0: 0.3063  loss_ctrl_points_0: 0.1199  loss_bd_points_0: 0.1566  loss_ce_1: 0.1777  loss_texts_1: 0.266  loss_ctrl_points_1: 0.1158  loss_bd_points_1: 0.155  loss_ce_2: 0.1694  loss_texts_2: 0.2303  loss_ctrl_points_2: 0.1181  loss_bd_points_2: 0.1546  loss_ce_3: 0.1661  loss_texts_3: 0.2177  loss_ctrl_points_3: 0.1247  loss_bd_points_3: 0.155  loss_ce_4: 0.1603  loss_texts_4: 0.2109  loss_ctrl_points_4: 0.1195  loss_bd_points_4: 0.1491  loss_ce_enc: 0.1748  loss_bezier_enc: 0.1052  total_loss: 4.258    time: 7.5066  last_time: 7.0353  data_time: 0.0035  last_data_time: 0.0036   lr: 1e-05  max_mem: 7275M
[04/29 04:15:18] d2.utils.events INFO:  eta: 2 days, 8:51:17  iter: 819  loss_ce: 0.1518  loss_texts: 0.2113  loss_ctrl_points: 0.123  loss_bd_points: 0.1718  loss_ce_0: 0.1779  loss_texts_0: 0.3241  loss_ctrl_points_0: 0.1207  loss_bd_points_0: 0.1728  loss_ce_1: 0.171  loss_texts_1: 0.2666  loss_ctrl_points_1: 0.1241  loss_bd_points_1: 0.1739  loss_ce_2: 0.162  loss_texts_2: 0.2383  loss_ctrl_points_2: 0.1211  loss_bd_points_2: 0.1714  loss_ce_3: 0.1583  loss_texts_3: 0.2247  loss_ctrl_points_3: 0.1236  loss_bd_points_3: 0.1727  loss_ce_4: 0.1551  loss_texts_4: 0.2142  loss_ctrl_points_4: 0.1229  loss_bd_points_4: 0.1709  loss_ce_enc: 0.1662  loss_bezier_enc: 0.1095  total_loss: 4.355    time: 7.4898  last_time: 7.1608  data_time: 0.0035  last_data_time: 0.0035   lr: 1e-05  max_mem: 7276M
[04/29 04:15:18] d2.utils.events INFO:  eta: 2 days, 8:51:17  iter: 819  loss_ce: 0.1518  loss_texts: 0.2113  loss_ctrl_points: 0.123  loss_bd_points: 0.1718  loss_ce_0: 0.1779  loss_texts_0: 0.3241  loss_ctrl_points_0: 0.1207  loss_bd_points_0: 0.1728  loss_ce_1: 0.171  loss_texts_1: 0.2666  loss_ctrl_points_1: 0.1241  loss_bd_points_1: 0.1739  loss_ce_2: 0.162  loss_texts_2: 0.2383  loss_ctrl_points_2: 0.1211  loss_bd_points_2: 0.1714  loss_ce_3: 0.1583  loss_texts_3: 0.2247  loss_ctrl_points_3: 0.1236  loss_bd_points_3: 0.1727  loss_ce_4: 0.1551  loss_texts_4: 0.2142  loss_ctrl_points_4: 0.1229  loss_bd_points_4: 0.1709  loss_ce_enc: 0.1662  loss_bezier_enc: 0.1095  total_loss: 4.355    time: 7.4897  last_time: 7.2074  data_time: 0.0035  last_data_time: 0.0035   lr: 1e-05  max_mem: 7276M
[04/29 04:17:38] d2.utils.events INFO:  eta: 2 days, 8:32:43  iter: 839  loss_ce: 0.1475  loss_texts: 0.1647  loss_ctrl_points: 0.1163  loss_bd_points: 0.1643  loss_ce_0: 0.1696  loss_texts_0: 0.308  loss_ctrl_points_0: 0.1103  loss_bd_points_0: 0.1567  loss_ce_1: 0.1685  loss_texts_1: 0.231  loss_ctrl_points_1: 0.1127  loss_bd_points_1: 0.1657  loss_ce_2: 0.1551  loss_texts_2: 0.2012  loss_ctrl_points_2: 0.1126  loss_bd_points_2: 0.1676  loss_ce_3: 0.1461  loss_texts_3: 0.1747  loss_ctrl_points_3: 0.1152  loss_bd_points_3: 0.1663  loss_ce_4: 0.1475  loss_texts_4: 0.1803  loss_ctrl_points_4: 0.1128  loss_bd_points_4: 0.1659  loss_ce_enc: 0.1684  loss_bezier_enc: 0.1039  total_loss: 4.355    time: 7.4776  last_time: 6.9992  data_time: 0.0036  last_data_time: 0.0033   lr: 1e-05  max_mem: 7276M
[04/29 04:17:38] d2.utils.events INFO:  eta: 2 days, 8:32:43  iter: 839  loss_ce: 0.1475  loss_texts: 0.1647  loss_ctrl_points: 0.1163  loss_bd_points: 0.1643  loss_ce_0: 0.1696  loss_texts_0: 0.308  loss_ctrl_points_0: 0.1103  loss_bd_points_0: 0.1567  loss_ce_1: 0.1685  loss_texts_1: 0.231  loss_ctrl_points_1: 0.1127  loss_bd_points_1: 0.1657  loss_ce_2: 0.1551  loss_texts_2: 0.2012  loss_ctrl_points_2: 0.1126  loss_bd_points_2: 0.1676  loss_ce_3: 0.1461  loss_texts_3: 0.1747  loss_ctrl_points_3: 0.1152  loss_bd_points_3: 0.1663  loss_ce_4: 0.1475  loss_texts_4: 0.1803  loss_ctrl_points_4: 0.1128  loss_bd_points_4: 0.1659  loss_ce_enc: 0.1684  loss_bezier_enc: 0.1039  total_loss: 4.355    time: 7.4774  last_time: 7.0500  data_time: 0.0036  last_data_time: 0.0033   lr: 1e-05  max_mem: 7276M
[04/29 04:19:52] d2.utils.events INFO:  eta: 2 days, 8:15:40  iter: 859  loss_ce: 0.1716  loss_texts: 0.2218  loss_ctrl_points: 0.1117  loss_bd_points: 0.1542  loss_ce_0: 0.1949  loss_texts_0: 0.3309  loss_ctrl_points_0: 0.1227  loss_bd_points_0: 0.1632  loss_ce_1: 0.1849  loss_texts_1: 0.2708  loss_ctrl_points_1: 0.1184  loss_bd_points_1: 0.1553  loss_ce_2: 0.1788  loss_texts_2: 0.2478  loss_ctrl_points_2: 0.1196  loss_bd_points_2: 0.165  loss_ce_3: 0.174  loss_texts_3: 0.234  loss_ctrl_points_3: 0.1133  loss_bd_points_3: 0.1572  loss_ce_4: 0.172  loss_texts_4: 0.2207  loss_ctrl_points_4: 0.113  loss_bd_points_4: 0.1528  loss_ce_enc: 0.172  loss_bezier_enc: 0.1063  total_loss: 4.62    time: 7.4593  last_time: 6.5180  data_time: 0.0036  last_data_time: 0.0033   lr: 1e-05  max_mem: 7276M
[04/29 04:19:52] d2.utils.events INFO:  eta: 2 days, 8:13:40  iter: 859  loss_ce: 0.1716  loss_texts: 0.2218  loss_ctrl_points: 0.1117  loss_bd_points: 0.1542  loss_ce_0: 0.1949  loss_texts_0: 0.3309  loss_ctrl_points_0: 0.1227  loss_bd_points_0: 0.1632  loss_ce_1: 0.1849  loss_texts_1: 0.2708  loss_ctrl_points_1: 0.1184  loss_bd_points_1: 0.1553  loss_ce_2: 0.1788  loss_texts_2: 0.2478  loss_ctrl_points_2: 0.1196  loss_bd_points_2: 0.165  loss_ce_3: 0.174  loss_texts_3: 0.234  loss_ctrl_points_3: 0.1133  loss_bd_points_3: 0.1572  loss_ce_4: 0.172  loss_texts_4: 0.2207  loss_ctrl_points_4: 0.113  loss_bd_points_4: 0.1528  loss_ce_enc: 0.172  loss_bezier_enc: 0.1063  total_loss: 4.62    time: 7.4588  last_time: 6.5726  data_time: 0.0036  last_data_time: 0.0033   lr: 1e-05  max_mem: 7276M
[04/29 04:22:10] d2.utils.events INFO:  eta: 2 days, 8:04:36  iter: 879  loss_ce: 0.1498  loss_texts: 0.2111  loss_ctrl_points: 0.1138  loss_bd_points: 0.1438  loss_ce_0: 0.1801  loss_texts_0: 0.3202  loss_ctrl_points_0: 0.1191  loss_bd_points_0: 0.154  loss_ce_1: 0.1601  loss_texts_1: 0.2721  loss_ctrl_points_1: 0.1168  loss_bd_points_1: 0.1487  loss_ce_2: 0.1551  loss_texts_2: 0.2383  loss_ctrl_points_2: 0.1147  loss_bd_points_2: 0.1464  loss_ce_3: 0.1533  loss_texts_3: 0.2207  loss_ctrl_points_3: 0.1172  loss_bd_points_3: 0.146  loss_ce_4: 0.1535  loss_texts_4: 0.2095  loss_ctrl_points_4: 0.1143  loss_bd_points_4: 0.1449  loss_ce_enc: 0.1625  loss_bezier_enc: 0.11  total_loss: 4.056    time: 7.4469  last_time: 7.0454  data_time: 0.0035  last_data_time: 0.0036   lr: 1e-05  max_mem: 7276M
[04/29 04:22:10] d2.utils.events INFO:  eta: 2 days, 8:04:36  iter: 879  loss_ce: 0.1498  loss_texts: 0.2111  loss_ctrl_points: 0.1138  loss_bd_points: 0.1438  loss_ce_0: 0.1801  loss_texts_0: 0.3202  loss_ctrl_points_0: 0.1191  loss_bd_points_0: 0.154  loss_ce_1: 0.1601  loss_texts_1: 0.2721  loss_ctrl_points_1: 0.1168  loss_bd_points_1: 0.1487  loss_ce_2: 0.1551  loss_texts_2: 0.2383  loss_ctrl_points_2: 0.1147  loss_bd_points_2: 0.1464  loss_ce_3: 0.1533  loss_texts_3: 0.2207  loss_ctrl_points_3: 0.1172  loss_bd_points_3: 0.146  loss_ce_4: 0.1535  loss_texts_4: 0.2095  loss_ctrl_points_4: 0.1143  loss_bd_points_4: 0.1449  loss_ce_enc: 0.1625  loss_bezier_enc: 0.11  total_loss: 4.056    time: 7.4467  last_time: 7.1015  data_time: 0.0035  last_data_time: 0.0036   lr: 1e-05  max_mem: 7276M
[04/29 04:24:28] d2.utils.events INFO:  eta: 2 days, 7:55:55  iter: 899  loss_ce: 0.1563  loss_texts: 0.2004  loss_ctrl_points: 0.1235  loss_bd_points: 0.16  loss_ce_0: 0.195  loss_texts_0: 0.3093  loss_ctrl_points_0: 0.1254  loss_bd_points_0: 0.1645  loss_ce_1: 0.1682  loss_texts_1: 0.2652  loss_ctrl_points_1: 0.1308  loss_bd_points_1: 0.1664  loss_ce_2: 0.1601  loss_texts_2: 0.2219  loss_ctrl_points_2: 0.1278  loss_bd_points_2: 0.1732  loss_ce_3: 0.1567  loss_texts_3: 0.2054  loss_ctrl_points_3: 0.1283  loss_bd_points_3: 0.1747  loss_ce_4: 0.1575  loss_texts_4: 0.2031  loss_ctrl_points_4: 0.1265  loss_bd_points_4: 0.1665  loss_ce_enc: 0.1675  loss_bezier_enc: 0.1158  total_loss: 4.453    time: 7.4349  last_time: 6.8786  data_time: 0.0034  last_data_time: 0.0034   lr: 1e-05  max_mem: 7276M
[04/29 04:24:28] d2.utils.events INFO:  eta: 2 days, 7:55:55  iter: 899  loss_ce: 0.1563  loss_texts: 0.2004  loss_ctrl_points: 0.1235  loss_bd_points: 0.16  loss_ce_0: 0.195  loss_texts_0: 0.3093  loss_ctrl_points_0: 0.1254  loss_bd_points_0: 0.1645  loss_ce_1: 0.1682  loss_texts_1: 0.2652  loss_ctrl_points_1: 0.1308  loss_bd_points_1: 0.1664  loss_ce_2: 0.1601  loss_texts_2: 0.2219  loss_ctrl_points_2: 0.1278  loss_bd_points_2: 0.1732  loss_ce_3: 0.1567  loss_texts_3: 0.2054  loss_ctrl_points_3: 0.1283  loss_bd_points_3: 0.1747  loss_ce_4: 0.1575  loss_texts_4: 0.2031  loss_ctrl_points_4: 0.1265  loss_bd_points_4: 0.1665  loss_ce_enc: 0.1675  loss_bezier_enc: 0.1158  total_loss: 4.453    time: 7.4346  last_time: 6.9332  data_time: 0.0034  last_data_time: 0.0034   lr: 1e-05  max_mem: 7276M
[04/29 04:26:43] d2.utils.events INFO:  eta: 2 days, 7:45:39  iter: 919  loss_ce: 0.1411  loss_texts: 0.1938  loss_ctrl_points: 0.1159  loss_bd_points: 0.1537  loss_ce_0: 0.1668  loss_texts_0: 0.2979  loss_ctrl_points_0: 0.1216  loss_bd_points_0: 0.1665  loss_ce_1: 0.1472  loss_texts_1: 0.2558  loss_ctrl_points_1: 0.1173  loss_bd_points_1: 0.1539  loss_ce_2: 0.1395  loss_texts_2: 0.216  loss_ctrl_points_2: 0.1206  loss_bd_points_2: 0.159  loss_ce_3: 0.1423  loss_texts_3: 0.2069  loss_ctrl_points_3: 0.1182  loss_bd_points_3: 0.1524  loss_ce_4: 0.1428  loss_texts_4: 0.1942  loss_ctrl_points_4: 0.1165  loss_bd_points_4: 0.1544  loss_ce_enc: 0.1643  loss_bezier_enc: 0.1135  total_loss: 4.466    time: 7.4194  last_time: 7.0068  data_time: 0.0035  last_data_time: 0.0033   lr: 1e-05  max_mem: 7284M
[04/29 04:26:43] d2.utils.events INFO:  eta: 2 days, 7:45:57  iter: 919  loss_ce: 0.1411  loss_texts: 0.1938  loss_ctrl_points: 0.1159  loss_bd_points: 0.1537  loss_ce_0: 0.1668  loss_texts_0: 0.2979  loss_ctrl_points_0: 0.1216  loss_bd_points_0: 0.1665  loss_ce_1: 0.1472  loss_texts_1: 0.2558  loss_ctrl_points_1: 0.1173  loss_bd_points_1: 0.1539  loss_ce_2: 0.1395  loss_texts_2: 0.216  loss_ctrl_points_2: 0.1206  loss_bd_points_2: 0.159  loss_ce_3: 0.1423  loss_texts_3: 0.2069  loss_ctrl_points_3: 0.1182  loss_bd_points_3: 0.1524  loss_ce_4: 0.1428  loss_texts_4: 0.1942  loss_ctrl_points_4: 0.1165  loss_bd_points_4: 0.1544  loss_ce_enc: 0.1643  loss_bezier_enc: 0.1135  total_loss: 4.466    time: 7.4192  last_time: 7.0662  data_time: 0.0035  last_data_time: 0.0033   lr: 1e-05  max_mem: 7284M
[04/29 04:29:01] d2.utils.events INFO:  eta: 2 days, 7:48:48  iter: 939  loss_ce: 0.157  loss_texts: 0.1797  loss_ctrl_points: 0.1028  loss_bd_points: 0.1378  loss_ce_0: 0.1753  loss_texts_0: 0.2475  loss_ctrl_points_0: 0.1118  loss_bd_points_0: 0.1502  loss_ce_1: 0.1732  loss_texts_1: 0.216  loss_ctrl_points_1: 0.1054  loss_bd_points_1: 0.1432  loss_ce_2: 0.1667  loss_texts_2: 0.1966  loss_ctrl_points_2: 0.1097  loss_bd_points_2: 0.1462  loss_ce_3: 0.1682  loss_texts_3: 0.1888  loss_ctrl_points_3: 0.1098  loss_bd_points_3: 0.1528  loss_ce_4: 0.1592  loss_texts_4: 0.1821  loss_ctrl_points_4: 0.1031  loss_bd_points_4: 0.1362  loss_ce_enc: 0.1615  loss_bezier_enc: 0.1033  total_loss: 3.892    time: 7.4078  last_time: 7.5054  data_time: 0.0036  last_data_time: 0.0038   lr: 1e-05  max_mem: 7284M
[04/29 04:29:01] d2.utils.events INFO:  eta: 2 days, 7:48:48  iter: 939  loss_ce: 0.157  loss_texts: 0.1797  loss_ctrl_points: 0.1028  loss_bd_points: 0.1378  loss_ce_0: 0.1753  loss_texts_0: 0.2475  loss_ctrl_points_0: 0.1118  loss_bd_points_0: 0.1502  loss_ce_1: 0.1732  loss_texts_1: 0.216  loss_ctrl_points_1: 0.1054  loss_bd_points_1: 0.1432  loss_ce_2: 0.1667  loss_texts_2: 0.1966  loss_ctrl_points_2: 0.1097  loss_bd_points_2: 0.1462  loss_ce_3: 0.1682  loss_texts_3: 0.1888  loss_ctrl_points_3: 0.1098  loss_bd_points_3: 0.1528  loss_ce_4: 0.1592  loss_texts_4: 0.1821  loss_ctrl_points_4: 0.1031  loss_bd_points_4: 0.1362  loss_ce_enc: 0.1615  loss_bezier_enc: 0.1033  total_loss: 3.892    time: 7.4079  last_time: 7.5639  data_time: 0.0036  last_data_time: 0.0038   lr: 1e-05  max_mem: 7284M
[04/29 04:31:21] d2.utils.events INFO:  eta: 2 days, 7:46:37  iter: 959  loss_ce: 0.1564  loss_texts: 0.189  loss_ctrl_points: 0.1108  loss_bd_points: 0.1466  loss_ce_0: 0.1861  loss_texts_0: 0.3075  loss_ctrl_points_0: 0.1176  loss_bd_points_0: 0.153  loss_ce_1: 0.1707  loss_texts_1: 0.256  loss_ctrl_points_1: 0.1125  loss_bd_points_1: 0.1466  loss_ce_2: 0.1623  loss_texts_2: 0.2232  loss_ctrl_points_2: 0.1105  loss_bd_points_2: 0.1447  loss_ce_3: 0.1607  loss_texts_3: 0.1944  loss_ctrl_points_3: 0.1116  loss_bd_points_3: 0.1478  loss_ce_4: 0.1574  loss_texts_4: 0.1865  loss_ctrl_points_4: 0.1106  loss_bd_points_4: 0.1422  loss_ce_enc: 0.1769  loss_bezier_enc: 0.1125  total_loss: 4.249    time: 7.3992  last_time: 6.6795  data_time: 0.0035  last_data_time: 0.0033   lr: 1e-05  max_mem: 7284M
[04/29 04:31:21] d2.utils.events INFO:  eta: 2 days, 7:46:30  iter: 959  loss_ce: 0.1564  loss_texts: 0.189  loss_ctrl_points: 0.1108  loss_bd_points: 0.1466  loss_ce_0: 0.1861  loss_texts_0: 0.3075  loss_ctrl_points_0: 0.1176  loss_bd_points_0: 0.153  loss_ce_1: 0.1707  loss_texts_1: 0.256  loss_ctrl_points_1: 0.1125  loss_bd_points_1: 0.1466  loss_ce_2: 0.1623  loss_texts_2: 0.2232  loss_ctrl_points_2: 0.1105  loss_bd_points_2: 0.1447  loss_ce_3: 0.1607  loss_texts_3: 0.1944  loss_ctrl_points_3: 0.1116  loss_bd_points_3: 0.1478  loss_ce_4: 0.1574  loss_texts_4: 0.1865  loss_ctrl_points_4: 0.1106  loss_bd_points_4: 0.1422  loss_ce_enc: 0.1769  loss_bezier_enc: 0.1125  total_loss: 4.249    time: 7.3989  last_time: 6.7319  data_time: 0.0035  last_data_time: 0.0033   lr: 1e-05  max_mem: 7284M
[04/29 04:33:42] d2.utils.events INFO:  eta: 2 days, 7:44:19  iter: 979  loss_ce: 0.1546  loss_texts: 0.2443  loss_ctrl_points: 0.1285  loss_bd_points: 0.166  loss_ce_0: 0.1827  loss_texts_0: 0.3369  loss_ctrl_points_0: 0.1323  loss_bd_points_0: 0.1788  loss_ce_1: 0.1714  loss_texts_1: 0.3054  loss_ctrl_points_1: 0.1391  loss_bd_points_1: 0.1712  loss_ce_2: 0.1579  loss_texts_2: 0.2759  loss_ctrl_points_2: 0.1317  loss_bd_points_2: 0.1743  loss_ce_3: 0.1572  loss_texts_3: 0.2608  loss_ctrl_points_3: 0.1301  loss_bd_points_3: 0.1722  loss_ce_4: 0.156  loss_texts_4: 0.255  loss_ctrl_points_4: 0.1295  loss_bd_points_4: 0.1671  loss_ce_enc: 0.1704  loss_bezier_enc: 0.1072  total_loss: 5.016    time: 7.3926  last_time: 6.6661  data_time: 0.0037  last_data_time: 0.0033   lr: 1e-05  max_mem: 7284M
[04/29 04:33:42] d2.utils.events INFO:  eta: 2 days, 7:44:12  iter: 979  loss_ce: 0.1546  loss_texts: 0.2443  loss_ctrl_points: 0.1285  loss_bd_points: 0.166  loss_ce_0: 0.1827  loss_texts_0: 0.3369  loss_ctrl_points_0: 0.1323  loss_bd_points_0: 0.1788  loss_ce_1: 0.1714  loss_texts_1: 0.3054  loss_ctrl_points_1: 0.1391  loss_bd_points_1: 0.1712  loss_ce_2: 0.1579  loss_texts_2: 0.2759  loss_ctrl_points_2: 0.1317  loss_bd_points_2: 0.1743  loss_ce_3: 0.1572  loss_texts_3: 0.2608  loss_ctrl_points_3: 0.1301  loss_bd_points_3: 0.1722  loss_ce_4: 0.156  loss_texts_4: 0.255  loss_ctrl_points_4: 0.1295  loss_bd_points_4: 0.1671  loss_ce_enc: 0.1704  loss_bezier_enc: 0.1072  total_loss: 5.016    time: 7.3923  last_time: 6.7164  data_time: 0.0037  last_data_time: 0.0033   lr: 1e-05  max_mem: 7284M
[04/29 04:36:02] fvcore.common.checkpoint INFO: Saving checkpoint to output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth
[04/29 04:36:02] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1440, 1440), max_size=4000, sample_style='choice')]
[04/29 04:36:02] adet.data.dataset_mapper INFO: Rebuilding the augmentations. The previous augmentations will be overridden.
[04/29 04:36:02] adet.data.datasets.text INFO: Loaded 500 images in COCO format from datasets/ic15/test.json
[04/29 04:36:02] d2.data.build INFO: Distribution of instances among all 1 categories:
[36m|  category  | #instances   |
|:----------:|:-------------|
|    text    | 0            |
|            |              |[0m
[04/29 04:36:02] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[04/29 04:36:02] d2.data.common INFO: Serializing 500 elements to byte tensors and concatenating them all ...
[04/29 04:36:02] d2.data.common INFO: Serialized dataset takes 0.06 MiB
[04/29 04:36:02] d2.evaluation.evaluator INFO: Start inference on 500 batches
[04/29 04:36:08] d2.evaluation.evaluator INFO: Inference done 11/500. Dataloading: 0.0010 s/iter. Inference: 0.4582 s/iter. Eval: 0.0005 s/iter. Total: 0.4597 s/iter. ETA=0:03:44
[04/29 04:36:13] d2.evaluation.evaluator INFO: Inference done 22/500. Dataloading: 0.0013 s/iter. Inference: 0.4589 s/iter. Eval: 0.0005 s/iter. Total: 0.4608 s/iter. ETA=0:03:40
[04/29 04:36:18] d2.evaluation.evaluator INFO: Inference done 33/500. Dataloading: 0.0014 s/iter. Inference: 0.4590 s/iter. Eval: 0.0005 s/iter. Total: 0.4610 s/iter. ETA=0:03:35
[04/29 04:36:23] d2.evaluation.evaluator INFO: Inference done 44/500. Dataloading: 0.0015 s/iter. Inference: 0.4591 s/iter. Eval: 0.0004 s/iter. Total: 0.4611 s/iter. ETA=0:03:30
[04/29 04:36:28] d2.evaluation.evaluator INFO: Inference done 55/500. Dataloading: 0.0016 s/iter. Inference: 0.4592 s/iter. Eval: 0.0004 s/iter. Total: 0.4613 s/iter. ETA=0:03:25
[04/29 04:36:33] d2.evaluation.evaluator INFO: Inference done 66/500. Dataloading: 0.0016 s/iter. Inference: 0.4593 s/iter. Eval: 0.0004 s/iter. Total: 0.4613 s/iter. ETA=0:03:20
[04/29 04:36:38] d2.evaluation.evaluator INFO: Inference done 77/500. Dataloading: 0.0016 s/iter. Inference: 0.4593 s/iter. Eval: 0.0004 s/iter. Total: 0.4613 s/iter. ETA=0:03:15
[04/29 04:36:43] d2.evaluation.evaluator INFO: Inference done 88/500. Dataloading: 0.0016 s/iter. Inference: 0.4593 s/iter. Eval: 0.0005 s/iter. Total: 0.4613 s/iter. ETA=0:03:10
[04/29 04:36:48] d2.evaluation.evaluator INFO: Inference done 99/500. Dataloading: 0.0016 s/iter. Inference: 0.4593 s/iter. Eval: 0.0004 s/iter. Total: 0.4613 s/iter. ETA=0:03:04
[04/29 04:36:54] d2.evaluation.evaluator INFO: Inference done 110/500. Dataloading: 0.0015 s/iter. Inference: 0.4594 s/iter. Eval: 0.0005 s/iter. Total: 0.4614 s/iter. ETA=0:02:59
[04/29 04:36:59] d2.evaluation.evaluator INFO: Inference done 121/500. Dataloading: 0.0015 s/iter. Inference: 0.4594 s/iter. Eval: 0.0005 s/iter. Total: 0.4614 s/iter. ETA=0:02:54
[04/29 04:37:04] d2.evaluation.evaluator INFO: Inference done 132/500. Dataloading: 0.0015 s/iter. Inference: 0.4594 s/iter. Eval: 0.0005 s/iter. Total: 0.4614 s/iter. ETA=0:02:49
[04/29 04:37:09] d2.evaluation.evaluator INFO: Inference done 143/500. Dataloading: 0.0015 s/iter. Inference: 0.4594 s/iter. Eval: 0.0005 s/iter. Total: 0.4614 s/iter. ETA=0:02:44
[04/29 04:37:14] d2.evaluation.evaluator INFO: Inference done 154/500. Dataloading: 0.0015 s/iter. Inference: 0.4594 s/iter. Eval: 0.0004 s/iter. Total: 0.4614 s/iter. ETA=0:02:39
[04/29 04:37:19] d2.evaluation.evaluator INFO: Inference done 165/500. Dataloading: 0.0015 s/iter. Inference: 0.4594 s/iter. Eval: 0.0004 s/iter. Total: 0.4614 s/iter. ETA=0:02:34
[04/29 04:37:24] d2.evaluation.evaluator INFO: Inference done 176/500. Dataloading: 0.0015 s/iter. Inference: 0.4594 s/iter. Eval: 0.0004 s/iter. Total: 0.4614 s/iter. ETA=0:02:29
[04/29 04:37:29] d2.evaluation.evaluator INFO: Inference done 187/500. Dataloading: 0.0015 s/iter. Inference: 0.4594 s/iter. Eval: 0.0004 s/iter. Total: 0.4614 s/iter. ETA=0:02:24
[04/29 04:37:34] d2.evaluation.evaluator INFO: Inference done 198/500. Dataloading: 0.0015 s/iter. Inference: 0.4594 s/iter. Eval: 0.0004 s/iter. Total: 0.4614 s/iter. ETA=0:02:19
[04/29 04:37:39] d2.evaluation.evaluator INFO: Inference done 209/500. Dataloading: 0.0015 s/iter. Inference: 0.4594 s/iter. Eval: 0.0004 s/iter. Total: 0.4614 s/iter. ETA=0:02:14
[04/29 04:37:44] d2.evaluation.evaluator INFO: Inference done 220/500. Dataloading: 0.0015 s/iter. Inference: 0.4594 s/iter. Eval: 0.0004 s/iter. Total: 0.4614 s/iter. ETA=0:02:09
[04/29 04:37:49] d2.evaluation.evaluator INFO: Inference done 231/500. Dataloading: 0.0015 s/iter. Inference: 0.4594 s/iter. Eval: 0.0004 s/iter. Total: 0.4614 s/iter. ETA=0:02:04
[04/29 04:37:54] d2.evaluation.evaluator INFO: Inference done 242/500. Dataloading: 0.0015 s/iter. Inference: 0.4594 s/iter. Eval: 0.0004 s/iter. Total: 0.4614 s/iter. ETA=0:01:59
[04/29 04:37:59] d2.evaluation.evaluator INFO: Inference done 253/500. Dataloading: 0.0015 s/iter. Inference: 0.4594 s/iter. Eval: 0.0004 s/iter. Total: 0.4614 s/iter. ETA=0:01:53
[04/29 04:38:05] d2.evaluation.evaluator INFO: Inference done 264/500. Dataloading: 0.0015 s/iter. Inference: 0.4594 s/iter. Eval: 0.0004 s/iter. Total: 0.4614 s/iter. ETA=0:01:48
[04/29 04:38:10] d2.evaluation.evaluator INFO: Inference done 275/500. Dataloading: 0.0015 s/iter. Inference: 0.4594 s/iter. Eval: 0.0004 s/iter. Total: 0.4614 s/iter. ETA=0:01:43
[04/29 04:38:15] d2.evaluation.evaluator INFO: Inference done 286/500. Dataloading: 0.0015 s/iter. Inference: 0.4594 s/iter. Eval: 0.0004 s/iter. Total: 0.4614 s/iter. ETA=0:01:38
[04/29 04:38:20] d2.evaluation.evaluator INFO: Inference done 297/500. Dataloading: 0.0015 s/iter. Inference: 0.4594 s/iter. Eval: 0.0004 s/iter. Total: 0.4614 s/iter. ETA=0:01:33
[04/29 04:38:25] d2.evaluation.evaluator INFO: Inference done 308/500. Dataloading: 0.0015 s/iter. Inference: 0.4594 s/iter. Eval: 0.0004 s/iter. Total: 0.4614 s/iter. ETA=0:01:28
[04/29 04:38:30] d2.evaluation.evaluator INFO: Inference done 319/500. Dataloading: 0.0015 s/iter. Inference: 0.4594 s/iter. Eval: 0.0004 s/iter. Total: 0.4614 s/iter. ETA=0:01:23
[04/29 04:38:35] d2.evaluation.evaluator INFO: Inference done 330/500. Dataloading: 0.0015 s/iter. Inference: 0.4594 s/iter. Eval: 0.0004 s/iter. Total: 0.4614 s/iter. ETA=0:01:18
[04/29 04:38:40] d2.evaluation.evaluator INFO: Inference done 341/500. Dataloading: 0.0015 s/iter. Inference: 0.4594 s/iter. Eval: 0.0004 s/iter. Total: 0.4614 s/iter. ETA=0:01:13
[04/29 04:38:45] d2.evaluation.evaluator INFO: Inference done 352/500. Dataloading: 0.0015 s/iter. Inference: 0.4594 s/iter. Eval: 0.0004 s/iter. Total: 0.4614 s/iter. ETA=0:01:08
[04/29 04:38:50] d2.evaluation.evaluator INFO: Inference done 363/500. Dataloading: 0.0015 s/iter. Inference: 0.4594 s/iter. Eval: 0.0004 s/iter. Total: 0.4614 s/iter. ETA=0:01:03
[04/29 04:38:55] d2.evaluation.evaluator INFO: Inference done 374/500. Dataloading: 0.0015 s/iter. Inference: 0.4594 s/iter. Eval: 0.0004 s/iter. Total: 0.4614 s/iter. ETA=0:00:58
[04/29 04:39:00] d2.evaluation.evaluator INFO: Inference done 385/500. Dataloading: 0.0015 s/iter. Inference: 0.4594 s/iter. Eval: 0.0004 s/iter. Total: 0.4614 s/iter. ETA=0:00:53
[04/29 04:39:05] d2.evaluation.evaluator INFO: Inference done 396/500. Dataloading: 0.0016 s/iter. Inference: 0.4594 s/iter. Eval: 0.0004 s/iter. Total: 0.4614 s/iter. ETA=0:00:47
[04/29 04:39:11] d2.evaluation.evaluator INFO: Inference done 407/500. Dataloading: 0.0015 s/iter. Inference: 0.4594 s/iter. Eval: 0.0004 s/iter. Total: 0.4614 s/iter. ETA=0:00:42
[04/29 04:39:16] d2.evaluation.evaluator INFO: Inference done 418/500. Dataloading: 0.0015 s/iter. Inference: 0.4594 s/iter. Eval: 0.0004 s/iter. Total: 0.4614 s/iter. ETA=0:00:37
[04/29 04:39:21] d2.evaluation.evaluator INFO: Inference done 429/500. Dataloading: 0.0015 s/iter. Inference: 0.4594 s/iter. Eval: 0.0004 s/iter. Total: 0.4614 s/iter. ETA=0:00:32
[04/29 04:39:26] d2.evaluation.evaluator INFO: Inference done 440/500. Dataloading: 0.0015 s/iter. Inference: 0.4594 s/iter. Eval: 0.0004 s/iter. Total: 0.4614 s/iter. ETA=0:00:27
[04/29 04:39:31] d2.evaluation.evaluator INFO: Inference done 451/500. Dataloading: 0.0015 s/iter. Inference: 0.4594 s/iter. Eval: 0.0004 s/iter. Total: 0.4614 s/iter. ETA=0:00:22
[04/29 04:39:36] d2.evaluation.evaluator INFO: Inference done 462/500. Dataloading: 0.0015 s/iter. Inference: 0.4594 s/iter. Eval: 0.0004 s/iter. Total: 0.4614 s/iter. ETA=0:00:17
[04/29 04:39:41] d2.evaluation.evaluator INFO: Inference done 473/500. Dataloading: 0.0015 s/iter. Inference: 0.4594 s/iter. Eval: 0.0004 s/iter. Total: 0.4614 s/iter. ETA=0:00:12
[04/29 04:39:46] d2.evaluation.evaluator INFO: Inference done 484/500. Dataloading: 0.0015 s/iter. Inference: 0.4594 s/iter. Eval: 0.0004 s/iter. Total: 0.4614 s/iter. ETA=0:00:07
[04/29 04:39:51] d2.evaluation.evaluator INFO: Inference done 495/500. Dataloading: 0.0015 s/iter. Inference: 0.4594 s/iter. Eval: 0.0004 s/iter. Total: 0.4614 s/iter. ETA=0:00:02
[04/29 04:39:53] d2.evaluation.evaluator INFO: Total inference time: 0:03:48.440623 (0.461496 s / iter per device, on 1 devices)
[04/29 04:39:53] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:03:47 (0.459393 s / iter per device, on 1 devices)
[04/29 04:39:53] adet.evaluation.text_evaluation_all INFO: Saving results to output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/inference/text_results.json
[04/29 13:03:22] detectron2 INFO: Rank of current process: 0. World size: 1
[04/29 13:03:25] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/kylee/DeepSolo_LangPriors/detectron2/detectron2
Compiler                         GCC 9.5
CUDA compiler                    CUDA 11.3
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.12.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 3060 (arch=8.6)
Driver version                   550.120
CUDA_HOME                        /usr
Pillow                           9.0.1
torchvision                      0.13.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[04/29 13:03:25] detectron2 INFO: Command line arguments: Namespace(config_file='configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml', resume=True, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50152', opts=[])
[04/29 13:03:25] detectron2 INFO: Contents of args.config_file=configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml:
_BASE_: "../Base_det.yaml"

MODEL:
  WEIGHTS: "output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth"
  TRANSFORMER:
    INFERENCE_TH_TEST: 0.3

DATASETS:
  TRAIN: ("ic15_train",)
  TEST: ("ic15_test",)

INPUT:
  MIN_SIZE_TRAIN: (640, 700, 760, 820, 880, 940, 1000)
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MAX_SIZE_TEST: 4000
  CROP:
    ENABLED: False
  ROTATE: False

SOLVER:
  AMP:
    ENABLED: True  # Enable Automatic Mixed Precision
  IMS_PER_BATCH: 2
  BASE_LR: 1e-5
  LR_BACKBONE: 1e-6
  WARMUP_ITERS: 250
  STEPS: (15000, 25000)  # no step
  MAX_ITER: 30000
  GAMMA: 0.1
  CHECKPOINT_PERIOD: 1000

TEST:
  EVAL_PERIOD: 1000
  # 1 - Generic, 2 - Weak, 3 - Strong (for icdar2015)
  LEXICON_TYPE: 3

OUTPUT_DIR: "output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15"

[04/29 13:03:25] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ic15_test
  TRAIN:
  - ic15_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    CROP_INSTANCE: false
    ENABLED: false
    SIZE:
    - 0.1
    - 0.1
    TYPE: relative_range
  FORMAT: RGB
  HFLIP_TRAIN: false
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 4000
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MIN_SIZE_TRAIN:
  - 640
  - 700
  - 760
  - 820
  - 880
  - 940
  - 1000
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  ROTATE: false
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    ANTI_ALIAS: false
    FREEZE_AT: 2
    NAME: build_resnet_backbone
  BASIS_MODULE:
    ANN_SET: coco
    COMMON_STRIDE: 8
    CONVS_DIM: 128
    IN_FEATURES:
    - p3
    - p4
    - p5
    LOSS_ON: false
    LOSS_WEIGHT: 0.3
    NAME: ProtoNet
    NORM: SyncBN
    NUM_BASES: 4
    NUM_CLASSES: 80
    NUM_CONVS: 3
  BATEXT:
    CANONICAL_SIZE: 96
    CONV_DIM: 256
    CUSTOM_DICT: ''
    IN_FEATURES:
    - p2
    - p3
    - p4
    NUM_CHARS: 25
    NUM_CONV: 2
    POOLER_RESOLUTION:
    - 8
    - 32
    POOLER_SCALES:
    - 0.25
    - 0.125
    - 0.0625
    RECOGNITION_LOSS: ctc
    RECOGNIZER: attn
    SAMPLING_RATIO: 1
    USE_AET: false
    USE_COORDCONV: false
    VOC_SIZE: 96
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: false
  META_ARCHITECTURE: TransformerPureDetector
  MOBILENET: false
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_INTERVAL: 1
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  SWIN:
    DROP_PATH_RATE: 0.2
    TYPE: tiny
  TOP_MODULE:
    DIM: 16
    NAME: conv
  TRANSFORMER:
    AUX_LOSS: true
    BOUNDARY_HEAD: true
    CUSTOM_DICT: ''
    DEC_LAYERS: 6
    DEC_N_POINTS: 4
    DIM_FEEDFORWARD: 1024
    DROPOUT: 0.0
    ENABLED: true
    ENC_LAYERS: 6
    ENC_N_POINTS: 4
    HIDDEN_DIM: 256
    INFERENCE_TH_TEST: 0.3
    LOSS:
      AUX_LOSS: true
      BEZIER_CLASS_WEIGHT: 1.0
      BEZIER_COORD_WEIGHT: 1.0
      BEZIER_SAMPLE_POINTS: 25
      BOUNDARY_WEIGHT: 0.5
      FOCAL_ALPHA: 0.25
      FOCAL_GAMMA: 2.0
      POINT_CLASS_WEIGHT: 1.0
      POINT_COORD_WEIGHT: 1.0
      POINT_TEXT_WEIGHT: 0.5
    NHEADS: 8
    NUM_FEATURE_LEVELS: 4
    NUM_POINTS: 25
    NUM_QUERIES: 100
    POSITION_EMBEDDING_SCALE: 6.283185307179586
    TEMPERATURE: 10000
    VOC_SIZE: 37
  ViTAEv2:
    DROP_PATH_RATE: 0.2
    TYPE: vitaev2_s
  WEIGHTS: output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth
OUTPUT_DIR: output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15
SEED: 42
SOLVER:
  AMP:
    ENABLED: true
  BASE_LR: 1.0e-05
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.1
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 2
  LR_BACKBONE: 1.0e-06
  LR_BACKBONE_NAMES:
  - backbone.0
  LR_LINEAR_PROJ_MULT: 1.0
  LR_LINEAR_PROJ_NAMES:
  - reference_points
  - sampling_offsets
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 30000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 15000
  - 25000
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 250
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  LEXICON_TYPE: 3
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/29 13:03:25] detectron2 INFO: Full config saved to output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/config.yaml
[04/29 13:03:42] d2.engine.defaults INFO: Model:
TransformerPureDetector(
  (detection_transformer): DETECTION_TRANSFORMER(
    (backbone): Joiner(
      (0): MaskedBackbone(
        (backbone): ResNet(
          (stem): BasicStem(
            (conv1): Conv2d(
              3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
          )
          (res2): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv1): Conv2d(
                64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
          )
          (res3): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv1): Conv2d(
                256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
          )
          (res4): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
              (conv1): Conv2d(
                512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (4): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (5): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
          )
          (res5): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
              (conv1): Conv2d(
                1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
          )
        )
      )
      (1): PositionalEncoding2D()
    )
    (point_embed): Embedding(2500, 256)
    (transformer): DeformableTransformer(
      (encoder): DeformableTransformerEncoder(
        (layers): ModuleList(
          (0): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (decoder): DeformableCompositeTransformerDecoder(
        (layers): ModuleList(
          (0): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ctrl_point_coord): ModuleList(
          (0): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (1): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (2): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (3): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (4): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (5): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
        )
      )
      (enc_output): Linear(in_features=256, out_features=256, bias=True)
      (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (bezier_coord_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=8, bias=True)
        )
      )
      (bezier_class_embed): Linear(in_features=256, out_features=1, bias=True)
    )
    (input_proj): ModuleList(
      (0): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (1): Sequential(
        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (2): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (3): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (bezier_proposal_coord): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=8, bias=True)
      )
    )
    (bezier_proposal_class): Linear(in_features=256, out_features=1, bias=True)
    (ctrl_point_coord): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
    )
    (ctrl_point_class): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (ctrl_point_text): ModuleList(
      (0): Linear(in_features=256, out_features=38, bias=True)
      (1): Linear(in_features=256, out_features=38, bias=True)
      (2): Linear(in_features=256, out_features=38, bias=True)
      (3): Linear(in_features=256, out_features=38, bias=True)
      (4): Linear(in_features=256, out_features=38, bias=True)
      (5): Linear(in_features=256, out_features=38, bias=True)
    )
    (boundary_offset): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (enc_matcher): BezierHungarianMatcher(
      (bezier_sampler): BezierSampler()
    )
    (dec_matcher): CtrlPointHungarianMatcher()
    (bezier_sampler): BezierSampler()
  )
)
[04/29 13:03:42] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 700, 760, 820, 880, 940, 1000), max_size=2000, sample_style='choice'), RandomFlip()]
[04/29 13:03:42] adet.data.dataset_mapper INFO: Rebuilding the augmentations. The previous augmentations will be overridden.
[04/29 13:03:42] adet.data.detection_utils INFO: Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 700, 760, 820, 880, 940, 1000), max_size=2000, sample_style='choice'), RandomContrast(intensity_min=0.3, intensity_max=1.7), RandomBrightness(intensity_min=0.3, intensity_max=1.7), RandomLighting(scale=1.1394267984578836), RandomSaturation(intensity_min=0.3, intensity_max=1.7)]
[04/29 13:03:42] adet.data.datasets.text INFO: Loaded 1000 images in COCO format from datasets/ic15/train_37voc.json
[04/29 13:03:42] d2.data.build INFO: Removed 0 images with no usable annotations. 979 images left.
[04/29 13:03:42] d2.data.build INFO: Distribution of instances among all 1 categories:
[36m|  category  | #instances   |
|:----------:|:-------------|
|    text    | 4441         |
|            |              |[0m
[04/29 13:03:42] d2.data.build INFO: Using training sampler TrainingSampler
[04/29 13:03:42] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[04/29 13:03:42] d2.data.common INFO: Serializing 979 elements to byte tensors and concatenating them all ...
[04/29 13:03:42] d2.data.common INFO: Serialized dataset takes 6.51 MiB
[04/29 13:03:42] d2.data.build INFO: Making batched data loader with batch_size=2
[04/29 13:03:42] d2.engine.defaults INFO: Model:
TransformerPureDetector(
  (detection_transformer): DETECTION_TRANSFORMER(
    (backbone): Joiner(
      (0): MaskedBackbone(
        (backbone): ResNet(
          (stem): BasicStem(
            (conv1): Conv2d(
              3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
          )
          (res2): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv1): Conv2d(
                64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
          )
          (res3): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv1): Conv2d(
                256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
          )
          (res4): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
              (conv1): Conv2d(
                512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (4): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (5): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
          )
          (res5): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
              (conv1): Conv2d(
                1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
          )
        )
      )
      (1): PositionalEncoding2D()
    )
    (point_embed): Embedding(2500, 256)
    (transformer): DeformableTransformer(
      (encoder): DeformableTransformerEncoder(
        (layers): ModuleList(
          (0): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (decoder): DeformableCompositeTransformerDecoder(
        (layers): ModuleList(
          (0): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ctrl_point_coord): ModuleList(
          (0): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (1): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (2): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (3): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (4): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (5): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
        )
      )
      (enc_output): Linear(in_features=256, out_features=256, bias=True)
      (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (bezier_coord_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=8, bias=True)
        )
      )
      (bezier_class_embed): Linear(in_features=256, out_features=1, bias=True)
    )
    (input_proj): ModuleList(
      (0): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (1): Sequential(
        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (2): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (3): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (bezier_proposal_coord): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=8, bias=True)
      )
    )
    (bezier_proposal_class): Linear(in_features=256, out_features=1, bias=True)
    (ctrl_point_coord): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
    )
    (ctrl_point_class): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (ctrl_point_text): ModuleList(
      (0): Linear(in_features=256, out_features=38, bias=True)
      (1): Linear(in_features=256, out_features=38, bias=True)
      (2): Linear(in_features=256, out_features=38, bias=True)
      (3): Linear(in_features=256, out_features=38, bias=True)
      (4): Linear(in_features=256, out_features=38, bias=True)
      (5): Linear(in_features=256, out_features=38, bias=True)
    )
    (boundary_offset): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (enc_matcher): BezierHungarianMatcher(
      (bezier_sampler): BezierSampler()
    )
    (dec_matcher): CtrlPointHungarianMatcher()
    (bezier_sampler): BezierSampler()
  )
)
[04/29 13:03:42] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 700, 760, 820, 880, 940, 1000), max_size=2000, sample_style='choice'), RandomFlip()]
[04/29 13:03:42] adet.data.dataset_mapper INFO: Rebuilding the augmentations. The previous augmentations will be overridden.
[04/29 13:03:42] adet.data.detection_utils INFO: Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 700, 760, 820, 880, 940, 1000), max_size=2000, sample_style='choice'), RandomContrast(intensity_min=0.3, intensity_max=1.7), RandomBrightness(intensity_min=0.3, intensity_max=1.7), RandomLighting(scale=0.5250107552226669), RandomSaturation(intensity_min=0.3, intensity_max=1.7)]
[04/29 13:03:42] adet.data.datasets.text INFO: Loaded 1000 images in COCO format from datasets/ic15/train_37voc.json
[04/29 13:03:43] d2.data.build INFO: Removed 0 images with no usable annotations. 979 images left.
[04/29 13:03:43] d2.data.build INFO: Using training sampler TrainingSampler
[04/29 13:03:43] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[04/29 13:03:43] d2.data.common INFO: Serializing 979 elements to byte tensors and concatenating them all ...
[04/29 13:03:43] d2.data.common INFO: Serialized dataset takes 6.51 MiB
[04/29 13:03:43] d2.data.build INFO: Making batched data loader with batch_size=2
[04/29 13:03:43] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth ...
[04/29 13:03:43] fvcore.common.checkpoint INFO: [Checkpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth ...
[04/29 13:03:46] adet.trainer INFO: Starting training from iteration 1000
[04/29 13:08:03] d2.utils.events INFO:  eta: 2 days, 8:41:50  iter: 1019  loss_ce: 1.142  loss_texts: 8.316  loss_ctrl_points: 16.51  loss_bd_points: 16.56  loss_ce_0: 1.089  loss_texts_0: 8.635  loss_ctrl_points_0: 16.52  loss_bd_points_0: 16.57  loss_ce_1: 1.003  loss_texts_1: 7.653  loss_ctrl_points_1: 16.55  loss_bd_points_1: 16.59  loss_ce_2: 1.074  loss_texts_2: 8.2  loss_ctrl_points_2: 16.52  loss_bd_points_2: 16.55  loss_ce_3: 1.072  loss_texts_3: 7.958  loss_ctrl_points_3: 16.52  loss_bd_points_3: 16.58  loss_ce_4: 1.22  loss_texts_4: 8.016  loss_ctrl_points_4: 16.49  loss_bd_points_4: 16.54  loss_ce_enc: 1.055  loss_bezier_enc: 0.5691  total_loss: 256.8    time: 6.8831  last_time: 6.0213  data_time: 0.0113  last_data_time: 0.0030   lr: 1.5285e-06  max_mem: 6977M
[04/29 13:08:05] d2.utils.events INFO:  eta: 2 days, 8:41:58  iter: 1019  loss_ce: 1.142  loss_texts: 8.316  loss_ctrl_points: 16.51  loss_bd_points: 16.56  loss_ce_0: 1.089  loss_texts_0: 8.635  loss_ctrl_points_0: 16.52  loss_bd_points_0: 16.57  loss_ce_1: 1.003  loss_texts_1: 7.653  loss_ctrl_points_1: 16.55  loss_bd_points_1: 16.59  loss_ce_2: 1.074  loss_texts_2: 8.2  loss_ctrl_points_2: 16.52  loss_bd_points_2: 16.55  loss_ce_3: 1.072  loss_texts_3: 7.958  loss_ctrl_points_3: 16.52  loss_bd_points_3: 16.58  loss_ce_4: 1.22  loss_texts_4: 8.016  loss_ctrl_points_4: 16.49  loss_bd_points_4: 16.54  loss_ce_enc: 1.055  loss_bezier_enc: 0.5691  total_loss: 256.8    time: 6.9215  last_time: 8.2651  data_time: 0.0113  last_data_time: 0.0030   lr: 1.5684e-06  max_mem: 6977M
[04/29 13:10:28] d2.utils.events INFO:  eta: 2 days, 9:14:51  iter: 1039  loss_ce: 1.144  loss_texts: 8.034  loss_ctrl_points: 16.08  loss_bd_points: 16.09  loss_ce_0: 1.092  loss_texts_0: 8.414  loss_ctrl_points_0: 16.13  loss_bd_points_0: 16.15  loss_ce_1: 1.002  loss_texts_1: 7.48  loss_ctrl_points_1: 16.15  loss_bd_points_1: 16.16  loss_ce_2: 1.078  loss_texts_2: 8.012  loss_ctrl_points_2: 16.14  loss_bd_points_2: 16.15  loss_ce_3: 1.074  loss_texts_3: 7.786  loss_ctrl_points_3: 16.08  loss_bd_points_3: 16.1  loss_ce_4: 1.213  loss_texts_4: 7.799  loss_ctrl_points_4: 16.09  loss_bd_points_4: 16.1  loss_ce_enc: 1.05  loss_bezier_enc: 0.5458  total_loss: 252.2    time: 7.0358  last_time: 7.4951  data_time: 0.0032  last_data_time: 0.0032   lr: 3.1269e-06  max_mem: 6977M
[04/29 13:10:28] d2.utils.events INFO:  eta: 2 days, 9:14:56  iter: 1039  loss_ce: 1.144  loss_texts: 8.034  loss_ctrl_points: 16.08  loss_bd_points: 16.09  loss_ce_0: 1.092  loss_texts_0: 8.414  loss_ctrl_points_0: 16.13  loss_bd_points_0: 16.15  loss_ce_1: 1.002  loss_texts_1: 7.48  loss_ctrl_points_1: 16.15  loss_bd_points_1: 16.16  loss_ce_2: 1.078  loss_texts_2: 8.012  loss_ctrl_points_2: 16.14  loss_bd_points_2: 16.15  loss_ce_3: 1.074  loss_texts_3: 7.786  loss_ctrl_points_3: 16.08  loss_bd_points_3: 16.1  loss_ce_4: 1.213  loss_texts_4: 7.799  loss_ctrl_points_4: 16.09  loss_bd_points_4: 16.1  loss_ce_enc: 1.05  loss_bezier_enc: 0.5458  total_loss: 252.2    time: 7.0443  last_time: 7.6775  data_time: 0.0032  last_data_time: 0.0032   lr: 3.1668e-06  max_mem: 6977M
[04/29 13:15:52] detectron2 INFO: Rank of current process: 0. World size: 1
[04/29 13:15:52] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/kylee/DeepSolo_LangPriors/detectron2/detectron2
Compiler                         GCC 9.5
CUDA compiler                    CUDA 11.3
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.12.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 3060 (arch=8.6)
Driver version                   550.120
CUDA_HOME                        /usr
Pillow                           9.0.1
torchvision                      0.13.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[04/29 13:15:52] detectron2 INFO: Command line arguments: Namespace(config_file='configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50152', opts=['MODEL.WEIGHTS', 'output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth'])
[04/29 13:15:52] detectron2 INFO: Contents of args.config_file=configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml:
_BASE_: "../Base_det.yaml"

MODEL:
  WEIGHTS: "output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth"
  TRANSFORMER:
    INFERENCE_TH_TEST: 0.3

DATASETS:
  TRAIN: ("ic15_train",)
  TEST: ("ic15_test",)

INPUT:
  MIN_SIZE_TRAIN: (640, 700, 760, 820, 880, 940, 1000)
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MAX_SIZE_TEST: 4000
  CROP:
    ENABLED: False
  ROTATE: False

SOLVER:
  AMP:
    ENABLED: True  # Enable Automatic Mixed Precision
  IMS_PER_BATCH: 2
  BASE_LR: 1e-5
  LR_BACKBONE: 1e-6
  WARMUP_ITERS: 250
  STEPS: (15000, 25000)  # no step
  MAX_ITER: 30000
  GAMMA: 0.1
  CHECKPOINT_PERIOD: 1000

TEST:
  EVAL_PERIOD: 1000
  # 1 - Generic, 2 - Weak, 3 - Strong (for icdar2015)
  LEXICON_TYPE: 3

OUTPUT_DIR: "output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15"

[04/29 13:15:52] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ic15_test
  TRAIN:
  - ic15_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    CROP_INSTANCE: false
    ENABLED: false
    SIZE:
    - 0.1
    - 0.1
    TYPE: relative_range
  FORMAT: RGB
  HFLIP_TRAIN: false
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 4000
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MIN_SIZE_TRAIN:
  - 640
  - 700
  - 760
  - 820
  - 880
  - 940
  - 1000
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  ROTATE: false
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    ANTI_ALIAS: false
    FREEZE_AT: 2
    NAME: build_resnet_backbone
  BASIS_MODULE:
    ANN_SET: coco
    COMMON_STRIDE: 8
    CONVS_DIM: 128
    IN_FEATURES:
    - p3
    - p4
    - p5
    LOSS_ON: false
    LOSS_WEIGHT: 0.3
    NAME: ProtoNet
    NORM: SyncBN
    NUM_BASES: 4
    NUM_CLASSES: 80
    NUM_CONVS: 3
  BATEXT:
    CANONICAL_SIZE: 96
    CONV_DIM: 256
    CUSTOM_DICT: ''
    IN_FEATURES:
    - p2
    - p3
    - p4
    NUM_CHARS: 25
    NUM_CONV: 2
    POOLER_RESOLUTION:
    - 8
    - 32
    POOLER_SCALES:
    - 0.25
    - 0.125
    - 0.0625
    RECOGNITION_LOSS: ctc
    RECOGNIZER: attn
    SAMPLING_RATIO: 1
    USE_AET: false
    USE_COORDCONV: false
    VOC_SIZE: 96
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: false
  META_ARCHITECTURE: TransformerPureDetector
  MOBILENET: false
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_INTERVAL: 1
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  SWIN:
    DROP_PATH_RATE: 0.2
    TYPE: tiny
  TOP_MODULE:
    DIM: 16
    NAME: conv
  TRANSFORMER:
    AUX_LOSS: true
    BOUNDARY_HEAD: true
    CUSTOM_DICT: ''
    DEC_LAYERS: 6
    DEC_N_POINTS: 4
    DIM_FEEDFORWARD: 1024
    DROPOUT: 0.0
    ENABLED: true
    ENC_LAYERS: 6
    ENC_N_POINTS: 4
    HIDDEN_DIM: 256
    INFERENCE_TH_TEST: 0.3
    LOSS:
      AUX_LOSS: true
      BEZIER_CLASS_WEIGHT: 1.0
      BEZIER_COORD_WEIGHT: 1.0
      BEZIER_SAMPLE_POINTS: 25
      BOUNDARY_WEIGHT: 0.5
      FOCAL_ALPHA: 0.25
      FOCAL_GAMMA: 2.0
      POINT_CLASS_WEIGHT: 1.0
      POINT_COORD_WEIGHT: 1.0
      POINT_TEXT_WEIGHT: 0.5
    NHEADS: 8
    NUM_FEATURE_LEVELS: 4
    NUM_POINTS: 25
    NUM_QUERIES: 100
    POSITION_EMBEDDING_SCALE: 6.283185307179586
    TEMPERATURE: 10000
    VOC_SIZE: 37
  ViTAEv2:
    DROP_PATH_RATE: 0.2
    TYPE: vitaev2_s
  WEIGHTS: output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth
OUTPUT_DIR: output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15
SEED: 42
SOLVER:
  AMP:
    ENABLED: true
  BASE_LR: 1.0e-05
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.1
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 2
  LR_BACKBONE: 1.0e-06
  LR_BACKBONE_NAMES:
  - backbone.0
  LR_LINEAR_PROJ_MULT: 1.0
  LR_LINEAR_PROJ_NAMES:
  - reference_points
  - sampling_offsets
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 30000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 15000
  - 25000
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 250
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  LEXICON_TYPE: 3
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/29 13:15:52] detectron2 INFO: Full config saved to output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/config.yaml
[04/29 13:18:23] detectron2 INFO: Rank of current process: 0. World size: 1
[04/29 13:18:24] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/kylee/DeepSolo_LangPriors/detectron2/detectron2
Compiler                         GCC 9.5
CUDA compiler                    CUDA 11.3
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.12.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 3060 (arch=8.6)
Driver version                   550.120
CUDA_HOME                        /usr
Pillow                           9.0.1
torchvision                      0.13.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[04/29 13:18:24] detectron2 INFO: Command line arguments: Namespace(config_file='configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50152', opts=['MODEL.WEIGHTS', 'output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth'])
[04/29 13:18:24] detectron2 INFO: Contents of args.config_file=configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml:
_BASE_: "../Base_det.yaml"

MODEL:
  WEIGHTS: "output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth"
  TRANSFORMER:
    INFERENCE_TH_TEST: 0.3

DATASETS:
  TRAIN: ("ic15_train",)
  TEST: ("ic15_test",)

INPUT:
  MIN_SIZE_TRAIN: (640, 700, 760, 820, 880, 940, 1000)
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MAX_SIZE_TEST: 4000
  CROP:
    ENABLED: False
  ROTATE: False

SOLVER:
  AMP:
    ENABLED: True  # Enable Automatic Mixed Precision
  IMS_PER_BATCH: 2
  BASE_LR: 1e-5
  LR_BACKBONE: 1e-6
  WARMUP_ITERS: 250
  STEPS: (15000, 25000)  # no step
  MAX_ITER: 30000
  GAMMA: 0.1
  CHECKPOINT_PERIOD: 1000

TEST:
  EVAL_PERIOD: 1000
  # 1 - Generic, 2 - Weak, 3 - Strong (for icdar2015)
  LEXICON_TYPE: 3

OUTPUT_DIR: "output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15"

[04/29 13:18:24] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ic15_test
  TRAIN:
  - ic15_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    CROP_INSTANCE: false
    ENABLED: false
    SIZE:
    - 0.1
    - 0.1
    TYPE: relative_range
  FORMAT: RGB
  HFLIP_TRAIN: false
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 4000
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MIN_SIZE_TRAIN:
  - 640
  - 700
  - 760
  - 820
  - 880
  - 940
  - 1000
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  ROTATE: false
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    ANTI_ALIAS: false
    FREEZE_AT: 2
    NAME: build_resnet_backbone
  BASIS_MODULE:
    ANN_SET: coco
    COMMON_STRIDE: 8
    CONVS_DIM: 128
    IN_FEATURES:
    - p3
    - p4
    - p5
    LOSS_ON: false
    LOSS_WEIGHT: 0.3
    NAME: ProtoNet
    NORM: SyncBN
    NUM_BASES: 4
    NUM_CLASSES: 80
    NUM_CONVS: 3
  BATEXT:
    CANONICAL_SIZE: 96
    CONV_DIM: 256
    CUSTOM_DICT: ''
    IN_FEATURES:
    - p2
    - p3
    - p4
    NUM_CHARS: 25
    NUM_CONV: 2
    POOLER_RESOLUTION:
    - 8
    - 32
    POOLER_SCALES:
    - 0.25
    - 0.125
    - 0.0625
    RECOGNITION_LOSS: ctc
    RECOGNIZER: attn
    SAMPLING_RATIO: 1
    USE_AET: false
    USE_COORDCONV: false
    VOC_SIZE: 96
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: false
  META_ARCHITECTURE: TransformerPureDetector
  MOBILENET: false
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_INTERVAL: 1
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  SWIN:
    DROP_PATH_RATE: 0.2
    TYPE: tiny
  TOP_MODULE:
    DIM: 16
    NAME: conv
  TRANSFORMER:
    AUX_LOSS: true
    BOUNDARY_HEAD: true
    CUSTOM_DICT: ''
    DEC_LAYERS: 6
    DEC_N_POINTS: 4
    DIM_FEEDFORWARD: 1024
    DROPOUT: 0.0
    ENABLED: true
    ENC_LAYERS: 6
    ENC_N_POINTS: 4
    HIDDEN_DIM: 256
    INFERENCE_TH_TEST: 0.3
    LOSS:
      AUX_LOSS: true
      BEZIER_CLASS_WEIGHT: 1.0
      BEZIER_COORD_WEIGHT: 1.0
      BEZIER_SAMPLE_POINTS: 25
      BOUNDARY_WEIGHT: 0.5
      FOCAL_ALPHA: 0.25
      FOCAL_GAMMA: 2.0
      POINT_CLASS_WEIGHT: 1.0
      POINT_COORD_WEIGHT: 1.0
      POINT_TEXT_WEIGHT: 0.5
    NHEADS: 8
    NUM_FEATURE_LEVELS: 4
    NUM_POINTS: 25
    NUM_QUERIES: 100
    POSITION_EMBEDDING_SCALE: 6.283185307179586
    TEMPERATURE: 10000
    VOC_SIZE: 37
  ViTAEv2:
    DROP_PATH_RATE: 0.2
    TYPE: vitaev2_s
  WEIGHTS: output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth
OUTPUT_DIR: output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15
SEED: 42
SOLVER:
  AMP:
    ENABLED: true
  BASE_LR: 1.0e-05
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.1
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 2
  LR_BACKBONE: 1.0e-06
  LR_BACKBONE_NAMES:
  - backbone.0
  LR_LINEAR_PROJ_MULT: 1.0
  LR_LINEAR_PROJ_NAMES:
  - reference_points
  - sampling_offsets
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 30000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 15000
  - 25000
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 250
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  LEXICON_TYPE: 3
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/29 13:18:24] detectron2 INFO: Full config saved to output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/config.yaml
[04/29 13:18:25] d2.engine.defaults INFO: Model:
TransformerPureDetector(
  (detection_transformer): DETECTION_TRANSFORMER(
    (backbone): Joiner(
      (0): MaskedBackbone(
        (backbone): ResNet(
          (stem): BasicStem(
            (conv1): Conv2d(
              3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
          )
          (res2): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv1): Conv2d(
                64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
          )
          (res3): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv1): Conv2d(
                256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
          )
          (res4): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
              (conv1): Conv2d(
                512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (4): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (5): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
          )
          (res5): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
              (conv1): Conv2d(
                1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
          )
        )
      )
      (1): PositionalEncoding2D()
    )
    (point_embed): Embedding(2500, 256)
    (transformer): DeformableTransformer(
      (encoder): DeformableTransformerEncoder(
        (layers): ModuleList(
          (0): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (decoder): DeformableCompositeTransformerDecoder(
        (layers): ModuleList(
          (0): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ctrl_point_coord): ModuleList(
          (0): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (1): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (2): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (3): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (4): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (5): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
        )
      )
      (enc_output): Linear(in_features=256, out_features=256, bias=True)
      (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (bezier_coord_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=8, bias=True)
        )
      )
      (bezier_class_embed): Linear(in_features=256, out_features=1, bias=True)
    )
    (input_proj): ModuleList(
      (0): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (1): Sequential(
        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (2): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (3): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (bezier_proposal_coord): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=8, bias=True)
      )
    )
    (bezier_proposal_class): Linear(in_features=256, out_features=1, bias=True)
    (ctrl_point_coord): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
    )
    (ctrl_point_class): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (ctrl_point_text): ModuleList(
      (0): Linear(in_features=256, out_features=38, bias=True)
      (1): Linear(in_features=256, out_features=38, bias=True)
      (2): Linear(in_features=256, out_features=38, bias=True)
      (3): Linear(in_features=256, out_features=38, bias=True)
      (4): Linear(in_features=256, out_features=38, bias=True)
      (5): Linear(in_features=256, out_features=38, bias=True)
    )
    (boundary_offset): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (enc_matcher): BezierHungarianMatcher(
      (bezier_sampler): BezierSampler()
    )
    (dec_matcher): CtrlPointHungarianMatcher()
    (bezier_sampler): BezierSampler()
  )
)
[04/29 13:18:25] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth ...
[04/29 13:18:25] fvcore.common.checkpoint INFO: [Checkpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth ...
[04/29 13:18:25] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1440, 1440), max_size=4000, sample_style='choice')]
[04/29 13:18:25] adet.data.dataset_mapper INFO: Rebuilding the augmentations. The previous augmentations will be overridden.
[04/29 13:18:25] adet.data.datasets.text INFO: Loaded 500 images in COCO format from datasets/ic15/test.json
[04/29 13:18:25] d2.data.build INFO: Distribution of instances among all 1 categories:
[36m|  category  | #instances   |
|:----------:|:-------------|
|    text    | 0            |
|            |              |[0m
[04/29 13:18:25] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[04/29 13:18:25] d2.data.common INFO: Serializing 500 elements to byte tensors and concatenating them all ...
[04/29 13:18:25] d2.data.common INFO: Serialized dataset takes 0.06 MiB
[04/29 13:18:25] d2.evaluation.evaluator INFO: Start inference on 500 batches
[04/29 13:18:32] d2.evaluation.evaluator INFO: Inference done 11/500. Dataloading: 0.0009 s/iter. Inference: 0.3737 s/iter. Eval: 0.0000 s/iter. Total: 0.3746 s/iter. ETA=0:03:03
[04/29 13:18:37] d2.evaluation.evaluator INFO: Inference done 25/500. Dataloading: 0.0010 s/iter. Inference: 0.3751 s/iter. Eval: 0.0000 s/iter. Total: 0.3761 s/iter. ETA=0:02:58
[04/29 13:18:42] d2.evaluation.evaluator INFO: Inference done 38/500. Dataloading: 0.0011 s/iter. Inference: 0.3818 s/iter. Eval: 0.0000 s/iter. Total: 0.3829 s/iter. ETA=0:02:56
[04/29 13:18:47] d2.evaluation.evaluator INFO: Inference done 52/500. Dataloading: 0.0011 s/iter. Inference: 0.3817 s/iter. Eval: 0.0000 s/iter. Total: 0.3829 s/iter. ETA=0:02:51
[04/29 13:18:52] d2.evaluation.evaluator INFO: Inference done 65/500. Dataloading: 0.0011 s/iter. Inference: 0.3827 s/iter. Eval: 0.0000 s/iter. Total: 0.3839 s/iter. ETA=0:02:46
[04/29 13:18:57] d2.evaluation.evaluator INFO: Inference done 78/500. Dataloading: 0.0011 s/iter. Inference: 0.3837 s/iter. Eval: 0.0000 s/iter. Total: 0.3849 s/iter. ETA=0:02:42
[04/29 13:19:03] d2.evaluation.evaluator INFO: Inference done 92/500. Dataloading: 0.0011 s/iter. Inference: 0.3834 s/iter. Eval: 0.0000 s/iter. Total: 0.3846 s/iter. ETA=0:02:36
[04/29 13:19:08] d2.evaluation.evaluator INFO: Inference done 106/500. Dataloading: 0.0012 s/iter. Inference: 0.3826 s/iter. Eval: 0.0000 s/iter. Total: 0.3838 s/iter. ETA=0:02:31
[04/29 13:19:13] d2.evaluation.evaluator INFO: Inference done 120/500. Dataloading: 0.0012 s/iter. Inference: 0.3820 s/iter. Eval: 0.0000 s/iter. Total: 0.3833 s/iter. ETA=0:02:25
[04/29 13:19:19] d2.evaluation.evaluator INFO: Inference done 134/500. Dataloading: 0.0012 s/iter. Inference: 0.3816 s/iter. Eval: 0.0000 s/iter. Total: 0.3828 s/iter. ETA=0:02:20
[04/29 13:19:24] d2.evaluation.evaluator INFO: Inference done 148/500. Dataloading: 0.0012 s/iter. Inference: 0.3812 s/iter. Eval: 0.0000 s/iter. Total: 0.3825 s/iter. ETA=0:02:14
[04/29 13:19:29] d2.evaluation.evaluator INFO: Inference done 162/500. Dataloading: 0.0012 s/iter. Inference: 0.3810 s/iter. Eval: 0.0000 s/iter. Total: 0.3822 s/iter. ETA=0:02:09
[04/29 13:19:35] d2.evaluation.evaluator INFO: Inference done 176/500. Dataloading: 0.0012 s/iter. Inference: 0.3808 s/iter. Eval: 0.0000 s/iter. Total: 0.3820 s/iter. ETA=0:02:03
[04/29 13:19:40] d2.evaluation.evaluator INFO: Inference done 190/500. Dataloading: 0.0012 s/iter. Inference: 0.3806 s/iter. Eval: 0.0000 s/iter. Total: 0.3818 s/iter. ETA=0:01:58
[04/29 13:19:45] d2.evaluation.evaluator INFO: Inference done 204/500. Dataloading: 0.0012 s/iter. Inference: 0.3804 s/iter. Eval: 0.0000 s/iter. Total: 0.3817 s/iter. ETA=0:01:52
[04/29 13:19:51] d2.evaluation.evaluator INFO: Inference done 218/500. Dataloading: 0.0012 s/iter. Inference: 0.3803 s/iter. Eval: 0.0000 s/iter. Total: 0.3816 s/iter. ETA=0:01:47
[04/29 13:19:56] d2.evaluation.evaluator INFO: Inference done 232/500. Dataloading: 0.0012 s/iter. Inference: 0.3802 s/iter. Eval: 0.0000 s/iter. Total: 0.3814 s/iter. ETA=0:01:42
[04/29 13:20:01] d2.evaluation.evaluator INFO: Inference done 246/500. Dataloading: 0.0012 s/iter. Inference: 0.3801 s/iter. Eval: 0.0000 s/iter. Total: 0.3814 s/iter. ETA=0:01:36
[04/29 13:20:07] d2.evaluation.evaluator INFO: Inference done 260/500. Dataloading: 0.0012 s/iter. Inference: 0.3800 s/iter. Eval: 0.0000 s/iter. Total: 0.3813 s/iter. ETA=0:01:31
[04/29 13:20:12] d2.evaluation.evaluator INFO: Inference done 274/500. Dataloading: 0.0012 s/iter. Inference: 0.3799 s/iter. Eval: 0.0000 s/iter. Total: 0.3812 s/iter. ETA=0:01:26
[04/29 13:20:17] d2.evaluation.evaluator INFO: Inference done 288/500. Dataloading: 0.0012 s/iter. Inference: 0.3799 s/iter. Eval: 0.0000 s/iter. Total: 0.3811 s/iter. ETA=0:01:20
[04/29 13:20:23] d2.evaluation.evaluator INFO: Inference done 302/500. Dataloading: 0.0012 s/iter. Inference: 0.3798 s/iter. Eval: 0.0000 s/iter. Total: 0.3811 s/iter. ETA=0:01:15
[04/29 13:20:28] d2.evaluation.evaluator INFO: Inference done 316/500. Dataloading: 0.0012 s/iter. Inference: 0.3798 s/iter. Eval: 0.0000 s/iter. Total: 0.3810 s/iter. ETA=0:01:10
[04/29 13:20:33] d2.evaluation.evaluator INFO: Inference done 330/500. Dataloading: 0.0012 s/iter. Inference: 0.3797 s/iter. Eval: 0.0000 s/iter. Total: 0.3810 s/iter. ETA=0:01:04
[04/29 13:20:38] d2.evaluation.evaluator INFO: Inference done 344/500. Dataloading: 0.0012 s/iter. Inference: 0.3797 s/iter. Eval: 0.0000 s/iter. Total: 0.3810 s/iter. ETA=0:00:59
[04/29 13:20:44] d2.evaluation.evaluator INFO: Inference done 358/500. Dataloading: 0.0012 s/iter. Inference: 0.3796 s/iter. Eval: 0.0000 s/iter. Total: 0.3809 s/iter. ETA=0:00:54
[04/29 13:20:49] d2.evaluation.evaluator INFO: Inference done 372/500. Dataloading: 0.0012 s/iter. Inference: 0.3796 s/iter. Eval: 0.0000 s/iter. Total: 0.3809 s/iter. ETA=0:00:48
[04/29 13:20:54] d2.evaluation.evaluator INFO: Inference done 386/500. Dataloading: 0.0012 s/iter. Inference: 0.3796 s/iter. Eval: 0.0000 s/iter. Total: 0.3809 s/iter. ETA=0:00:43
[04/29 13:21:00] d2.evaluation.evaluator INFO: Inference done 400/500. Dataloading: 0.0012 s/iter. Inference: 0.3795 s/iter. Eval: 0.0000 s/iter. Total: 0.3808 s/iter. ETA=0:00:38
[04/29 13:21:05] d2.evaluation.evaluator INFO: Inference done 414/500. Dataloading: 0.0012 s/iter. Inference: 0.3795 s/iter. Eval: 0.0000 s/iter. Total: 0.3808 s/iter. ETA=0:00:32
[04/29 13:21:10] d2.evaluation.evaluator INFO: Inference done 428/500. Dataloading: 0.0012 s/iter. Inference: 0.3795 s/iter. Eval: 0.0000 s/iter. Total: 0.3808 s/iter. ETA=0:00:27
[04/29 13:21:16] d2.evaluation.evaluator INFO: Inference done 442/500. Dataloading: 0.0012 s/iter. Inference: 0.3795 s/iter. Eval: 0.0000 s/iter. Total: 0.3808 s/iter. ETA=0:00:22
[04/29 13:21:21] d2.evaluation.evaluator INFO: Inference done 456/500. Dataloading: 0.0012 s/iter. Inference: 0.3795 s/iter. Eval: 0.0000 s/iter. Total: 0.3808 s/iter. ETA=0:00:16
[04/29 13:21:26] d2.evaluation.evaluator INFO: Inference done 470/500. Dataloading: 0.0012 s/iter. Inference: 0.3795 s/iter. Eval: 0.0000 s/iter. Total: 0.3807 s/iter. ETA=0:00:11
[04/29 13:21:32] d2.evaluation.evaluator INFO: Inference done 484/500. Dataloading: 0.0012 s/iter. Inference: 0.3794 s/iter. Eval: 0.0000 s/iter. Total: 0.3807 s/iter. ETA=0:00:06
[04/29 13:21:37] d2.evaluation.evaluator INFO: Inference done 498/500. Dataloading: 0.0012 s/iter. Inference: 0.3794 s/iter. Eval: 0.0000 s/iter. Total: 0.3807 s/iter. ETA=0:00:00
[04/29 13:21:38] d2.evaluation.evaluator INFO: Total inference time: 0:03:08.469227 (0.380746 s / iter per device, on 1 devices)
[04/29 13:21:38] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:03:07 (0.379414 s / iter per device, on 1 devices)
[04/29 13:21:38] adet.evaluation.text_evaluation_all INFO: Saving results to output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/inference/text_results.json
[04/29 13:36:57] detectron2 INFO: Rank of current process: 0. World size: 1
[04/29 13:36:58] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/kylee/DeepSolo_LangPriors/detectron2/detectron2
Compiler                         GCC 9.5
CUDA compiler                    CUDA 11.3
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.12.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 3060 (arch=8.6)
Driver version                   550.120
CUDA_HOME                        /usr
Pillow                           9.0.1
torchvision                      0.13.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[04/29 13:36:58] detectron2 INFO: Command line arguments: Namespace(config_file='configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50152', opts=['MODEL.WEIGHTS', 'output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth'])
[04/29 13:36:58] detectron2 INFO: Contents of args.config_file=configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml:
_BASE_: "../Base_det.yaml"

MODEL:
  WEIGHTS: "output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth"
  TRANSFORMER:
    INFERENCE_TH_TEST: 0.3

DATASETS:
  TRAIN: ("ic15_train",)
  TEST: ("ic15_test",)

INPUT:
  MIN_SIZE_TRAIN: (640, 700, 760, 820, 880, 940, 1000)
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MAX_SIZE_TEST: 4000
  CROP:
    ENABLED: False
  ROTATE: False

SOLVER:
  AMP:
    ENABLED: True  # Enable Automatic Mixed Precision
  IMS_PER_BATCH: 2
  BASE_LR: 1e-5
  LR_BACKBONE: 1e-6
  WARMUP_ITERS: 250
  STEPS: (15000, 25000)  # no step
  MAX_ITER: 30000
  GAMMA: 0.1
  CHECKPOINT_PERIOD: 1000

TEST:
  EVAL_PERIOD: 1000
  # 1 - Generic, 2 - Weak, 3 - Strong (for icdar2015)
  LEXICON_TYPE: 3

OUTPUT_DIR: "output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15"

[04/29 13:36:58] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ic15_test
  TRAIN:
  - ic15_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    CROP_INSTANCE: false
    ENABLED: false
    SIZE:
    - 0.1
    - 0.1
    TYPE: relative_range
  FORMAT: RGB
  HFLIP_TRAIN: false
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 4000
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MIN_SIZE_TRAIN:
  - 640
  - 700
  - 760
  - 820
  - 880
  - 940
  - 1000
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  ROTATE: false
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    ANTI_ALIAS: false
    FREEZE_AT: 2
    NAME: build_resnet_backbone
  BASIS_MODULE:
    ANN_SET: coco
    COMMON_STRIDE: 8
    CONVS_DIM: 128
    IN_FEATURES:
    - p3
    - p4
    - p5
    LOSS_ON: false
    LOSS_WEIGHT: 0.3
    NAME: ProtoNet
    NORM: SyncBN
    NUM_BASES: 4
    NUM_CLASSES: 80
    NUM_CONVS: 3
  BATEXT:
    CANONICAL_SIZE: 96
    CONV_DIM: 256
    CUSTOM_DICT: ''
    IN_FEATURES:
    - p2
    - p3
    - p4
    NUM_CHARS: 25
    NUM_CONV: 2
    POOLER_RESOLUTION:
    - 8
    - 32
    POOLER_SCALES:
    - 0.25
    - 0.125
    - 0.0625
    RECOGNITION_LOSS: ctc
    RECOGNIZER: attn
    SAMPLING_RATIO: 1
    USE_AET: false
    USE_COORDCONV: false
    VOC_SIZE: 96
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: false
  META_ARCHITECTURE: TransformerPureDetector
  MOBILENET: false
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_INTERVAL: 1
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  SWIN:
    DROP_PATH_RATE: 0.2
    TYPE: tiny
  TOP_MODULE:
    DIM: 16
    NAME: conv
  TRANSFORMER:
    AUX_LOSS: true
    BOUNDARY_HEAD: true
    CUSTOM_DICT: ''
    DEC_LAYERS: 6
    DEC_N_POINTS: 4
    DIM_FEEDFORWARD: 1024
    DROPOUT: 0.0
    ENABLED: true
    ENC_LAYERS: 6
    ENC_N_POINTS: 4
    HIDDEN_DIM: 256
    INFERENCE_TH_TEST: 0.3
    LOSS:
      AUX_LOSS: true
      BEZIER_CLASS_WEIGHT: 1.0
      BEZIER_COORD_WEIGHT: 1.0
      BEZIER_SAMPLE_POINTS: 25
      BOUNDARY_WEIGHT: 0.5
      FOCAL_ALPHA: 0.25
      FOCAL_GAMMA: 2.0
      POINT_CLASS_WEIGHT: 1.0
      POINT_COORD_WEIGHT: 1.0
      POINT_TEXT_WEIGHT: 0.5
    NHEADS: 8
    NUM_FEATURE_LEVELS: 4
    NUM_POINTS: 25
    NUM_QUERIES: 100
    POSITION_EMBEDDING_SCALE: 6.283185307179586
    TEMPERATURE: 10000
    VOC_SIZE: 37
  ViTAEv2:
    DROP_PATH_RATE: 0.2
    TYPE: vitaev2_s
  WEIGHTS: output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth
OUTPUT_DIR: output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15
SEED: 42
SOLVER:
  AMP:
    ENABLED: true
  BASE_LR: 1.0e-05
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.1
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 2
  LR_BACKBONE: 1.0e-06
  LR_BACKBONE_NAMES:
  - backbone.0
  LR_LINEAR_PROJ_MULT: 1.0
  LR_LINEAR_PROJ_NAMES:
  - reference_points
  - sampling_offsets
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 30000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 15000
  - 25000
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 250
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  LEXICON_TYPE: 3
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/29 13:36:58] detectron2 INFO: Full config saved to output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/config.yaml
[04/29 13:36:59] d2.engine.defaults INFO: Model:
TransformerPureDetector(
  (detection_transformer): DETECTION_TRANSFORMER(
    (backbone): Joiner(
      (0): MaskedBackbone(
        (backbone): ResNet(
          (stem): BasicStem(
            (conv1): Conv2d(
              3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
          )
          (res2): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv1): Conv2d(
                64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
          )
          (res3): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv1): Conv2d(
                256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
          )
          (res4): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
              (conv1): Conv2d(
                512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (4): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (5): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
          )
          (res5): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
              (conv1): Conv2d(
                1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
          )
        )
      )
      (1): PositionalEncoding2D()
    )
    (point_embed): Embedding(2500, 256)
    (transformer): DeformableTransformer(
      (encoder): DeformableTransformerEncoder(
        (layers): ModuleList(
          (0): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (decoder): DeformableCompositeTransformerDecoder(
        (layers): ModuleList(
          (0): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ctrl_point_coord): ModuleList(
          (0): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (1): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (2): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (3): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (4): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (5): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
        )
      )
      (enc_output): Linear(in_features=256, out_features=256, bias=True)
      (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (bezier_coord_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=8, bias=True)
        )
      )
      (bezier_class_embed): Linear(in_features=256, out_features=1, bias=True)
    )
    (input_proj): ModuleList(
      (0): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (1): Sequential(
        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (2): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (3): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (bezier_proposal_coord): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=8, bias=True)
      )
    )
    (bezier_proposal_class): Linear(in_features=256, out_features=1, bias=True)
    (ctrl_point_coord): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
    )
    (ctrl_point_class): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (ctrl_point_text): ModuleList(
      (0): Linear(in_features=256, out_features=38, bias=True)
      (1): Linear(in_features=256, out_features=38, bias=True)
      (2): Linear(in_features=256, out_features=38, bias=True)
      (3): Linear(in_features=256, out_features=38, bias=True)
      (4): Linear(in_features=256, out_features=38, bias=True)
      (5): Linear(in_features=256, out_features=38, bias=True)
    )
    (boundary_offset): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (enc_matcher): BezierHungarianMatcher(
      (bezier_sampler): BezierSampler()
    )
    (dec_matcher): CtrlPointHungarianMatcher()
    (bezier_sampler): BezierSampler()
  )
)
[04/29 13:36:59] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth ...
[04/29 13:36:59] fvcore.common.checkpoint INFO: [Checkpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth ...
[04/29 13:36:59] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1440, 1440), max_size=4000, sample_style='choice')]
[04/29 13:36:59] adet.data.dataset_mapper INFO: Rebuilding the augmentations. The previous augmentations will be overridden.
[04/29 13:36:59] adet.data.datasets.text INFO: Loaded 500 images in COCO format from datasets/ic15/test.json
[04/29 13:36:59] d2.data.build INFO: Distribution of instances among all 1 categories:
[36m|  category  | #instances   |
|:----------:|:-------------|
|    text    | 0            |
|            |              |[0m
[04/29 13:36:59] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[04/29 13:36:59] d2.data.common INFO: Serializing 500 elements to byte tensors and concatenating them all ...
[04/29 13:36:59] d2.data.common INFO: Serialized dataset takes 0.06 MiB
[04/29 13:36:59] d2.evaluation.evaluator INFO: Start inference on 500 batches
[04/29 13:37:05] d2.evaluation.evaluator INFO: Inference done 11/500. Dataloading: 0.0010 s/iter. Inference: 0.3831 s/iter. Eval: 0.0000 s/iter. Total: 0.3842 s/iter. ETA=0:03:07
[04/29 13:37:10] d2.evaluation.evaluator INFO: Inference done 25/500. Dataloading: 0.0011 s/iter. Inference: 0.3801 s/iter. Eval: 0.0000 s/iter. Total: 0.3813 s/iter. ETA=0:03:01
[04/29 13:37:16] d2.evaluation.evaluator INFO: Inference done 38/500. Dataloading: 0.0012 s/iter. Inference: 0.3869 s/iter. Eval: 0.0000 s/iter. Total: 0.3881 s/iter. ETA=0:02:59
[04/29 13:37:21] d2.evaluation.evaluator INFO: Inference done 52/500. Dataloading: 0.0012 s/iter. Inference: 0.3842 s/iter. Eval: 0.0000 s/iter. Total: 0.3855 s/iter. ETA=0:02:52
[04/29 13:37:26] d2.evaluation.evaluator INFO: Inference done 66/500. Dataloading: 0.0012 s/iter. Inference: 0.3827 s/iter. Eval: 0.0000 s/iter. Total: 0.3840 s/iter. ETA=0:02:46
[04/29 13:37:32] d2.evaluation.evaluator INFO: Inference done 80/500. Dataloading: 0.0012 s/iter. Inference: 0.3818 s/iter. Eval: 0.0000 s/iter. Total: 0.3831 s/iter. ETA=0:02:40
[04/29 13:37:37] d2.evaluation.evaluator INFO: Inference done 94/500. Dataloading: 0.0012 s/iter. Inference: 0.3812 s/iter. Eval: 0.0000 s/iter. Total: 0.3825 s/iter. ETA=0:02:35
[04/29 13:37:42] d2.evaluation.evaluator INFO: Inference done 108/500. Dataloading: 0.0012 s/iter. Inference: 0.3807 s/iter. Eval: 0.0000 s/iter. Total: 0.3820 s/iter. ETA=0:02:29
[04/29 13:37:48] d2.evaluation.evaluator INFO: Inference done 122/500. Dataloading: 0.0012 s/iter. Inference: 0.3804 s/iter. Eval: 0.0000 s/iter. Total: 0.3817 s/iter. ETA=0:02:24
[04/29 13:37:53] d2.evaluation.evaluator INFO: Inference done 136/500. Dataloading: 0.0012 s/iter. Inference: 0.3802 s/iter. Eval: 0.0000 s/iter. Total: 0.3815 s/iter. ETA=0:02:18
[04/29 13:37:58] d2.evaluation.evaluator INFO: Inference done 150/500. Dataloading: 0.0012 s/iter. Inference: 0.3800 s/iter. Eval: 0.0000 s/iter. Total: 0.3813 s/iter. ETA=0:02:13
[04/29 13:38:03] d2.evaluation.evaluator INFO: Inference done 164/500. Dataloading: 0.0012 s/iter. Inference: 0.3798 s/iter. Eval: 0.0000 s/iter. Total: 0.3811 s/iter. ETA=0:02:08
[04/29 13:38:09] d2.evaluation.evaluator INFO: Inference done 178/500. Dataloading: 0.0012 s/iter. Inference: 0.3797 s/iter. Eval: 0.0000 s/iter. Total: 0.3810 s/iter. ETA=0:02:02
[04/29 13:38:14] d2.evaluation.evaluator INFO: Inference done 192/500. Dataloading: 0.0012 s/iter. Inference: 0.3796 s/iter. Eval: 0.0000 s/iter. Total: 0.3809 s/iter. ETA=0:01:57
[04/29 13:38:19] d2.evaluation.evaluator INFO: Inference done 206/500. Dataloading: 0.0012 s/iter. Inference: 0.3795 s/iter. Eval: 0.0000 s/iter. Total: 0.3808 s/iter. ETA=0:01:51
[04/29 13:38:25] d2.evaluation.evaluator INFO: Inference done 220/500. Dataloading: 0.0012 s/iter. Inference: 0.3795 s/iter. Eval: 0.0000 s/iter. Total: 0.3808 s/iter. ETA=0:01:46
[04/29 13:38:30] d2.evaluation.evaluator INFO: Inference done 234/500. Dataloading: 0.0012 s/iter. Inference: 0.3794 s/iter. Eval: 0.0000 s/iter. Total: 0.3807 s/iter. ETA=0:01:41
[04/29 13:38:35] d2.evaluation.evaluator INFO: Inference done 248/500. Dataloading: 0.0012 s/iter. Inference: 0.3793 s/iter. Eval: 0.0000 s/iter. Total: 0.3807 s/iter. ETA=0:01:35
[04/29 13:38:41] d2.evaluation.evaluator INFO: Inference done 262/500. Dataloading: 0.0012 s/iter. Inference: 0.3793 s/iter. Eval: 0.0000 s/iter. Total: 0.3806 s/iter. ETA=0:01:30
[04/29 13:38:46] d2.evaluation.evaluator INFO: Inference done 276/500. Dataloading: 0.0012 s/iter. Inference: 0.3793 s/iter. Eval: 0.0000 s/iter. Total: 0.3806 s/iter. ETA=0:01:25
[04/29 13:38:51] d2.evaluation.evaluator INFO: Inference done 290/500. Dataloading: 0.0012 s/iter. Inference: 0.3792 s/iter. Eval: 0.0000 s/iter. Total: 0.3806 s/iter. ETA=0:01:19
[04/29 13:38:57] d2.evaluation.evaluator INFO: Inference done 304/500. Dataloading: 0.0012 s/iter. Inference: 0.3792 s/iter. Eval: 0.0000 s/iter. Total: 0.3805 s/iter. ETA=0:01:14
[04/29 13:39:02] d2.evaluation.evaluator INFO: Inference done 318/500. Dataloading: 0.0012 s/iter. Inference: 0.3792 s/iter. Eval: 0.0000 s/iter. Total: 0.3805 s/iter. ETA=0:01:09
[04/29 13:39:07] d2.evaluation.evaluator INFO: Inference done 331/500. Dataloading: 0.0012 s/iter. Inference: 0.3794 s/iter. Eval: 0.0000 s/iter. Total: 0.3807 s/iter. ETA=0:01:04
[04/29 13:39:12] d2.evaluation.evaluator INFO: Inference done 345/500. Dataloading: 0.0012 s/iter. Inference: 0.3795 s/iter. Eval: 0.0000 s/iter. Total: 0.3808 s/iter. ETA=0:00:59
[04/29 13:39:18] d2.evaluation.evaluator INFO: Inference done 359/500. Dataloading: 0.0012 s/iter. Inference: 0.3795 s/iter. Eval: 0.0000 s/iter. Total: 0.3808 s/iter. ETA=0:00:53
[04/29 13:39:23] d2.evaluation.evaluator INFO: Inference done 373/500. Dataloading: 0.0012 s/iter. Inference: 0.3795 s/iter. Eval: 0.0000 s/iter. Total: 0.3808 s/iter. ETA=0:00:48
[04/29 13:39:28] d2.evaluation.evaluator INFO: Inference done 387/500. Dataloading: 0.0012 s/iter. Inference: 0.3794 s/iter. Eval: 0.0000 s/iter. Total: 0.3807 s/iter. ETA=0:00:43
[04/29 13:39:34] d2.evaluation.evaluator INFO: Inference done 401/500. Dataloading: 0.0012 s/iter. Inference: 0.3794 s/iter. Eval: 0.0000 s/iter. Total: 0.3807 s/iter. ETA=0:00:37
[04/29 13:39:39] d2.evaluation.evaluator INFO: Inference done 415/500. Dataloading: 0.0012 s/iter. Inference: 0.3794 s/iter. Eval: 0.0000 s/iter. Total: 0.3807 s/iter. ETA=0:00:32
[04/29 13:39:44] d2.evaluation.evaluator INFO: Inference done 429/500. Dataloading: 0.0012 s/iter. Inference: 0.3794 s/iter. Eval: 0.0000 s/iter. Total: 0.3807 s/iter. ETA=0:00:27
[04/29 13:39:50] d2.evaluation.evaluator INFO: Inference done 443/500. Dataloading: 0.0012 s/iter. Inference: 0.3794 s/iter. Eval: 0.0000 s/iter. Total: 0.3807 s/iter. ETA=0:00:21
[04/29 13:39:55] d2.evaluation.evaluator INFO: Inference done 457/500. Dataloading: 0.0012 s/iter. Inference: 0.3794 s/iter. Eval: 0.0000 s/iter. Total: 0.3807 s/iter. ETA=0:00:16
[04/29 13:40:00] d2.evaluation.evaluator INFO: Inference done 471/500. Dataloading: 0.0012 s/iter. Inference: 0.3794 s/iter. Eval: 0.0000 s/iter. Total: 0.3806 s/iter. ETA=0:00:11
[04/29 13:40:06] d2.evaluation.evaluator INFO: Inference done 485/500. Dataloading: 0.0012 s/iter. Inference: 0.3793 s/iter. Eval: 0.0000 s/iter. Total: 0.3806 s/iter. ETA=0:00:05
[04/29 13:40:11] d2.evaluation.evaluator INFO: Inference done 499/500. Dataloading: 0.0012 s/iter. Inference: 0.3793 s/iter. Eval: 0.0000 s/iter. Total: 0.3806 s/iter. ETA=0:00:00
[04/29 13:40:11] d2.evaluation.evaluator INFO: Total inference time: 0:03:08.432590 (0.380672 s / iter per device, on 1 devices)
[04/29 13:40:11] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:03:07 (0.379336 s / iter per device, on 1 devices)
[04/29 13:40:11] adet.evaluation.text_evaluation_all INFO: Saving results to output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/inference/text_results.json
[04/29 13:54:37] detectron2 INFO: Rank of current process: 0. World size: 1
[04/29 13:54:37] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/kylee/DeepSolo_LangPriors/detectron2/detectron2
Compiler                         GCC 9.5
CUDA compiler                    CUDA 11.3
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.12.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 3060 (arch=8.6)
Driver version                   550.120
CUDA_HOME                        /usr
Pillow                           9.0.1
torchvision                      0.13.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[04/29 13:54:37] detectron2 INFO: Command line arguments: Namespace(config_file='configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50152', opts=['MODEL.WEIGHTS', 'output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth'])
[04/29 13:54:37] detectron2 INFO: Contents of args.config_file=configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml:
_BASE_: "../Base_det.yaml"

MODEL:
  WEIGHTS: "output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth"
  TRANSFORMER:
    INFERENCE_TH_TEST: 0.3

DATASETS:
  TRAIN: ("ic15_train",)
  TEST: ("ic15_test",)

INPUT:
  MIN_SIZE_TRAIN: (640, 700, 760, 820, 880, 940, 1000)
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MAX_SIZE_TEST: 4000
  CROP:
    ENABLED: False
  ROTATE: False

SOLVER:
  AMP:
    ENABLED: True  # Enable Automatic Mixed Precision
  IMS_PER_BATCH: 2
  BASE_LR: 1e-5
  LR_BACKBONE: 1e-6
  WARMUP_ITERS: 250
  STEPS: (15000, 25000)  # no step
  MAX_ITER: 30000
  GAMMA: 0.1
  CHECKPOINT_PERIOD: 1000

TEST:
  EVAL_PERIOD: 1000
  # 1 - Generic, 2 - Weak, 3 - Strong (for icdar2015)
  LEXICON_TYPE: 3

OUTPUT_DIR: "output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15"

[04/29 13:54:37] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ic15_test
  TRAIN:
  - ic15_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    CROP_INSTANCE: false
    ENABLED: false
    SIZE:
    - 0.1
    - 0.1
    TYPE: relative_range
  FORMAT: RGB
  HFLIP_TRAIN: false
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 4000
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MIN_SIZE_TRAIN:
  - 640
  - 700
  - 760
  - 820
  - 880
  - 940
  - 1000
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  ROTATE: false
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    ANTI_ALIAS: false
    FREEZE_AT: 2
    NAME: build_resnet_backbone
  BASIS_MODULE:
    ANN_SET: coco
    COMMON_STRIDE: 8
    CONVS_DIM: 128
    IN_FEATURES:
    - p3
    - p4
    - p5
    LOSS_ON: false
    LOSS_WEIGHT: 0.3
    NAME: ProtoNet
    NORM: SyncBN
    NUM_BASES: 4
    NUM_CLASSES: 80
    NUM_CONVS: 3
  BATEXT:
    CANONICAL_SIZE: 96
    CONV_DIM: 256
    CUSTOM_DICT: ''
    IN_FEATURES:
    - p2
    - p3
    - p4
    NUM_CHARS: 25
    NUM_CONV: 2
    POOLER_RESOLUTION:
    - 8
    - 32
    POOLER_SCALES:
    - 0.25
    - 0.125
    - 0.0625
    RECOGNITION_LOSS: ctc
    RECOGNIZER: attn
    SAMPLING_RATIO: 1
    USE_AET: false
    USE_COORDCONV: false
    VOC_SIZE: 96
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: false
  META_ARCHITECTURE: TransformerPureDetector
  MOBILENET: false
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_INTERVAL: 1
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  SWIN:
    DROP_PATH_RATE: 0.2
    TYPE: tiny
  TOP_MODULE:
    DIM: 16
    NAME: conv
  TRANSFORMER:
    AUX_LOSS: true
    BOUNDARY_HEAD: true
    CUSTOM_DICT: ''
    DEC_LAYERS: 6
    DEC_N_POINTS: 4
    DIM_FEEDFORWARD: 1024
    DROPOUT: 0.0
    ENABLED: true
    ENC_LAYERS: 6
    ENC_N_POINTS: 4
    HIDDEN_DIM: 256
    INFERENCE_TH_TEST: 0.3
    LOSS:
      AUX_LOSS: true
      BEZIER_CLASS_WEIGHT: 1.0
      BEZIER_COORD_WEIGHT: 1.0
      BEZIER_SAMPLE_POINTS: 25
      BOUNDARY_WEIGHT: 0.5
      FOCAL_ALPHA: 0.25
      FOCAL_GAMMA: 2.0
      POINT_CLASS_WEIGHT: 1.0
      POINT_COORD_WEIGHT: 1.0
      POINT_TEXT_WEIGHT: 0.5
    NHEADS: 8
    NUM_FEATURE_LEVELS: 4
    NUM_POINTS: 25
    NUM_QUERIES: 100
    POSITION_EMBEDDING_SCALE: 6.283185307179586
    TEMPERATURE: 10000
    VOC_SIZE: 37
  ViTAEv2:
    DROP_PATH_RATE: 0.2
    TYPE: vitaev2_s
  WEIGHTS: output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth
OUTPUT_DIR: output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15
SEED: 42
SOLVER:
  AMP:
    ENABLED: true
  BASE_LR: 1.0e-05
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.1
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 2
  LR_BACKBONE: 1.0e-06
  LR_BACKBONE_NAMES:
  - backbone.0
  LR_LINEAR_PROJ_MULT: 1.0
  LR_LINEAR_PROJ_NAMES:
  - reference_points
  - sampling_offsets
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 30000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 15000
  - 25000
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 250
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  LEXICON_TYPE: 3
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/29 13:54:37] detectron2 INFO: Full config saved to output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/config.yaml
[04/29 13:54:39] d2.engine.defaults INFO: Model:
TransformerPureDetector(
  (detection_transformer): DETECTION_TRANSFORMER(
    (backbone): Joiner(
      (0): MaskedBackbone(
        (backbone): ResNet(
          (stem): BasicStem(
            (conv1): Conv2d(
              3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
          )
          (res2): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv1): Conv2d(
                64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
          )
          (res3): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv1): Conv2d(
                256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
          )
          (res4): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
              (conv1): Conv2d(
                512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (4): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (5): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
          )
          (res5): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
              (conv1): Conv2d(
                1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
          )
        )
      )
      (1): PositionalEncoding2D()
    )
    (point_embed): Embedding(2500, 256)
    (transformer): DeformableTransformer(
      (encoder): DeformableTransformerEncoder(
        (layers): ModuleList(
          (0): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (decoder): DeformableCompositeTransformerDecoder(
        (layers): ModuleList(
          (0): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ctrl_point_coord): ModuleList(
          (0): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (1): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (2): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (3): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (4): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (5): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
        )
      )
      (enc_output): Linear(in_features=256, out_features=256, bias=True)
      (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (bezier_coord_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=8, bias=True)
        )
      )
      (bezier_class_embed): Linear(in_features=256, out_features=1, bias=True)
    )
    (input_proj): ModuleList(
      (0): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (1): Sequential(
        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (2): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (3): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (bezier_proposal_coord): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=8, bias=True)
      )
    )
    (bezier_proposal_class): Linear(in_features=256, out_features=1, bias=True)
    (ctrl_point_coord): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
    )
    (ctrl_point_class): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (ctrl_point_text): ModuleList(
      (0): Linear(in_features=256, out_features=38, bias=True)
      (1): Linear(in_features=256, out_features=38, bias=True)
      (2): Linear(in_features=256, out_features=38, bias=True)
      (3): Linear(in_features=256, out_features=38, bias=True)
      (4): Linear(in_features=256, out_features=38, bias=True)
      (5): Linear(in_features=256, out_features=38, bias=True)
    )
    (boundary_offset): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (enc_matcher): BezierHungarianMatcher(
      (bezier_sampler): BezierSampler()
    )
    (dec_matcher): CtrlPointHungarianMatcher()
    (bezier_sampler): BezierSampler()
  )
)
[04/29 13:54:39] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth ...
[04/29 13:54:39] fvcore.common.checkpoint INFO: [Checkpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth ...
[04/29 13:54:39] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1440, 1440), max_size=4000, sample_style='choice')]
[04/29 13:54:39] adet.data.dataset_mapper INFO: Rebuilding the augmentations. The previous augmentations will be overridden.
[04/29 13:54:39] adet.data.datasets.text INFO: Loaded 500 images in COCO format from datasets/ic15/test.json
[04/29 13:54:39] d2.data.build INFO: Distribution of instances among all 1 categories:
[36m|  category  | #instances   |
|:----------:|:-------------|
|    text    | 0            |
|            |              |[0m
[04/29 13:54:39] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[04/29 13:54:39] d2.data.common INFO: Serializing 500 elements to byte tensors and concatenating them all ...
[04/29 13:54:39] d2.data.common INFO: Serialized dataset takes 0.06 MiB
[04/29 13:54:39] d2.evaluation.evaluator INFO: Start inference on 500 batches
[04/29 13:54:45] d2.evaluation.evaluator INFO: Inference done 11/500. Dataloading: 0.0010 s/iter. Inference: 0.3836 s/iter. Eval: 0.0000 s/iter. Total: 0.3847 s/iter. ETA=0:03:08
[04/29 13:54:50] d2.evaluation.evaluator INFO: Inference done 24/500. Dataloading: 0.0010 s/iter. Inference: 0.3851 s/iter. Eval: 0.0000 s/iter. Total: 0.3862 s/iter. ETA=0:03:03
[04/29 13:54:55] d2.evaluation.evaluator INFO: Inference done 37/500. Dataloading: 0.0011 s/iter. Inference: 0.3857 s/iter. Eval: 0.0000 s/iter. Total: 0.3869 s/iter. ETA=0:02:59
[04/29 13:55:00] d2.evaluation.evaluator INFO: Inference done 51/500. Dataloading: 0.0012 s/iter. Inference: 0.3840 s/iter. Eval: 0.0000 s/iter. Total: 0.3852 s/iter. ETA=0:02:52
[04/29 13:55:06] d2.evaluation.evaluator INFO: Inference done 65/500. Dataloading: 0.0012 s/iter. Inference: 0.3836 s/iter. Eval: 0.0000 s/iter. Total: 0.3849 s/iter. ETA=0:02:47
[04/29 13:55:11] d2.evaluation.evaluator INFO: Inference done 78/500. Dataloading: 0.0012 s/iter. Inference: 0.3867 s/iter. Eval: 0.0000 s/iter. Total: 0.3880 s/iter. ETA=0:02:43
[04/29 13:55:16] d2.evaluation.evaluator INFO: Inference done 91/500. Dataloading: 0.0012 s/iter. Inference: 0.3891 s/iter. Eval: 0.0000 s/iter. Total: 0.3903 s/iter. ETA=0:02:39
[04/29 13:55:22] d2.evaluation.evaluator INFO: Inference done 104/500. Dataloading: 0.0012 s/iter. Inference: 0.3914 s/iter. Eval: 0.0000 s/iter. Total: 0.3927 s/iter. ETA=0:02:35
[04/29 13:55:27] d2.evaluation.evaluator INFO: Inference done 117/500. Dataloading: 0.0012 s/iter. Inference: 0.3933 s/iter. Eval: 0.0000 s/iter. Total: 0.3946 s/iter. ETA=0:02:31
[04/29 13:55:32] d2.evaluation.evaluator INFO: Inference done 130/500. Dataloading: 0.0012 s/iter. Inference: 0.3948 s/iter. Eval: 0.0000 s/iter. Total: 0.3961 s/iter. ETA=0:02:26
[04/29 13:55:37] d2.evaluation.evaluator INFO: Inference done 143/500. Dataloading: 0.0012 s/iter. Inference: 0.3960 s/iter. Eval: 0.0000 s/iter. Total: 0.3973 s/iter. ETA=0:02:21
[04/29 13:55:43] d2.evaluation.evaluator INFO: Inference done 156/500. Dataloading: 0.0012 s/iter. Inference: 0.3971 s/iter. Eval: 0.0000 s/iter. Total: 0.3984 s/iter. ETA=0:02:17
[04/29 13:55:48] d2.evaluation.evaluator INFO: Inference done 169/500. Dataloading: 0.0012 s/iter. Inference: 0.3980 s/iter. Eval: 0.0000 s/iter. Total: 0.3993 s/iter. ETA=0:02:12
[04/29 13:55:53] d2.evaluation.evaluator INFO: Inference done 182/500. Dataloading: 0.0012 s/iter. Inference: 0.3987 s/iter. Eval: 0.0000 s/iter. Total: 0.4000 s/iter. ETA=0:02:07
[04/29 13:55:59] d2.evaluation.evaluator INFO: Inference done 195/500. Dataloading: 0.0012 s/iter. Inference: 0.3993 s/iter. Eval: 0.0000 s/iter. Total: 0.4006 s/iter. ETA=0:02:02
[04/29 13:56:04] d2.evaluation.evaluator INFO: Inference done 208/500. Dataloading: 0.0012 s/iter. Inference: 0.3998 s/iter. Eval: 0.0000 s/iter. Total: 0.4011 s/iter. ETA=0:01:57
[04/29 13:56:09] d2.evaluation.evaluator INFO: Inference done 221/500. Dataloading: 0.0012 s/iter. Inference: 0.4003 s/iter. Eval: 0.0000 s/iter. Total: 0.4016 s/iter. ETA=0:01:52
[04/29 13:56:15] d2.evaluation.evaluator INFO: Inference done 234/500. Dataloading: 0.0012 s/iter. Inference: 0.4007 s/iter. Eval: 0.0000 s/iter. Total: 0.4020 s/iter. ETA=0:01:46
[04/29 13:56:20] d2.evaluation.evaluator INFO: Inference done 247/500. Dataloading: 0.0012 s/iter. Inference: 0.4011 s/iter. Eval: 0.0000 s/iter. Total: 0.4024 s/iter. ETA=0:01:41
[04/29 13:56:25] d2.evaluation.evaluator INFO: Inference done 260/500. Dataloading: 0.0012 s/iter. Inference: 0.4014 s/iter. Eval: 0.0000 s/iter. Total: 0.4027 s/iter. ETA=0:01:36
[04/29 13:56:31] d2.evaluation.evaluator INFO: Inference done 273/500. Dataloading: 0.0012 s/iter. Inference: 0.4018 s/iter. Eval: 0.0000 s/iter. Total: 0.4031 s/iter. ETA=0:01:31
[04/29 13:56:36] d2.evaluation.evaluator INFO: Inference done 286/500. Dataloading: 0.0012 s/iter. Inference: 0.4022 s/iter. Eval: 0.0000 s/iter. Total: 0.4035 s/iter. ETA=0:01:26
[04/29 13:56:41] d2.evaluation.evaluator INFO: Inference done 299/500. Dataloading: 0.0012 s/iter. Inference: 0.4025 s/iter. Eval: 0.0000 s/iter. Total: 0.4038 s/iter. ETA=0:01:21
[04/29 13:56:47] d2.evaluation.evaluator INFO: Inference done 312/500. Dataloading: 0.0012 s/iter. Inference: 0.4027 s/iter. Eval: 0.0000 s/iter. Total: 0.4040 s/iter. ETA=0:01:15
[04/29 13:56:52] d2.evaluation.evaluator INFO: Inference done 325/500. Dataloading: 0.0012 s/iter. Inference: 0.4029 s/iter. Eval: 0.0000 s/iter. Total: 0.4042 s/iter. ETA=0:01:10
[04/29 13:56:57] d2.evaluation.evaluator INFO: Inference done 338/500. Dataloading: 0.0012 s/iter. Inference: 0.4031 s/iter. Eval: 0.0000 s/iter. Total: 0.4045 s/iter. ETA=0:01:05
[04/29 13:57:03] d2.evaluation.evaluator INFO: Inference done 351/500. Dataloading: 0.0012 s/iter. Inference: 0.4034 s/iter. Eval: 0.0000 s/iter. Total: 0.4047 s/iter. ETA=0:01:00
[04/29 13:57:08] d2.evaluation.evaluator INFO: Inference done 364/500. Dataloading: 0.0012 s/iter. Inference: 0.4036 s/iter. Eval: 0.0000 s/iter. Total: 0.4049 s/iter. ETA=0:00:55
[04/29 13:57:13] d2.evaluation.evaluator INFO: Inference done 377/500. Dataloading: 0.0012 s/iter. Inference: 0.4038 s/iter. Eval: 0.0000 s/iter. Total: 0.4051 s/iter. ETA=0:00:49
[04/29 13:57:19] d2.evaluation.evaluator INFO: Inference done 390/500. Dataloading: 0.0012 s/iter. Inference: 0.4040 s/iter. Eval: 0.0000 s/iter. Total: 0.4053 s/iter. ETA=0:00:44
[04/29 13:57:24] d2.evaluation.evaluator INFO: Inference done 403/500. Dataloading: 0.0012 s/iter. Inference: 0.4041 s/iter. Eval: 0.0000 s/iter. Total: 0.4054 s/iter. ETA=0:00:39
[04/29 13:57:29] d2.evaluation.evaluator INFO: Inference done 416/500. Dataloading: 0.0012 s/iter. Inference: 0.4043 s/iter. Eval: 0.0000 s/iter. Total: 0.4056 s/iter. ETA=0:00:34
[04/29 13:57:35] d2.evaluation.evaluator INFO: Inference done 429/500. Dataloading: 0.0012 s/iter. Inference: 0.4045 s/iter. Eval: 0.0000 s/iter. Total: 0.4058 s/iter. ETA=0:00:28
[04/29 13:57:40] d2.evaluation.evaluator INFO: Inference done 442/500. Dataloading: 0.0012 s/iter. Inference: 0.4047 s/iter. Eval: 0.0000 s/iter. Total: 0.4060 s/iter. ETA=0:00:23
[04/29 13:57:45] d2.evaluation.evaluator INFO: Inference done 455/500. Dataloading: 0.0013 s/iter. Inference: 0.4048 s/iter. Eval: 0.0000 s/iter. Total: 0.4062 s/iter. ETA=0:00:18
[04/29 13:57:51] d2.evaluation.evaluator INFO: Inference done 468/500. Dataloading: 0.0013 s/iter. Inference: 0.4050 s/iter. Eval: 0.0000 s/iter. Total: 0.4064 s/iter. ETA=0:00:13
[04/29 13:57:56] d2.evaluation.evaluator INFO: Inference done 481/500. Dataloading: 0.0013 s/iter. Inference: 0.4045 s/iter. Eval: 0.0000 s/iter. Total: 0.4058 s/iter. ETA=0:00:07
[04/29 13:58:01] d2.evaluation.evaluator INFO: Inference done 494/500. Dataloading: 0.0013 s/iter. Inference: 0.4043 s/iter. Eval: 0.0000 s/iter. Total: 0.4056 s/iter. ETA=0:00:02
[04/29 13:58:03] d2.evaluation.evaluator INFO: Total inference time: 0:03:20.728709 (0.405513 s / iter per device, on 1 devices)
[04/29 13:58:03] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:03:20 (0.404143 s / iter per device, on 1 devices)
[04/29 13:58:03] adet.evaluation.text_evaluation_all INFO: Saving results to output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/inference/text_results.json
[04/29 14:23:26] detectron2 INFO: Rank of current process: 0. World size: 1
[04/29 14:23:27] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/kylee/DeepSolo_LangPriors/detectron2/detectron2
Compiler                         GCC 9.5
CUDA compiler                    CUDA 11.3
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.12.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 3060 (arch=8.6)
Driver version                   550.120
CUDA_HOME                        /usr
Pillow                           9.0.1
torchvision                      0.13.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[04/29 14:23:27] detectron2 INFO: Command line arguments: Namespace(config_file='configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50152', opts=['MODEL.WEIGHTS', 'output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth'])
[04/29 14:23:27] detectron2 INFO: Contents of args.config_file=configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml:
_BASE_: "../Base_det.yaml"

MODEL:
  WEIGHTS: "output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth"
  TRANSFORMER:
    INFERENCE_TH_TEST: 0.3

DATASETS:
  TRAIN: ("ic15_train",)
  TEST: ("ic15_test",)

INPUT:
  MIN_SIZE_TRAIN: (640, 700, 760, 820, 880, 940, 1000)
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MAX_SIZE_TEST: 4000
  CROP:
    ENABLED: False
  ROTATE: False

SOLVER:
  AMP:
    ENABLED: True  # Enable Automatic Mixed Precision
  IMS_PER_BATCH: 2
  BASE_LR: 1e-5
  LR_BACKBONE: 1e-6
  WARMUP_ITERS: 250
  STEPS: (15000, 25000)  # no step
  MAX_ITER: 30000
  GAMMA: 0.1
  CHECKPOINT_PERIOD: 1000

TEST:
  EVAL_PERIOD: 1000
  # 1 - Generic, 2 - Weak, 3 - Strong (for icdar2015)
  LEXICON_TYPE: 3

OUTPUT_DIR: "output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15"

[04/29 14:23:27] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ic15_test
  TRAIN:
  - ic15_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    CROP_INSTANCE: false
    ENABLED: false
    SIZE:
    - 0.1
    - 0.1
    TYPE: relative_range
  FORMAT: RGB
  HFLIP_TRAIN: false
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 4000
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MIN_SIZE_TRAIN:
  - 640
  - 700
  - 760
  - 820
  - 880
  - 940
  - 1000
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  ROTATE: false
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    ANTI_ALIAS: false
    FREEZE_AT: 2
    NAME: build_resnet_backbone
  BASIS_MODULE:
    ANN_SET: coco
    COMMON_STRIDE: 8
    CONVS_DIM: 128
    IN_FEATURES:
    - p3
    - p4
    - p5
    LOSS_ON: false
    LOSS_WEIGHT: 0.3
    NAME: ProtoNet
    NORM: SyncBN
    NUM_BASES: 4
    NUM_CLASSES: 80
    NUM_CONVS: 3
  BATEXT:
    CANONICAL_SIZE: 96
    CONV_DIM: 256
    CUSTOM_DICT: ''
    IN_FEATURES:
    - p2
    - p3
    - p4
    NUM_CHARS: 25
    NUM_CONV: 2
    POOLER_RESOLUTION:
    - 8
    - 32
    POOLER_SCALES:
    - 0.25
    - 0.125
    - 0.0625
    RECOGNITION_LOSS: ctc
    RECOGNIZER: attn
    SAMPLING_RATIO: 1
    USE_AET: false
    USE_COORDCONV: false
    VOC_SIZE: 96
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: false
  META_ARCHITECTURE: TransformerPureDetector
  MOBILENET: false
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_INTERVAL: 1
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  SWIN:
    DROP_PATH_RATE: 0.2
    TYPE: tiny
  TOP_MODULE:
    DIM: 16
    NAME: conv
  TRANSFORMER:
    AUX_LOSS: true
    BOUNDARY_HEAD: true
    CUSTOM_DICT: ''
    DEC_LAYERS: 6
    DEC_N_POINTS: 4
    DIM_FEEDFORWARD: 1024
    DROPOUT: 0.0
    ENABLED: true
    ENC_LAYERS: 6
    ENC_N_POINTS: 4
    HIDDEN_DIM: 256
    INFERENCE_TH_TEST: 0.3
    LOSS:
      AUX_LOSS: true
      BEZIER_CLASS_WEIGHT: 1.0
      BEZIER_COORD_WEIGHT: 1.0
      BEZIER_SAMPLE_POINTS: 25
      BOUNDARY_WEIGHT: 0.5
      FOCAL_ALPHA: 0.25
      FOCAL_GAMMA: 2.0
      POINT_CLASS_WEIGHT: 1.0
      POINT_COORD_WEIGHT: 1.0
      POINT_TEXT_WEIGHT: 0.5
    NHEADS: 8
    NUM_FEATURE_LEVELS: 4
    NUM_POINTS: 25
    NUM_QUERIES: 100
    POSITION_EMBEDDING_SCALE: 6.283185307179586
    TEMPERATURE: 10000
    VOC_SIZE: 37
  ViTAEv2:
    DROP_PATH_RATE: 0.2
    TYPE: vitaev2_s
  WEIGHTS: output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth
OUTPUT_DIR: output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15
SEED: 42
SOLVER:
  AMP:
    ENABLED: true
  BASE_LR: 1.0e-05
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.1
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 2
  LR_BACKBONE: 1.0e-06
  LR_BACKBONE_NAMES:
  - backbone.0
  LR_LINEAR_PROJ_MULT: 1.0
  LR_LINEAR_PROJ_NAMES:
  - reference_points
  - sampling_offsets
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 30000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 15000
  - 25000
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 250
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  LEXICON_TYPE: 3
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/29 14:23:27] detectron2 INFO: Full config saved to output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/config.yaml
[04/29 14:23:28] d2.engine.defaults INFO: Model:
TransformerPureDetector(
  (detection_transformer): DETECTION_TRANSFORMER(
    (backbone): Joiner(
      (0): MaskedBackbone(
        (backbone): ResNet(
          (stem): BasicStem(
            (conv1): Conv2d(
              3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
          )
          (res2): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv1): Conv2d(
                64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
          )
          (res3): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv1): Conv2d(
                256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
          )
          (res4): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
              (conv1): Conv2d(
                512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (4): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (5): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
          )
          (res5): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
              (conv1): Conv2d(
                1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
          )
        )
      )
      (1): PositionalEncoding2D()
    )
    (point_embed): Embedding(2500, 256)
    (transformer): DeformableTransformer(
      (encoder): DeformableTransformerEncoder(
        (layers): ModuleList(
          (0): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (decoder): DeformableCompositeTransformerDecoder(
        (layers): ModuleList(
          (0): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ctrl_point_coord): ModuleList(
          (0): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (1): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (2): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (3): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (4): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (5): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
        )
      )
      (enc_output): Linear(in_features=256, out_features=256, bias=True)
      (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (bezier_coord_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=8, bias=True)
        )
      )
      (bezier_class_embed): Linear(in_features=256, out_features=1, bias=True)
    )
    (input_proj): ModuleList(
      (0): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (1): Sequential(
        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (2): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (3): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (bezier_proposal_coord): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=8, bias=True)
      )
    )
    (bezier_proposal_class): Linear(in_features=256, out_features=1, bias=True)
    (ctrl_point_coord): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
    )
    (ctrl_point_class): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (ctrl_point_text): ModuleList(
      (0): Linear(in_features=256, out_features=38, bias=True)
      (1): Linear(in_features=256, out_features=38, bias=True)
      (2): Linear(in_features=256, out_features=38, bias=True)
      (3): Linear(in_features=256, out_features=38, bias=True)
      (4): Linear(in_features=256, out_features=38, bias=True)
      (5): Linear(in_features=256, out_features=38, bias=True)
    )
    (boundary_offset): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (enc_matcher): BezierHungarianMatcher(
      (bezier_sampler): BezierSampler()
    )
    (dec_matcher): CtrlPointHungarianMatcher()
    (bezier_sampler): BezierSampler()
  )
)
[04/29 14:23:28] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth ...
[04/29 14:23:28] fvcore.common.checkpoint INFO: [Checkpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth ...
[04/29 14:23:28] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1440, 1440), max_size=4000, sample_style='choice')]
[04/29 14:23:28] adet.data.dataset_mapper INFO: Rebuilding the augmentations. The previous augmentations will be overridden.
[04/29 14:23:28] adet.data.datasets.text INFO: Loaded 500 images in COCO format from datasets/ic15/test.json
[04/29 14:23:28] d2.data.build INFO: Distribution of instances among all 1 categories:
[36m|  category  | #instances   |
|:----------:|:-------------|
|    text    | 0            |
|            |              |[0m
[04/29 14:23:28] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[04/29 14:23:28] d2.data.common INFO: Serializing 500 elements to byte tensors and concatenating them all ...
[04/29 14:23:28] d2.data.common INFO: Serialized dataset takes 0.06 MiB
[04/29 14:23:28] d2.evaluation.evaluator INFO: Start inference on 500 batches
[04/29 14:23:34] d2.evaluation.evaluator INFO: Inference done 11/500. Dataloading: 0.0009 s/iter. Inference: 0.3705 s/iter. Eval: 0.0000 s/iter. Total: 0.3715 s/iter. ETA=0:03:01
[04/29 14:23:40] d2.evaluation.evaluator INFO: Inference done 25/500. Dataloading: 0.0011 s/iter. Inference: 0.3795 s/iter. Eval: 0.0000 s/iter. Total: 0.3807 s/iter. ETA=0:03:00
[04/29 14:23:45] d2.evaluation.evaluator INFO: Inference done 38/500. Dataloading: 0.0012 s/iter. Inference: 0.3932 s/iter. Eval: 0.0000 s/iter. Total: 0.3945 s/iter. ETA=0:03:02
[04/29 14:23:50] d2.evaluation.evaluator INFO: Inference done 51/500. Dataloading: 0.0012 s/iter. Inference: 0.3992 s/iter. Eval: 0.0000 s/iter. Total: 0.4005 s/iter. ETA=0:02:59
[04/29 14:23:56] d2.evaluation.evaluator INFO: Inference done 64/500. Dataloading: 0.0013 s/iter. Inference: 0.4027 s/iter. Eval: 0.0000 s/iter. Total: 0.4041 s/iter. ETA=0:02:56
[04/29 14:24:01] d2.evaluation.evaluator INFO: Inference done 76/500. Dataloading: 0.0013 s/iter. Inference: 0.4051 s/iter. Eval: 0.0000 s/iter. Total: 0.4065 s/iter. ETA=0:02:52
[04/29 14:24:06] d2.evaluation.evaluator INFO: Inference done 88/500. Dataloading: 0.0013 s/iter. Inference: 0.4070 s/iter. Eval: 0.0000 s/iter. Total: 0.4084 s/iter. ETA=0:02:48
[04/29 14:24:11] d2.evaluation.evaluator INFO: Inference done 100/500. Dataloading: 0.0013 s/iter. Inference: 0.4084 s/iter. Eval: 0.0000 s/iter. Total: 0.4098 s/iter. ETA=0:02:43
[04/29 14:24:16] d2.evaluation.evaluator INFO: Inference done 112/500. Dataloading: 0.0013 s/iter. Inference: 0.4094 s/iter. Eval: 0.0000 s/iter. Total: 0.4108 s/iter. ETA=0:02:39
[04/29 14:24:21] d2.evaluation.evaluator INFO: Inference done 124/500. Dataloading: 0.0014 s/iter. Inference: 0.4103 s/iter. Eval: 0.0000 s/iter. Total: 0.4117 s/iter. ETA=0:02:34
[04/29 14:24:26] d2.evaluation.evaluator INFO: Inference done 136/500. Dataloading: 0.0014 s/iter. Inference: 0.4110 s/iter. Eval: 0.0000 s/iter. Total: 0.4124 s/iter. ETA=0:02:30
[04/29 14:24:31] d2.evaluation.evaluator INFO: Inference done 148/500. Dataloading: 0.0014 s/iter. Inference: 0.4117 s/iter. Eval: 0.0000 s/iter. Total: 0.4131 s/iter. ETA=0:02:25
[04/29 14:24:36] d2.evaluation.evaluator INFO: Inference done 160/500. Dataloading: 0.0014 s/iter. Inference: 0.4123 s/iter. Eval: 0.0000 s/iter. Total: 0.4138 s/iter. ETA=0:02:20
[04/29 14:24:41] d2.evaluation.evaluator INFO: Inference done 172/500. Dataloading: 0.0014 s/iter. Inference: 0.4128 s/iter. Eval: 0.0000 s/iter. Total: 0.4143 s/iter. ETA=0:02:15
[04/29 14:24:46] d2.evaluation.evaluator INFO: Inference done 184/500. Dataloading: 0.0014 s/iter. Inference: 0.4132 s/iter. Eval: 0.0000 s/iter. Total: 0.4147 s/iter. ETA=0:02:11
[04/29 14:24:51] d2.evaluation.evaluator INFO: Inference done 196/500. Dataloading: 0.0014 s/iter. Inference: 0.4137 s/iter. Eval: 0.0000 s/iter. Total: 0.4152 s/iter. ETA=0:02:06
[04/29 14:24:56] d2.evaluation.evaluator INFO: Inference done 208/500. Dataloading: 0.0014 s/iter. Inference: 0.4140 s/iter. Eval: 0.0000 s/iter. Total: 0.4155 s/iter. ETA=0:02:01
[04/29 14:25:02] d2.evaluation.evaluator INFO: Inference done 221/500. Dataloading: 0.0014 s/iter. Inference: 0.4139 s/iter. Eval: 0.0000 s/iter. Total: 0.4154 s/iter. ETA=0:01:55
[04/29 14:25:07] d2.evaluation.evaluator INFO: Inference done 233/500. Dataloading: 0.0014 s/iter. Inference: 0.4140 s/iter. Eval: 0.0000 s/iter. Total: 0.4155 s/iter. ETA=0:01:50
[04/29 14:25:12] d2.evaluation.evaluator INFO: Inference done 245/500. Dataloading: 0.0014 s/iter. Inference: 0.4143 s/iter. Eval: 0.0000 s/iter. Total: 0.4158 s/iter. ETA=0:01:46
[04/29 14:25:17] d2.evaluation.evaluator INFO: Inference done 257/500. Dataloading: 0.0014 s/iter. Inference: 0.4145 s/iter. Eval: 0.0000 s/iter. Total: 0.4160 s/iter. ETA=0:01:41
[04/29 14:25:22] d2.evaluation.evaluator INFO: Inference done 269/500. Dataloading: 0.0014 s/iter. Inference: 0.4148 s/iter. Eval: 0.0000 s/iter. Total: 0.4163 s/iter. ETA=0:01:36
[04/29 14:25:27] d2.evaluation.evaluator INFO: Inference done 281/500. Dataloading: 0.0014 s/iter. Inference: 0.4151 s/iter. Eval: 0.0000 s/iter. Total: 0.4166 s/iter. ETA=0:01:31
[04/29 14:25:32] d2.evaluation.evaluator INFO: Inference done 293/500. Dataloading: 0.0014 s/iter. Inference: 0.4154 s/iter. Eval: 0.0000 s/iter. Total: 0.4169 s/iter. ETA=0:01:26
[04/29 14:25:37] d2.evaluation.evaluator INFO: Inference done 305/500. Dataloading: 0.0014 s/iter. Inference: 0.4156 s/iter. Eval: 0.0000 s/iter. Total: 0.4171 s/iter. ETA=0:01:21
[04/29 14:25:42] d2.evaluation.evaluator INFO: Inference done 317/500. Dataloading: 0.0014 s/iter. Inference: 0.4158 s/iter. Eval: 0.0000 s/iter. Total: 0.4173 s/iter. ETA=0:01:16
[04/29 14:25:47] d2.evaluation.evaluator INFO: Inference done 329/500. Dataloading: 0.0014 s/iter. Inference: 0.4160 s/iter. Eval: 0.0000 s/iter. Total: 0.4175 s/iter. ETA=0:01:11
[04/29 14:25:52] d2.evaluation.evaluator INFO: Inference done 341/500. Dataloading: 0.0014 s/iter. Inference: 0.4162 s/iter. Eval: 0.0000 s/iter. Total: 0.4177 s/iter. ETA=0:01:06
[04/29 14:25:57] d2.evaluation.evaluator INFO: Inference done 353/500. Dataloading: 0.0014 s/iter. Inference: 0.4163 s/iter. Eval: 0.0000 s/iter. Total: 0.4178 s/iter. ETA=0:01:01
[04/29 14:26:02] d2.evaluation.evaluator INFO: Inference done 365/500. Dataloading: 0.0014 s/iter. Inference: 0.4164 s/iter. Eval: 0.0000 s/iter. Total: 0.4179 s/iter. ETA=0:00:56
[04/29 14:26:08] d2.evaluation.evaluator INFO: Inference done 378/500. Dataloading: 0.0014 s/iter. Inference: 0.4164 s/iter. Eval: 0.0000 s/iter. Total: 0.4179 s/iter. ETA=0:00:50
[04/29 14:26:13] d2.evaluation.evaluator INFO: Inference done 391/500. Dataloading: 0.0014 s/iter. Inference: 0.4161 s/iter. Eval: 0.0000 s/iter. Total: 0.4176 s/iter. ETA=0:00:45
[04/29 14:26:19] d2.evaluation.evaluator INFO: Inference done 404/500. Dataloading: 0.0014 s/iter. Inference: 0.4159 s/iter. Eval: 0.0000 s/iter. Total: 0.4174 s/iter. ETA=0:00:40
[04/29 14:26:24] d2.evaluation.evaluator INFO: Inference done 416/500. Dataloading: 0.0014 s/iter. Inference: 0.4160 s/iter. Eval: 0.0000 s/iter. Total: 0.4176 s/iter. ETA=0:00:35
[04/29 14:26:29] d2.evaluation.evaluator INFO: Inference done 428/500. Dataloading: 0.0014 s/iter. Inference: 0.4162 s/iter. Eval: 0.0000 s/iter. Total: 0.4177 s/iter. ETA=0:00:30
[04/29 14:26:34] d2.evaluation.evaluator INFO: Inference done 440/500. Dataloading: 0.0014 s/iter. Inference: 0.4163 s/iter. Eval: 0.0000 s/iter. Total: 0.4179 s/iter. ETA=0:00:25
[04/29 14:26:39] d2.evaluation.evaluator INFO: Inference done 452/500. Dataloading: 0.0014 s/iter. Inference: 0.4165 s/iter. Eval: 0.0000 s/iter. Total: 0.4180 s/iter. ETA=0:00:20
[04/29 14:26:44] d2.evaluation.evaluator INFO: Inference done 464/500. Dataloading: 0.0014 s/iter. Inference: 0.4166 s/iter. Eval: 0.0000 s/iter. Total: 0.4181 s/iter. ETA=0:00:15
[04/29 14:26:49] d2.evaluation.evaluator INFO: Inference done 476/500. Dataloading: 0.0014 s/iter. Inference: 0.4167 s/iter. Eval: 0.0000 s/iter. Total: 0.4182 s/iter. ETA=0:00:10
[04/29 14:26:54] d2.evaluation.evaluator INFO: Inference done 488/500. Dataloading: 0.0014 s/iter. Inference: 0.4168 s/iter. Eval: 0.0000 s/iter. Total: 0.4183 s/iter. ETA=0:00:05
[04/29 14:26:59] d2.evaluation.evaluator INFO: Inference done 500/500. Dataloading: 0.0014 s/iter. Inference: 0.4169 s/iter. Eval: 0.0000 s/iter. Total: 0.4184 s/iter. ETA=0:00:00
[04/29 14:26:59] d2.evaluation.evaluator INFO: Total inference time: 0:03:27.149798 (0.418484 s / iter per device, on 1 devices)
[04/29 14:26:59] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:03:26 (0.416907 s / iter per device, on 1 devices)
[04/29 14:26:59] adet.evaluation.text_evaluation_all INFO: Saving results to output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/inference/text_results.json
[04/29 14:34:40] detectron2 INFO: Rank of current process: 0. World size: 1
[04/29 14:34:40] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/kylee/DeepSolo_LangPriors/detectron2/detectron2
Compiler                         GCC 9.5
CUDA compiler                    CUDA 11.3
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.12.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 3060 (arch=8.6)
Driver version                   550.120
CUDA_HOME                        /usr
Pillow                           9.0.1
torchvision                      0.13.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[04/29 14:34:40] detectron2 INFO: Command line arguments: Namespace(config_file='configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50152', opts=['MODEL.WEIGHTS', 'output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth'])
[04/29 14:34:40] detectron2 INFO: Contents of args.config_file=configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml:
_BASE_: "../Base_det.yaml"

MODEL:
  WEIGHTS: "output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth"
  TRANSFORMER:
    INFERENCE_TH_TEST: 0.3

DATASETS:
  TRAIN: ("ic15_train",)
  TEST: ("ic15_test",)

INPUT:
  MIN_SIZE_TRAIN: (640, 700, 760, 820, 880, 940, 1000)
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MAX_SIZE_TEST: 4000
  CROP:
    ENABLED: False
  ROTATE: False

SOLVER:
  AMP:
    ENABLED: True  # Enable Automatic Mixed Precision
  IMS_PER_BATCH: 2
  BASE_LR: 1e-5
  LR_BACKBONE: 1e-6
  WARMUP_ITERS: 250
  STEPS: (15000, 25000)  # no step
  MAX_ITER: 30000
  GAMMA: 0.1
  CHECKPOINT_PERIOD: 1000

TEST:
  EVAL_PERIOD: 1000
  # 1 - Generic, 2 - Weak, 3 - Strong (for icdar2015)
  LEXICON_TYPE: 3

OUTPUT_DIR: "output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15"

[04/29 14:34:40] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ic15_test
  TRAIN:
  - ic15_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    CROP_INSTANCE: false
    ENABLED: false
    SIZE:
    - 0.1
    - 0.1
    TYPE: relative_range
  FORMAT: RGB
  HFLIP_TRAIN: false
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 4000
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MIN_SIZE_TRAIN:
  - 640
  - 700
  - 760
  - 820
  - 880
  - 940
  - 1000
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  ROTATE: false
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    ANTI_ALIAS: false
    FREEZE_AT: 2
    NAME: build_resnet_backbone
  BASIS_MODULE:
    ANN_SET: coco
    COMMON_STRIDE: 8
    CONVS_DIM: 128
    IN_FEATURES:
    - p3
    - p4
    - p5
    LOSS_ON: false
    LOSS_WEIGHT: 0.3
    NAME: ProtoNet
    NORM: SyncBN
    NUM_BASES: 4
    NUM_CLASSES: 80
    NUM_CONVS: 3
  BATEXT:
    CANONICAL_SIZE: 96
    CONV_DIM: 256
    CUSTOM_DICT: ''
    IN_FEATURES:
    - p2
    - p3
    - p4
    NUM_CHARS: 25
    NUM_CONV: 2
    POOLER_RESOLUTION:
    - 8
    - 32
    POOLER_SCALES:
    - 0.25
    - 0.125
    - 0.0625
    RECOGNITION_LOSS: ctc
    RECOGNIZER: attn
    SAMPLING_RATIO: 1
    USE_AET: false
    USE_COORDCONV: false
    VOC_SIZE: 96
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: false
  META_ARCHITECTURE: TransformerPureDetector
  MOBILENET: false
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_INTERVAL: 1
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  SWIN:
    DROP_PATH_RATE: 0.2
    TYPE: tiny
  TOP_MODULE:
    DIM: 16
    NAME: conv
  TRANSFORMER:
    AUX_LOSS: true
    BOUNDARY_HEAD: true
    CUSTOM_DICT: ''
    DEC_LAYERS: 6
    DEC_N_POINTS: 4
    DIM_FEEDFORWARD: 1024
    DROPOUT: 0.0
    ENABLED: true
    ENC_LAYERS: 6
    ENC_N_POINTS: 4
    HIDDEN_DIM: 256
    INFERENCE_TH_TEST: 0.3
    LOSS:
      AUX_LOSS: true
      BEZIER_CLASS_WEIGHT: 1.0
      BEZIER_COORD_WEIGHT: 1.0
      BEZIER_SAMPLE_POINTS: 25
      BOUNDARY_WEIGHT: 0.5
      FOCAL_ALPHA: 0.25
      FOCAL_GAMMA: 2.0
      POINT_CLASS_WEIGHT: 1.0
      POINT_COORD_WEIGHT: 1.0
      POINT_TEXT_WEIGHT: 0.5
    NHEADS: 8
    NUM_FEATURE_LEVELS: 4
    NUM_POINTS: 25
    NUM_QUERIES: 100
    POSITION_EMBEDDING_SCALE: 6.283185307179586
    TEMPERATURE: 10000
    VOC_SIZE: 37
  ViTAEv2:
    DROP_PATH_RATE: 0.2
    TYPE: vitaev2_s
  WEIGHTS: output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth
OUTPUT_DIR: output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15
SEED: 42
SOLVER:
  AMP:
    ENABLED: true
  BASE_LR: 1.0e-05
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.1
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 2
  LR_BACKBONE: 1.0e-06
  LR_BACKBONE_NAMES:
  - backbone.0
  LR_LINEAR_PROJ_MULT: 1.0
  LR_LINEAR_PROJ_NAMES:
  - reference_points
  - sampling_offsets
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 30000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 15000
  - 25000
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 250
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  LEXICON_TYPE: 3
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/29 14:34:40] detectron2 INFO: Full config saved to output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/config.yaml
[04/29 14:34:42] d2.engine.defaults INFO: Model:
TransformerPureDetector(
  (detection_transformer): DETECTION_TRANSFORMER(
    (backbone): Joiner(
      (0): MaskedBackbone(
        (backbone): ResNet(
          (stem): BasicStem(
            (conv1): Conv2d(
              3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
          )
          (res2): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv1): Conv2d(
                64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
          )
          (res3): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv1): Conv2d(
                256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
          )
          (res4): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
              (conv1): Conv2d(
                512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (4): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (5): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
          )
          (res5): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
              (conv1): Conv2d(
                1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
          )
        )
      )
      (1): PositionalEncoding2D()
    )
    (point_embed): Embedding(2500, 256)
    (transformer): DeformableTransformer(
      (encoder): DeformableTransformerEncoder(
        (layers): ModuleList(
          (0): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (decoder): DeformableCompositeTransformerDecoder(
        (layers): ModuleList(
          (0): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ctrl_point_coord): ModuleList(
          (0): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (1): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (2): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (3): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (4): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (5): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
        )
      )
      (enc_output): Linear(in_features=256, out_features=256, bias=True)
      (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (bezier_coord_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=8, bias=True)
        )
      )
      (bezier_class_embed): Linear(in_features=256, out_features=1, bias=True)
    )
    (input_proj): ModuleList(
      (0): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (1): Sequential(
        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (2): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (3): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (bezier_proposal_coord): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=8, bias=True)
      )
    )
    (bezier_proposal_class): Linear(in_features=256, out_features=1, bias=True)
    (ctrl_point_coord): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
    )
    (ctrl_point_class): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (ctrl_point_text): ModuleList(
      (0): Linear(in_features=256, out_features=38, bias=True)
      (1): Linear(in_features=256, out_features=38, bias=True)
      (2): Linear(in_features=256, out_features=38, bias=True)
      (3): Linear(in_features=256, out_features=38, bias=True)
      (4): Linear(in_features=256, out_features=38, bias=True)
      (5): Linear(in_features=256, out_features=38, bias=True)
    )
    (boundary_offset): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (enc_matcher): BezierHungarianMatcher(
      (bezier_sampler): BezierSampler()
    )
    (dec_matcher): CtrlPointHungarianMatcher()
    (bezier_sampler): BezierSampler()
  )
)
[04/29 14:34:42] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth ...
[04/29 14:34:42] fvcore.common.checkpoint INFO: [Checkpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth ...
[04/29 14:34:42] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1440, 1440), max_size=4000, sample_style='choice')]
[04/29 14:34:42] adet.data.dataset_mapper INFO: Rebuilding the augmentations. The previous augmentations will be overridden.
[04/29 14:34:42] adet.data.datasets.text INFO: Loaded 500 images in COCO format from datasets/ic15/test.json
[04/29 14:34:42] d2.data.build INFO: Distribution of instances among all 1 categories:
[36m|  category  | #instances   |
|:----------:|:-------------|
|    text    | 0            |
|            |              |[0m
[04/29 14:34:42] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[04/29 14:34:42] d2.data.common INFO: Serializing 500 elements to byte tensors and concatenating them all ...
[04/29 14:34:42] d2.data.common INFO: Serialized dataset takes 0.06 MiB
[04/29 14:34:42] d2.evaluation.evaluator INFO: Start inference on 500 batches
[04/29 14:34:48] d2.evaluation.evaluator INFO: Inference done 11/500. Dataloading: 0.0010 s/iter. Inference: 0.3909 s/iter. Eval: 0.0000 s/iter. Total: 0.3920 s/iter. ETA=0:03:11
[04/29 14:34:54] d2.evaluation.evaluator INFO: Inference done 24/500. Dataloading: 0.0011 s/iter. Inference: 0.4001 s/iter. Eval: 0.0000 s/iter. Total: 0.4013 s/iter. ETA=0:03:11
[04/29 14:34:59] d2.evaluation.evaluator INFO: Inference done 36/500. Dataloading: 0.0012 s/iter. Inference: 0.4068 s/iter. Eval: 0.0000 s/iter. Total: 0.4080 s/iter. ETA=0:03:09
[04/29 14:35:04] d2.evaluation.evaluator INFO: Inference done 48/500. Dataloading: 0.0012 s/iter. Inference: 0.4100 s/iter. Eval: 0.0000 s/iter. Total: 0.4113 s/iter. ETA=0:03:05
[04/29 14:35:09] d2.evaluation.evaluator INFO: Inference done 60/500. Dataloading: 0.0012 s/iter. Inference: 0.4120 s/iter. Eval: 0.0000 s/iter. Total: 0.4133 s/iter. ETA=0:03:01
[04/29 14:35:14] d2.evaluation.evaluator INFO: Inference done 72/500. Dataloading: 0.0012 s/iter. Inference: 0.4133 s/iter. Eval: 0.0000 s/iter. Total: 0.4146 s/iter. ETA=0:02:57
[04/29 14:35:19] d2.evaluation.evaluator INFO: Inference done 84/500. Dataloading: 0.0012 s/iter. Inference: 0.4142 s/iter. Eval: 0.0000 s/iter. Total: 0.4155 s/iter. ETA=0:02:52
[04/29 14:35:24] d2.evaluation.evaluator INFO: Inference done 96/500. Dataloading: 0.0013 s/iter. Inference: 0.4148 s/iter. Eval: 0.0000 s/iter. Total: 0.4161 s/iter. ETA=0:02:48
[04/29 14:35:29] d2.evaluation.evaluator INFO: Inference done 108/500. Dataloading: 0.0013 s/iter. Inference: 0.4152 s/iter. Eval: 0.0000 s/iter. Total: 0.4166 s/iter. ETA=0:02:43
[04/29 14:35:34] d2.evaluation.evaluator INFO: Inference done 120/500. Dataloading: 0.0013 s/iter. Inference: 0.4156 s/iter. Eval: 0.0000 s/iter. Total: 0.4169 s/iter. ETA=0:02:38
[04/29 14:35:39] d2.evaluation.evaluator INFO: Inference done 132/500. Dataloading: 0.0013 s/iter. Inference: 0.4159 s/iter. Eval: 0.0000 s/iter. Total: 0.4173 s/iter. ETA=0:02:33
[04/29 14:35:44] d2.evaluation.evaluator INFO: Inference done 144/500. Dataloading: 0.0013 s/iter. Inference: 0.4162 s/iter. Eval: 0.0000 s/iter. Total: 0.4176 s/iter. ETA=0:02:28
[04/29 14:35:49] d2.evaluation.evaluator INFO: Inference done 156/500. Dataloading: 0.0013 s/iter. Inference: 0.4164 s/iter. Eval: 0.0000 s/iter. Total: 0.4178 s/iter. ETA=0:02:23
[04/29 14:35:54] d2.evaluation.evaluator INFO: Inference done 168/500. Dataloading: 0.0013 s/iter. Inference: 0.4166 s/iter. Eval: 0.0000 s/iter. Total: 0.4180 s/iter. ETA=0:02:18
[04/29 14:35:59] d2.evaluation.evaluator INFO: Inference done 180/500. Dataloading: 0.0013 s/iter. Inference: 0.4168 s/iter. Eval: 0.0000 s/iter. Total: 0.4182 s/iter. ETA=0:02:13
[04/29 14:36:04] d2.evaluation.evaluator INFO: Inference done 192/500. Dataloading: 0.0013 s/iter. Inference: 0.4169 s/iter. Eval: 0.0000 s/iter. Total: 0.4183 s/iter. ETA=0:02:08
[04/29 14:36:09] d2.evaluation.evaluator INFO: Inference done 204/500. Dataloading: 0.0013 s/iter. Inference: 0.4170 s/iter. Eval: 0.0000 s/iter. Total: 0.4184 s/iter. ETA=0:02:03
[04/29 14:36:14] d2.evaluation.evaluator INFO: Inference done 216/500. Dataloading: 0.0013 s/iter. Inference: 0.4171 s/iter. Eval: 0.0000 s/iter. Total: 0.4185 s/iter. ETA=0:01:58
[04/29 14:36:19] d2.evaluation.evaluator INFO: Inference done 228/500. Dataloading: 0.0013 s/iter. Inference: 0.4172 s/iter. Eval: 0.0000 s/iter. Total: 0.4186 s/iter. ETA=0:01:53
[04/29 14:36:24] d2.evaluation.evaluator INFO: Inference done 240/500. Dataloading: 0.0013 s/iter. Inference: 0.4173 s/iter. Eval: 0.0000 s/iter. Total: 0.4187 s/iter. ETA=0:01:48
[04/29 14:36:29] d2.evaluation.evaluator INFO: Inference done 252/500. Dataloading: 0.0013 s/iter. Inference: 0.4174 s/iter. Eval: 0.0000 s/iter. Total: 0.4188 s/iter. ETA=0:01:43
[04/29 14:36:34] d2.evaluation.evaluator INFO: Inference done 264/500. Dataloading: 0.0013 s/iter. Inference: 0.4176 s/iter. Eval: 0.0000 s/iter. Total: 0.4190 s/iter. ETA=0:01:38
[04/29 14:36:39] d2.evaluation.evaluator INFO: Inference done 276/500. Dataloading: 0.0013 s/iter. Inference: 0.4177 s/iter. Eval: 0.0000 s/iter. Total: 0.4191 s/iter. ETA=0:01:33
[04/29 14:36:45] d2.evaluation.evaluator INFO: Inference done 288/500. Dataloading: 0.0013 s/iter. Inference: 0.4178 s/iter. Eval: 0.0000 s/iter. Total: 0.4192 s/iter. ETA=0:01:28
[04/29 14:36:50] d2.evaluation.evaluator INFO: Inference done 300/500. Dataloading: 0.0013 s/iter. Inference: 0.4178 s/iter. Eval: 0.0000 s/iter. Total: 0.4193 s/iter. ETA=0:01:23
[04/29 14:36:55] d2.evaluation.evaluator INFO: Inference done 312/500. Dataloading: 0.0014 s/iter. Inference: 0.4179 s/iter. Eval: 0.0000 s/iter. Total: 0.4193 s/iter. ETA=0:01:18
[04/29 14:37:00] d2.evaluation.evaluator INFO: Inference done 324/500. Dataloading: 0.0014 s/iter. Inference: 0.4180 s/iter. Eval: 0.0000 s/iter. Total: 0.4194 s/iter. ETA=0:01:13
[04/29 14:37:05] d2.evaluation.evaluator INFO: Inference done 336/500. Dataloading: 0.0014 s/iter. Inference: 0.4180 s/iter. Eval: 0.0000 s/iter. Total: 0.4194 s/iter. ETA=0:01:08
[04/29 14:37:10] d2.evaluation.evaluator INFO: Inference done 349/500. Dataloading: 0.0014 s/iter. Inference: 0.4174 s/iter. Eval: 0.0000 s/iter. Total: 0.4189 s/iter. ETA=0:01:03
[04/29 14:37:15] d2.evaluation.evaluator INFO: Inference done 361/500. Dataloading: 0.0014 s/iter. Inference: 0.4176 s/iter. Eval: 0.0000 s/iter. Total: 0.4190 s/iter. ETA=0:00:58
[04/29 14:37:20] d2.evaluation.evaluator INFO: Inference done 373/500. Dataloading: 0.0014 s/iter. Inference: 0.4176 s/iter. Eval: 0.0000 s/iter. Total: 0.4191 s/iter. ETA=0:00:53
[04/29 14:37:25] d2.evaluation.evaluator INFO: Inference done 385/500. Dataloading: 0.0014 s/iter. Inference: 0.4177 s/iter. Eval: 0.0000 s/iter. Total: 0.4192 s/iter. ETA=0:00:48
[04/29 14:37:30] d2.evaluation.evaluator INFO: Inference done 397/500. Dataloading: 0.0014 s/iter. Inference: 0.4178 s/iter. Eval: 0.0000 s/iter. Total: 0.4193 s/iter. ETA=0:00:43
[04/29 14:37:35] d2.evaluation.evaluator INFO: Inference done 409/500. Dataloading: 0.0014 s/iter. Inference: 0.4179 s/iter. Eval: 0.0000 s/iter. Total: 0.4193 s/iter. ETA=0:00:38
[04/29 14:37:40] d2.evaluation.evaluator INFO: Inference done 421/500. Dataloading: 0.0014 s/iter. Inference: 0.4180 s/iter. Eval: 0.0000 s/iter. Total: 0.4194 s/iter. ETA=0:00:33
[04/29 14:37:45] d2.evaluation.evaluator INFO: Inference done 433/500. Dataloading: 0.0014 s/iter. Inference: 0.4181 s/iter. Eval: 0.0000 s/iter. Total: 0.4195 s/iter. ETA=0:00:28
[04/29 14:37:51] d2.evaluation.evaluator INFO: Inference done 445/500. Dataloading: 0.0014 s/iter. Inference: 0.4182 s/iter. Eval: 0.0000 s/iter. Total: 0.4196 s/iter. ETA=0:00:23
[04/29 14:37:56] d2.evaluation.evaluator INFO: Inference done 457/500. Dataloading: 0.0014 s/iter. Inference: 0.4183 s/iter. Eval: 0.0000 s/iter. Total: 0.4197 s/iter. ETA=0:00:18
[04/29 14:38:01] d2.evaluation.evaluator INFO: Inference done 469/500. Dataloading: 0.0014 s/iter. Inference: 0.4184 s/iter. Eval: 0.0000 s/iter. Total: 0.4198 s/iter. ETA=0:00:13
[04/29 14:38:06] d2.evaluation.evaluator INFO: Inference done 481/500. Dataloading: 0.0014 s/iter. Inference: 0.4185 s/iter. Eval: 0.0000 s/iter. Total: 0.4199 s/iter. ETA=0:00:07
[04/29 14:38:11] d2.evaluation.evaluator INFO: Inference done 493/500. Dataloading: 0.0014 s/iter. Inference: 0.4185 s/iter. Eval: 0.0000 s/iter. Total: 0.4200 s/iter. ETA=0:00:02
[04/29 14:38:14] d2.evaluation.evaluator INFO: Total inference time: 0:03:27.943346 (0.420088 s / iter per device, on 1 devices)
[04/29 14:38:14] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:03:27 (0.418566 s / iter per device, on 1 devices)
[04/29 14:38:14] adet.evaluation.text_evaluation_all INFO: Saving results to output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/inference/text_results.json
[04/29 16:05:14] detectron2 INFO: Rank of current process: 0. World size: 1
[04/29 16:05:14] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/kylee/DeepSolo_LangPriors/detectron2/detectron2
Compiler                         GCC 9.5
CUDA compiler                    CUDA 11.3
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.12.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 3060 (arch=8.6)
Driver version                   550.120
CUDA_HOME                        /usr
Pillow                           9.0.1
torchvision                      0.13.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[04/29 16:05:14] detectron2 INFO: Command line arguments: Namespace(config_file='configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50152', opts=['MODEL.WEIGHTS', 'output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth'])
[04/29 16:05:14] detectron2 INFO: Contents of args.config_file=configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml:
_BASE_: "../Base_det.yaml"

MODEL:
  WEIGHTS: "output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth"
  TRANSFORMER:
    INFERENCE_TH_TEST: 0.3

DATASETS:
  TRAIN: ("ic15_train",)
  TEST: ("ic15_test",)

INPUT:
  MIN_SIZE_TRAIN: (640, 700, 760, 820, 880, 940, 1000)
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MAX_SIZE_TEST: 4000
  CROP:
    ENABLED: False
  ROTATE: False

SOLVER:
  AMP:
    ENABLED: True  # Enable Automatic Mixed Precision
  IMS_PER_BATCH: 2
  BASE_LR: 1e-5
  LR_BACKBONE: 1e-6
  WARMUP_ITERS: 250
  STEPS: (15000, 25000)  # no step
  MAX_ITER: 30000
  GAMMA: 0.1
  CHECKPOINT_PERIOD: 1000

TEST:
  EVAL_PERIOD: 1000
  # 1 - Generic, 2 - Weak, 3 - Strong (for icdar2015)
  LEXICON_TYPE: 3

OUTPUT_DIR: "output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15"

[04/29 16:05:14] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ic15_test
  TRAIN:
  - ic15_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    CROP_INSTANCE: false
    ENABLED: false
    SIZE:
    - 0.1
    - 0.1
    TYPE: relative_range
  FORMAT: RGB
  HFLIP_TRAIN: false
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 4000
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MIN_SIZE_TRAIN:
  - 640
  - 700
  - 760
  - 820
  - 880
  - 940
  - 1000
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  ROTATE: false
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    ANTI_ALIAS: false
    FREEZE_AT: 2
    NAME: build_resnet_backbone
  BASIS_MODULE:
    ANN_SET: coco
    COMMON_STRIDE: 8
    CONVS_DIM: 128
    IN_FEATURES:
    - p3
    - p4
    - p5
    LOSS_ON: false
    LOSS_WEIGHT: 0.3
    NAME: ProtoNet
    NORM: SyncBN
    NUM_BASES: 4
    NUM_CLASSES: 80
    NUM_CONVS: 3
  BATEXT:
    CANONICAL_SIZE: 96
    CONV_DIM: 256
    CUSTOM_DICT: ''
    IN_FEATURES:
    - p2
    - p3
    - p4
    NUM_CHARS: 25
    NUM_CONV: 2
    POOLER_RESOLUTION:
    - 8
    - 32
    POOLER_SCALES:
    - 0.25
    - 0.125
    - 0.0625
    RECOGNITION_LOSS: ctc
    RECOGNIZER: attn
    SAMPLING_RATIO: 1
    USE_AET: false
    USE_COORDCONV: false
    VOC_SIZE: 96
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: false
  META_ARCHITECTURE: TransformerPureDetector
  MOBILENET: false
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_INTERVAL: 1
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  SWIN:
    DROP_PATH_RATE: 0.2
    TYPE: tiny
  TOP_MODULE:
    DIM: 16
    NAME: conv
  TRANSFORMER:
    AUX_LOSS: true
    BOUNDARY_HEAD: true
    CUSTOM_DICT: ''
    DEC_LAYERS: 6
    DEC_N_POINTS: 4
    DIM_FEEDFORWARD: 1024
    DROPOUT: 0.0
    ENABLED: true
    ENC_LAYERS: 6
    ENC_N_POINTS: 4
    HIDDEN_DIM: 256
    INFERENCE_TH_TEST: 0.3
    LOSS:
      AUX_LOSS: true
      BEZIER_CLASS_WEIGHT: 1.0
      BEZIER_COORD_WEIGHT: 1.0
      BEZIER_SAMPLE_POINTS: 25
      BOUNDARY_WEIGHT: 0.5
      FOCAL_ALPHA: 0.25
      FOCAL_GAMMA: 2.0
      POINT_CLASS_WEIGHT: 1.0
      POINT_COORD_WEIGHT: 1.0
      POINT_TEXT_WEIGHT: 0.5
    NHEADS: 8
    NUM_FEATURE_LEVELS: 4
    NUM_POINTS: 25
    NUM_QUERIES: 100
    POSITION_EMBEDDING_SCALE: 6.283185307179586
    TEMPERATURE: 10000
    VOC_SIZE: 37
  ViTAEv2:
    DROP_PATH_RATE: 0.2
    TYPE: vitaev2_s
  WEIGHTS: output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth
OUTPUT_DIR: output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15
SEED: 42
SOLVER:
  AMP:
    ENABLED: true
  BASE_LR: 1.0e-05
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.1
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 2
  LR_BACKBONE: 1.0e-06
  LR_BACKBONE_NAMES:
  - backbone.0
  LR_LINEAR_PROJ_MULT: 1.0
  LR_LINEAR_PROJ_NAMES:
  - reference_points
  - sampling_offsets
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 30000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 15000
  - 25000
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 250
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  LEXICON_TYPE: 3
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/29 16:05:14] detectron2 INFO: Full config saved to output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/config.yaml
[04/29 16:05:16] d2.engine.defaults INFO: Model:
TransformerPureDetector(
  (detection_transformer): DETECTION_TRANSFORMER(
    (backbone): Joiner(
      (0): MaskedBackbone(
        (backbone): ResNet(
          (stem): BasicStem(
            (conv1): Conv2d(
              3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
          )
          (res2): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv1): Conv2d(
                64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
          )
          (res3): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv1): Conv2d(
                256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
          )
          (res4): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
              (conv1): Conv2d(
                512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (4): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (5): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
          )
          (res5): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
              (conv1): Conv2d(
                1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
          )
        )
      )
      (1): PositionalEncoding2D()
    )
    (point_embed): Embedding(2500, 256)
    (transformer): DeformableTransformer(
      (encoder): DeformableTransformerEncoder(
        (layers): ModuleList(
          (0): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (decoder): DeformableCompositeTransformerDecoder(
        (layers): ModuleList(
          (0): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ctrl_point_coord): ModuleList(
          (0): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (1): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (2): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (3): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (4): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (5): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
        )
      )
      (enc_output): Linear(in_features=256, out_features=256, bias=True)
      (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (bezier_coord_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=8, bias=True)
        )
      )
      (bezier_class_embed): Linear(in_features=256, out_features=1, bias=True)
    )
    (input_proj): ModuleList(
      (0): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (1): Sequential(
        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (2): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (3): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (bezier_proposal_coord): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=8, bias=True)
      )
    )
    (bezier_proposal_class): Linear(in_features=256, out_features=1, bias=True)
    (ctrl_point_coord): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
    )
    (ctrl_point_class): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (ctrl_point_text): ModuleList(
      (0): Linear(in_features=256, out_features=38, bias=True)
      (1): Linear(in_features=256, out_features=38, bias=True)
      (2): Linear(in_features=256, out_features=38, bias=True)
      (3): Linear(in_features=256, out_features=38, bias=True)
      (4): Linear(in_features=256, out_features=38, bias=True)
      (5): Linear(in_features=256, out_features=38, bias=True)
    )
    (boundary_offset): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (enc_matcher): BezierHungarianMatcher(
      (bezier_sampler): BezierSampler()
    )
    (dec_matcher): CtrlPointHungarianMatcher()
    (bezier_sampler): BezierSampler()
  )
)
[04/29 16:05:16] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth ...
[04/29 16:05:16] fvcore.common.checkpoint INFO: [Checkpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth ...
[04/29 16:05:16] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1440, 1440), max_size=4000, sample_style='choice')]
[04/29 16:05:16] adet.data.dataset_mapper INFO: Rebuilding the augmentations. The previous augmentations will be overridden.
[04/29 16:05:16] adet.data.datasets.text INFO: Loaded 500 images in COCO format from datasets/ic15/test.json
[04/29 16:05:16] d2.data.build INFO: Distribution of instances among all 1 categories:
[36m|  category  | #instances   |
|:----------:|:-------------|
|    text    | 0            |
|            |              |[0m
[04/29 16:05:16] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[04/29 16:05:16] d2.data.common INFO: Serializing 500 elements to byte tensors and concatenating them all ...
[04/29 16:05:16] d2.data.common INFO: Serialized dataset takes 0.06 MiB
[04/29 16:05:16] d2.evaluation.evaluator INFO: Start inference on 500 batches
[04/29 16:05:22] d2.evaluation.evaluator INFO: Inference done 11/500. Dataloading: 0.0010 s/iter. Inference: 0.3833 s/iter. Eval: 0.0000 s/iter. Total: 0.3843 s/iter. ETA=0:03:07
[04/29 16:05:27] d2.evaluation.evaluator INFO: Inference done 24/500. Dataloading: 0.0010 s/iter. Inference: 0.3841 s/iter. Eval: 0.0000 s/iter. Total: 0.3852 s/iter. ETA=0:03:03
[04/29 16:05:32] d2.evaluation.evaluator INFO: Inference done 37/500. Dataloading: 0.0012 s/iter. Inference: 0.3922 s/iter. Eval: 0.0000 s/iter. Total: 0.3935 s/iter. ETA=0:03:02
[04/29 16:05:37] d2.evaluation.evaluator INFO: Inference done 49/500. Dataloading: 0.0012 s/iter. Inference: 0.3991 s/iter. Eval: 0.0000 s/iter. Total: 0.4003 s/iter. ETA=0:03:00
[04/29 16:05:42] d2.evaluation.evaluator INFO: Inference done 61/500. Dataloading: 0.0012 s/iter. Inference: 0.4028 s/iter. Eval: 0.0000 s/iter. Total: 0.4041 s/iter. ETA=0:02:57
[04/29 16:05:47] d2.evaluation.evaluator INFO: Inference done 73/500. Dataloading: 0.0012 s/iter. Inference: 0.4054 s/iter. Eval: 0.0000 s/iter. Total: 0.4067 s/iter. ETA=0:02:53
[04/29 16:05:52] d2.evaluation.evaluator INFO: Inference done 85/500. Dataloading: 0.0013 s/iter. Inference: 0.4073 s/iter. Eval: 0.0000 s/iter. Total: 0.4087 s/iter. ETA=0:02:49
[04/29 16:05:57] d2.evaluation.evaluator INFO: Inference done 97/500. Dataloading: 0.0013 s/iter. Inference: 0.4086 s/iter. Eval: 0.0000 s/iter. Total: 0.4100 s/iter. ETA=0:02:45
[04/29 16:06:02] d2.evaluation.evaluator INFO: Inference done 109/500. Dataloading: 0.0013 s/iter. Inference: 0.4098 s/iter. Eval: 0.0000 s/iter. Total: 0.4111 s/iter. ETA=0:02:40
[04/29 16:06:07] d2.evaluation.evaluator INFO: Inference done 121/500. Dataloading: 0.0013 s/iter. Inference: 0.4105 s/iter. Eval: 0.0000 s/iter. Total: 0.4119 s/iter. ETA=0:02:36
[04/29 16:06:12] d2.evaluation.evaluator INFO: Inference done 133/500. Dataloading: 0.0013 s/iter. Inference: 0.4112 s/iter. Eval: 0.0000 s/iter. Total: 0.4126 s/iter. ETA=0:02:31
[04/29 16:06:17] d2.evaluation.evaluator INFO: Inference done 145/500. Dataloading: 0.0013 s/iter. Inference: 0.4117 s/iter. Eval: 0.0000 s/iter. Total: 0.4131 s/iter. ETA=0:02:26
[04/29 16:06:22] d2.evaluation.evaluator INFO: Inference done 157/500. Dataloading: 0.0013 s/iter. Inference: 0.4122 s/iter. Eval: 0.0000 s/iter. Total: 0.4136 s/iter. ETA=0:02:21
[04/29 16:06:27] d2.evaluation.evaluator INFO: Inference done 169/500. Dataloading: 0.0013 s/iter. Inference: 0.4127 s/iter. Eval: 0.0000 s/iter. Total: 0.4141 s/iter. ETA=0:02:17
[04/29 16:06:32] d2.evaluation.evaluator INFO: Inference done 181/500. Dataloading: 0.0013 s/iter. Inference: 0.4130 s/iter. Eval: 0.0000 s/iter. Total: 0.4144 s/iter. ETA=0:02:12
[04/29 16:06:37] d2.evaluation.evaluator INFO: Inference done 193/500. Dataloading: 0.0013 s/iter. Inference: 0.4133 s/iter. Eval: 0.0000 s/iter. Total: 0.4147 s/iter. ETA=0:02:07
[04/29 16:06:42] d2.evaluation.evaluator INFO: Inference done 205/500. Dataloading: 0.0013 s/iter. Inference: 0.4136 s/iter. Eval: 0.0000 s/iter. Total: 0.4151 s/iter. ETA=0:02:02
[04/29 16:06:47] d2.evaluation.evaluator INFO: Inference done 217/500. Dataloading: 0.0013 s/iter. Inference: 0.4138 s/iter. Eval: 0.0000 s/iter. Total: 0.4152 s/iter. ETA=0:01:57
[04/29 16:06:53] d2.evaluation.evaluator INFO: Inference done 230/500. Dataloading: 0.0013 s/iter. Inference: 0.4136 s/iter. Eval: 0.0000 s/iter. Total: 0.4150 s/iter. ETA=0:01:52
[04/29 16:06:58] d2.evaluation.evaluator INFO: Inference done 242/500. Dataloading: 0.0013 s/iter. Inference: 0.4139 s/iter. Eval: 0.0000 s/iter. Total: 0.4153 s/iter. ETA=0:01:47
[04/29 16:07:03] d2.evaluation.evaluator INFO: Inference done 254/500. Dataloading: 0.0013 s/iter. Inference: 0.4141 s/iter. Eval: 0.0000 s/iter. Total: 0.4155 s/iter. ETA=0:01:42
[04/29 16:07:08] d2.evaluation.evaluator INFO: Inference done 266/500. Dataloading: 0.0014 s/iter. Inference: 0.4143 s/iter. Eval: 0.0000 s/iter. Total: 0.4157 s/iter. ETA=0:01:37
[04/29 16:07:13] d2.evaluation.evaluator INFO: Inference done 278/500. Dataloading: 0.0014 s/iter. Inference: 0.4145 s/iter. Eval: 0.0000 s/iter. Total: 0.4159 s/iter. ETA=0:01:32
[04/29 16:07:18] d2.evaluation.evaluator INFO: Inference done 290/500. Dataloading: 0.0014 s/iter. Inference: 0.4146 s/iter. Eval: 0.0000 s/iter. Total: 0.4161 s/iter. ETA=0:01:27
[04/29 16:07:23] d2.evaluation.evaluator INFO: Inference done 302/500. Dataloading: 0.0014 s/iter. Inference: 0.4149 s/iter. Eval: 0.0000 s/iter. Total: 0.4163 s/iter. ETA=0:01:22
[04/29 16:07:28] d2.evaluation.evaluator INFO: Inference done 314/500. Dataloading: 0.0014 s/iter. Inference: 0.4151 s/iter. Eval: 0.0000 s/iter. Total: 0.4165 s/iter. ETA=0:01:17
[04/29 16:07:33] d2.evaluation.evaluator INFO: Inference done 326/500. Dataloading: 0.0014 s/iter. Inference: 0.4152 s/iter. Eval: 0.0000 s/iter. Total: 0.4167 s/iter. ETA=0:01:12
[04/29 16:07:38] d2.evaluation.evaluator INFO: Inference done 338/500. Dataloading: 0.0014 s/iter. Inference: 0.4154 s/iter. Eval: 0.0000 s/iter. Total: 0.4168 s/iter. ETA=0:01:07
[04/29 16:07:43] d2.evaluation.evaluator INFO: Inference done 350/500. Dataloading: 0.0014 s/iter. Inference: 0.4155 s/iter. Eval: 0.0000 s/iter. Total: 0.4169 s/iter. ETA=0:01:02
[04/29 16:07:48] d2.evaluation.evaluator INFO: Inference done 362/500. Dataloading: 0.0014 s/iter. Inference: 0.4156 s/iter. Eval: 0.0000 s/iter. Total: 0.4170 s/iter. ETA=0:00:57
[04/29 16:07:53] d2.evaluation.evaluator INFO: Inference done 374/500. Dataloading: 0.0014 s/iter. Inference: 0.4157 s/iter. Eval: 0.0000 s/iter. Total: 0.4172 s/iter. ETA=0:00:52
[04/29 16:07:58] d2.evaluation.evaluator INFO: Inference done 386/500. Dataloading: 0.0014 s/iter. Inference: 0.4159 s/iter. Eval: 0.0000 s/iter. Total: 0.4173 s/iter. ETA=0:00:47
[04/29 16:08:03] d2.evaluation.evaluator INFO: Inference done 398/500. Dataloading: 0.0014 s/iter. Inference: 0.4160 s/iter. Eval: 0.0000 s/iter. Total: 0.4174 s/iter. ETA=0:00:42
[04/29 16:08:09] d2.evaluation.evaluator INFO: Inference done 410/500. Dataloading: 0.0014 s/iter. Inference: 0.4161 s/iter. Eval: 0.0000 s/iter. Total: 0.4176 s/iter. ETA=0:00:37
[04/29 16:08:14] d2.evaluation.evaluator INFO: Inference done 422/500. Dataloading: 0.0014 s/iter. Inference: 0.4162 s/iter. Eval: 0.0000 s/iter. Total: 0.4177 s/iter. ETA=0:00:32
[04/29 16:08:19] d2.evaluation.evaluator INFO: Inference done 434/500. Dataloading: 0.0014 s/iter. Inference: 0.4163 s/iter. Eval: 0.0000 s/iter. Total: 0.4178 s/iter. ETA=0:00:27
[04/29 16:08:24] d2.evaluation.evaluator INFO: Inference done 446/500. Dataloading: 0.0014 s/iter. Inference: 0.4164 s/iter. Eval: 0.0000 s/iter. Total: 0.4179 s/iter. ETA=0:00:22
[04/29 16:08:29] d2.evaluation.evaluator INFO: Inference done 458/500. Dataloading: 0.0014 s/iter. Inference: 0.4166 s/iter. Eval: 0.0000 s/iter. Total: 0.4180 s/iter. ETA=0:00:17
[04/29 16:08:34] d2.evaluation.evaluator INFO: Inference done 470/500. Dataloading: 0.0014 s/iter. Inference: 0.4166 s/iter. Eval: 0.0000 s/iter. Total: 0.4181 s/iter. ETA=0:00:12
[04/29 16:08:39] d2.evaluation.evaluator INFO: Inference done 482/500. Dataloading: 0.0014 s/iter. Inference: 0.4166 s/iter. Eval: 0.0000 s/iter. Total: 0.4181 s/iter. ETA=0:00:07
[04/29 16:08:44] d2.evaluation.evaluator INFO: Inference done 494/500. Dataloading: 0.0014 s/iter. Inference: 0.4167 s/iter. Eval: 0.0000 s/iter. Total: 0.4181 s/iter. ETA=0:00:02
[04/29 16:08:46] d2.evaluation.evaluator INFO: Total inference time: 0:03:27.025513 (0.418233 s / iter per device, on 1 devices)
[04/29 16:08:46] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:03:26 (0.416710 s / iter per device, on 1 devices)
[04/29 16:08:46] adet.evaluation.text_evaluation_all INFO: Saving results to output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/inference/text_results.json
[04/29 16:17:43] detectron2 INFO: Rank of current process: 0. World size: 1
[04/29 16:17:43] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/kylee/DeepSolo_LangPriors/detectron2/detectron2
Compiler                         GCC 9.5
CUDA compiler                    CUDA 11.3
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.12.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 3060 (arch=8.6)
Driver version                   550.120
CUDA_HOME                        /usr
Pillow                           9.0.1
torchvision                      0.13.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[04/29 16:17:43] detectron2 INFO: Command line arguments: Namespace(config_file='configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50152', opts=['MODEL.WEIGHTS', 'output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth'])
[04/29 16:17:43] detectron2 INFO: Contents of args.config_file=configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml:
_BASE_: "../Base_det.yaml"

MODEL:
  WEIGHTS: "output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth"
  TRANSFORMER:
    INFERENCE_TH_TEST: 0.3

DATASETS:
  TRAIN: ("ic15_train",)
  TEST: ("ic15_test",)

INPUT:
  MIN_SIZE_TRAIN: (640, 700, 760, 820, 880, 940, 1000)
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MAX_SIZE_TEST: 4000
  CROP:
    ENABLED: False
  ROTATE: False

SOLVER:
  AMP:
    ENABLED: True  # Enable Automatic Mixed Precision
  IMS_PER_BATCH: 2
  BASE_LR: 1e-5
  LR_BACKBONE: 1e-6
  WARMUP_ITERS: 250
  STEPS: (15000, 25000)  # no step
  MAX_ITER: 30000
  GAMMA: 0.1
  CHECKPOINT_PERIOD: 1000

TEST:
  EVAL_PERIOD: 1000
  # 1 - Generic, 2 - Weak, 3 - Strong (for icdar2015)
  LEXICON_TYPE: 3

OUTPUT_DIR: "output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15"

[04/29 16:17:43] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ic15_test
  TRAIN:
  - ic15_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    CROP_INSTANCE: false
    ENABLED: false
    SIZE:
    - 0.1
    - 0.1
    TYPE: relative_range
  FORMAT: RGB
  HFLIP_TRAIN: false
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 4000
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MIN_SIZE_TRAIN:
  - 640
  - 700
  - 760
  - 820
  - 880
  - 940
  - 1000
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  ROTATE: false
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    ANTI_ALIAS: false
    FREEZE_AT: 2
    NAME: build_resnet_backbone
  BASIS_MODULE:
    ANN_SET: coco
    COMMON_STRIDE: 8
    CONVS_DIM: 128
    IN_FEATURES:
    - p3
    - p4
    - p5
    LOSS_ON: false
    LOSS_WEIGHT: 0.3
    NAME: ProtoNet
    NORM: SyncBN
    NUM_BASES: 4
    NUM_CLASSES: 80
    NUM_CONVS: 3
  BATEXT:
    CANONICAL_SIZE: 96
    CONV_DIM: 256
    CUSTOM_DICT: ''
    IN_FEATURES:
    - p2
    - p3
    - p4
    NUM_CHARS: 25
    NUM_CONV: 2
    POOLER_RESOLUTION:
    - 8
    - 32
    POOLER_SCALES:
    - 0.25
    - 0.125
    - 0.0625
    RECOGNITION_LOSS: ctc
    RECOGNIZER: attn
    SAMPLING_RATIO: 1
    USE_AET: false
    USE_COORDCONV: false
    VOC_SIZE: 96
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: false
  META_ARCHITECTURE: TransformerPureDetector
  MOBILENET: false
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_INTERVAL: 1
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  SWIN:
    DROP_PATH_RATE: 0.2
    TYPE: tiny
  TOP_MODULE:
    DIM: 16
    NAME: conv
  TRANSFORMER:
    AUX_LOSS: true
    BOUNDARY_HEAD: true
    CUSTOM_DICT: ''
    DEC_LAYERS: 6
    DEC_N_POINTS: 4
    DIM_FEEDFORWARD: 1024
    DROPOUT: 0.0
    ENABLED: true
    ENC_LAYERS: 6
    ENC_N_POINTS: 4
    HIDDEN_DIM: 256
    INFERENCE_TH_TEST: 0.3
    LOSS:
      AUX_LOSS: true
      BEZIER_CLASS_WEIGHT: 1.0
      BEZIER_COORD_WEIGHT: 1.0
      BEZIER_SAMPLE_POINTS: 25
      BOUNDARY_WEIGHT: 0.5
      FOCAL_ALPHA: 0.25
      FOCAL_GAMMA: 2.0
      POINT_CLASS_WEIGHT: 1.0
      POINT_COORD_WEIGHT: 1.0
      POINT_TEXT_WEIGHT: 0.5
    NHEADS: 8
    NUM_FEATURE_LEVELS: 4
    NUM_POINTS: 25
    NUM_QUERIES: 100
    POSITION_EMBEDDING_SCALE: 6.283185307179586
    TEMPERATURE: 10000
    VOC_SIZE: 37
  ViTAEv2:
    DROP_PATH_RATE: 0.2
    TYPE: vitaev2_s
  WEIGHTS: output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth
OUTPUT_DIR: output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15
SEED: 42
SOLVER:
  AMP:
    ENABLED: true
  BASE_LR: 1.0e-05
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.1
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 2
  LR_BACKBONE: 1.0e-06
  LR_BACKBONE_NAMES:
  - backbone.0
  LR_LINEAR_PROJ_MULT: 1.0
  LR_LINEAR_PROJ_NAMES:
  - reference_points
  - sampling_offsets
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 30000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 15000
  - 25000
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 250
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  LEXICON_TYPE: 3
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/29 16:17:43] detectron2 INFO: Full config saved to output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/config.yaml
[04/29 16:17:45] d2.engine.defaults INFO: Model:
TransformerPureDetector(
  (detection_transformer): DETECTION_TRANSFORMER(
    (backbone): Joiner(
      (0): MaskedBackbone(
        (backbone): ResNet(
          (stem): BasicStem(
            (conv1): Conv2d(
              3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
          )
          (res2): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv1): Conv2d(
                64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
          )
          (res3): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv1): Conv2d(
                256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
          )
          (res4): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
              (conv1): Conv2d(
                512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (4): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (5): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
          )
          (res5): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
              (conv1): Conv2d(
                1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
          )
        )
      )
      (1): PositionalEncoding2D()
    )
    (point_embed): Embedding(2500, 256)
    (transformer): DeformableTransformer(
      (encoder): DeformableTransformerEncoder(
        (layers): ModuleList(
          (0): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (decoder): DeformableCompositeTransformerDecoder(
        (layers): ModuleList(
          (0): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ctrl_point_coord): ModuleList(
          (0): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (1): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (2): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (3): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (4): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (5): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
        )
      )
      (enc_output): Linear(in_features=256, out_features=256, bias=True)
      (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (bezier_coord_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=8, bias=True)
        )
      )
      (bezier_class_embed): Linear(in_features=256, out_features=1, bias=True)
    )
    (input_proj): ModuleList(
      (0): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (1): Sequential(
        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (2): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (3): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (bezier_proposal_coord): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=8, bias=True)
      )
    )
    (bezier_proposal_class): Linear(in_features=256, out_features=1, bias=True)
    (ctrl_point_coord): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
    )
    (ctrl_point_class): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (ctrl_point_text): ModuleList(
      (0): Linear(in_features=256, out_features=38, bias=True)
      (1): Linear(in_features=256, out_features=38, bias=True)
      (2): Linear(in_features=256, out_features=38, bias=True)
      (3): Linear(in_features=256, out_features=38, bias=True)
      (4): Linear(in_features=256, out_features=38, bias=True)
      (5): Linear(in_features=256, out_features=38, bias=True)
    )
    (boundary_offset): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (enc_matcher): BezierHungarianMatcher(
      (bezier_sampler): BezierSampler()
    )
    (dec_matcher): CtrlPointHungarianMatcher()
    (bezier_sampler): BezierSampler()
  )
)
[04/29 16:17:45] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth ...
[04/29 16:17:45] fvcore.common.checkpoint INFO: [Checkpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth ...
[04/29 16:17:45] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1440, 1440), max_size=4000, sample_style='choice')]
[04/29 16:17:45] adet.data.dataset_mapper INFO: Rebuilding the augmentations. The previous augmentations will be overridden.
[04/29 16:17:45] adet.data.datasets.text INFO: Loaded 500 images in COCO format from datasets/ic15/test.json
[04/29 16:17:45] d2.data.build INFO: Distribution of instances among all 1 categories:
[36m|  category  | #instances   |
|:----------:|:-------------|
|    text    | 0            |
|            |              |[0m
[04/29 16:17:45] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[04/29 16:17:45] d2.data.common INFO: Serializing 500 elements to byte tensors and concatenating them all ...
[04/29 16:17:45] d2.data.common INFO: Serialized dataset takes 0.06 MiB
[04/29 16:17:45] d2.evaluation.evaluator INFO: Start inference on 500 batches
[04/29 16:17:51] d2.evaluation.evaluator INFO: Inference done 11/500. Dataloading: 0.0010 s/iter. Inference: 0.4121 s/iter. Eval: 0.0001 s/iter. Total: 0.4133 s/iter. ETA=0:03:22
[04/29 16:17:57] d2.evaluation.evaluator INFO: Inference done 24/500. Dataloading: 0.0012 s/iter. Inference: 0.4138 s/iter. Eval: 0.0001 s/iter. Total: 0.4151 s/iter. ETA=0:03:17
[04/29 16:18:02] d2.evaluation.evaluator INFO: Inference done 37/500. Dataloading: 0.0012 s/iter. Inference: 0.4141 s/iter. Eval: 0.0001 s/iter. Total: 0.4155 s/iter. ETA=0:03:12
[04/29 16:18:07] d2.evaluation.evaluator INFO: Inference done 49/500. Dataloading: 0.0013 s/iter. Inference: 0.4146 s/iter. Eval: 0.0001 s/iter. Total: 0.4160 s/iter. ETA=0:03:07
[04/29 16:18:12] d2.evaluation.evaluator INFO: Inference done 61/500. Dataloading: 0.0013 s/iter. Inference: 0.4149 s/iter. Eval: 0.0001 s/iter. Total: 0.4163 s/iter. ETA=0:03:02
[04/29 16:18:17] d2.evaluation.evaluator INFO: Inference done 73/500. Dataloading: 0.0013 s/iter. Inference: 0.4151 s/iter. Eval: 0.0001 s/iter. Total: 0.4165 s/iter. ETA=0:02:57
[04/29 16:18:22] d2.evaluation.evaluator INFO: Inference done 85/500. Dataloading: 0.0013 s/iter. Inference: 0.4153 s/iter. Eval: 0.0001 s/iter. Total: 0.4168 s/iter. ETA=0:02:52
[04/29 16:18:27] d2.evaluation.evaluator INFO: Inference done 97/500. Dataloading: 0.0013 s/iter. Inference: 0.4156 s/iter. Eval: 0.0001 s/iter. Total: 0.4170 s/iter. ETA=0:02:48
[04/29 16:18:32] d2.evaluation.evaluator INFO: Inference done 109/500. Dataloading: 0.0013 s/iter. Inference: 0.4159 s/iter. Eval: 0.0001 s/iter. Total: 0.4174 s/iter. ETA=0:02:43
[04/29 16:18:37] d2.evaluation.evaluator INFO: Inference done 121/500. Dataloading: 0.0013 s/iter. Inference: 0.4161 s/iter. Eval: 0.0001 s/iter. Total: 0.4176 s/iter. ETA=0:02:38
[04/29 16:18:42] d2.evaluation.evaluator INFO: Inference done 133/500. Dataloading: 0.0014 s/iter. Inference: 0.4164 s/iter. Eval: 0.0001 s/iter. Total: 0.4179 s/iter. ETA=0:02:33
[04/29 16:18:47] d2.evaluation.evaluator INFO: Inference done 145/500. Dataloading: 0.0014 s/iter. Inference: 0.4166 s/iter. Eval: 0.0001 s/iter. Total: 0.4181 s/iter. ETA=0:02:28
[04/29 16:18:52] d2.evaluation.evaluator INFO: Inference done 157/500. Dataloading: 0.0014 s/iter. Inference: 0.4167 s/iter. Eval: 0.0001 s/iter. Total: 0.4182 s/iter. ETA=0:02:23
[04/29 16:18:58] d2.evaluation.evaluator INFO: Inference done 169/500. Dataloading: 0.0014 s/iter. Inference: 0.4169 s/iter. Eval: 0.0001 s/iter. Total: 0.4184 s/iter. ETA=0:02:18
[04/29 16:19:03] d2.evaluation.evaluator INFO: Inference done 181/500. Dataloading: 0.0014 s/iter. Inference: 0.4171 s/iter. Eval: 0.0001 s/iter. Total: 0.4186 s/iter. ETA=0:02:13
[04/29 16:19:08] d2.evaluation.evaluator INFO: Inference done 193/500. Dataloading: 0.0014 s/iter. Inference: 0.4172 s/iter. Eval: 0.0001 s/iter. Total: 0.4187 s/iter. ETA=0:02:08
[04/29 16:19:13] d2.evaluation.evaluator INFO: Inference done 205/500. Dataloading: 0.0014 s/iter. Inference: 0.4173 s/iter. Eval: 0.0001 s/iter. Total: 0.4188 s/iter. ETA=0:02:03
[04/29 16:19:18] d2.evaluation.evaluator INFO: Inference done 217/500. Dataloading: 0.0014 s/iter. Inference: 0.4174 s/iter. Eval: 0.0001 s/iter. Total: 0.4189 s/iter. ETA=0:01:58
[04/29 16:19:23] d2.evaluation.evaluator INFO: Inference done 229/500. Dataloading: 0.0014 s/iter. Inference: 0.4175 s/iter. Eval: 0.0001 s/iter. Total: 0.4190 s/iter. ETA=0:01:53
[04/29 16:19:28] d2.evaluation.evaluator INFO: Inference done 242/500. Dataloading: 0.0014 s/iter. Inference: 0.4171 s/iter. Eval: 0.0001 s/iter. Total: 0.4186 s/iter. ETA=0:01:48
[04/29 16:19:33] d2.evaluation.evaluator INFO: Inference done 255/500. Dataloading: 0.0014 s/iter. Inference: 0.4157 s/iter. Eval: 0.0001 s/iter. Total: 0.4172 s/iter. ETA=0:01:42
[04/29 16:19:39] d2.evaluation.evaluator INFO: Inference done 268/500. Dataloading: 0.0014 s/iter. Inference: 0.4154 s/iter. Eval: 0.0001 s/iter. Total: 0.4170 s/iter. ETA=0:01:36
[04/29 16:19:44] d2.evaluation.evaluator INFO: Inference done 280/500. Dataloading: 0.0014 s/iter. Inference: 0.4156 s/iter. Eval: 0.0001 s/iter. Total: 0.4171 s/iter. ETA=0:01:31
[04/29 16:19:49] d2.evaluation.evaluator INFO: Inference done 292/500. Dataloading: 0.0014 s/iter. Inference: 0.4158 s/iter. Eval: 0.0001 s/iter. Total: 0.4174 s/iter. ETA=0:01:26
[04/29 16:19:54] d2.evaluation.evaluator INFO: Inference done 304/500. Dataloading: 0.0014 s/iter. Inference: 0.4160 s/iter. Eval: 0.0001 s/iter. Total: 0.4175 s/iter. ETA=0:01:21
[04/29 16:19:59] d2.evaluation.evaluator INFO: Inference done 316/500. Dataloading: 0.0014 s/iter. Inference: 0.4162 s/iter. Eval: 0.0001 s/iter. Total: 0.4177 s/iter. ETA=0:01:16
[04/29 16:20:04] d2.evaluation.evaluator INFO: Inference done 328/500. Dataloading: 0.0014 s/iter. Inference: 0.4163 s/iter. Eval: 0.0001 s/iter. Total: 0.4179 s/iter. ETA=0:01:11
[04/29 16:20:09] d2.evaluation.evaluator INFO: Inference done 340/500. Dataloading: 0.0014 s/iter. Inference: 0.4165 s/iter. Eval: 0.0001 s/iter. Total: 0.4180 s/iter. ETA=0:01:06
[04/29 16:20:14] d2.evaluation.evaluator INFO: Inference done 352/500. Dataloading: 0.0014 s/iter. Inference: 0.4167 s/iter. Eval: 0.0001 s/iter. Total: 0.4182 s/iter. ETA=0:01:01
[04/29 16:20:19] d2.evaluation.evaluator INFO: Inference done 364/500. Dataloading: 0.0014 s/iter. Inference: 0.4168 s/iter. Eval: 0.0001 s/iter. Total: 0.4184 s/iter. ETA=0:00:56
[04/29 16:20:24] d2.evaluation.evaluator INFO: Inference done 376/500. Dataloading: 0.0014 s/iter. Inference: 0.4170 s/iter. Eval: 0.0001 s/iter. Total: 0.4185 s/iter. ETA=0:00:51
[04/29 16:20:29] d2.evaluation.evaluator INFO: Inference done 388/500. Dataloading: 0.0014 s/iter. Inference: 0.4171 s/iter. Eval: 0.0001 s/iter. Total: 0.4186 s/iter. ETA=0:00:46
[04/29 16:20:34] d2.evaluation.evaluator INFO: Inference done 400/500. Dataloading: 0.0014 s/iter. Inference: 0.4172 s/iter. Eval: 0.0001 s/iter. Total: 0.4188 s/iter. ETA=0:00:41
[04/29 16:20:39] d2.evaluation.evaluator INFO: Inference done 412/500. Dataloading: 0.0014 s/iter. Inference: 0.4173 s/iter. Eval: 0.0001 s/iter. Total: 0.4189 s/iter. ETA=0:00:36
[04/29 16:20:44] d2.evaluation.evaluator INFO: Inference done 424/500. Dataloading: 0.0014 s/iter. Inference: 0.4174 s/iter. Eval: 0.0001 s/iter. Total: 0.4190 s/iter. ETA=0:00:31
[04/29 16:20:50] d2.evaluation.evaluator INFO: Inference done 436/500. Dataloading: 0.0014 s/iter. Inference: 0.4175 s/iter. Eval: 0.0001 s/iter. Total: 0.4191 s/iter. ETA=0:00:26
[04/29 16:20:55] d2.evaluation.evaluator INFO: Inference done 448/500. Dataloading: 0.0014 s/iter. Inference: 0.4176 s/iter. Eval: 0.0001 s/iter. Total: 0.4192 s/iter. ETA=0:00:21
[04/29 16:21:00] d2.evaluation.evaluator INFO: Inference done 460/500. Dataloading: 0.0014 s/iter. Inference: 0.4177 s/iter. Eval: 0.0001 s/iter. Total: 0.4193 s/iter. ETA=0:00:16
[04/29 16:21:05] d2.evaluation.evaluator INFO: Inference done 472/500. Dataloading: 0.0014 s/iter. Inference: 0.4178 s/iter. Eval: 0.0001 s/iter. Total: 0.4194 s/iter. ETA=0:00:11
[04/29 16:21:10] d2.evaluation.evaluator INFO: Inference done 484/500. Dataloading: 0.0014 s/iter. Inference: 0.4179 s/iter. Eval: 0.0001 s/iter. Total: 0.4194 s/iter. ETA=0:00:06
[04/29 16:21:15] d2.evaluation.evaluator INFO: Inference done 496/500. Dataloading: 0.0014 s/iter. Inference: 0.4179 s/iter. Eval: 0.0001 s/iter. Total: 0.4195 s/iter. ETA=0:00:01
[04/29 16:21:17] d2.evaluation.evaluator INFO: Total inference time: 0:03:27.691448 (0.419579 s / iter per device, on 1 devices)
[04/29 16:21:17] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:03:26 (0.417967 s / iter per device, on 1 devices)
[04/29 16:21:17] adet.evaluation.text_evaluation_all INFO: Saving results to output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/inference/text_results.json
[04/29 16:26:35] detectron2 INFO: Rank of current process: 0. World size: 1
[04/29 16:26:35] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/kylee/DeepSolo_LangPriors/detectron2/detectron2
Compiler                         GCC 9.5
CUDA compiler                    CUDA 11.3
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.12.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 3060 (arch=8.6)
Driver version                   550.120
CUDA_HOME                        /usr
Pillow                           9.0.1
torchvision                      0.13.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[04/29 16:26:35] detectron2 INFO: Command line arguments: Namespace(config_file='configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50152', opts=['MODEL.WEIGHTS', 'output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth'])
[04/29 16:26:35] detectron2 INFO: Contents of args.config_file=configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml:
_BASE_: "../Base_det.yaml"

MODEL:
  WEIGHTS: "output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth"
  TRANSFORMER:
    INFERENCE_TH_TEST: 0.01

DATASETS:
  TRAIN: ("ic15_train",)
  TEST: ("ic15_test",)

INPUT:
  MIN_SIZE_TRAIN: (640, 700, 760, 820, 880, 940, 1000)
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MAX_SIZE_TEST: 4000
  CROP:
    ENABLED: False
  ROTATE: False

SOLVER:
  AMP:
    ENABLED: True  # Enable Automatic Mixed Precision
  IMS_PER_BATCH: 2
  BASE_LR: 1e-5
  LR_BACKBONE: 1e-6
  WARMUP_ITERS: 250
  STEPS: (15000, 25000)  # no step
  MAX_ITER: 30000
  GAMMA: 0.1
  CHECKPOINT_PERIOD: 1000

TEST:
  EVAL_PERIOD: 1000
  # 1 - Generic, 2 - Weak, 3 - Strong (for icdar2015)
  LEXICON_TYPE: 3

OUTPUT_DIR: "output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15"

[04/29 16:26:35] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ic15_test
  TRAIN:
  - ic15_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    CROP_INSTANCE: false
    ENABLED: false
    SIZE:
    - 0.1
    - 0.1
    TYPE: relative_range
  FORMAT: RGB
  HFLIP_TRAIN: false
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 4000
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MIN_SIZE_TRAIN:
  - 640
  - 700
  - 760
  - 820
  - 880
  - 940
  - 1000
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  ROTATE: false
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    ANTI_ALIAS: false
    FREEZE_AT: 2
    NAME: build_resnet_backbone
  BASIS_MODULE:
    ANN_SET: coco
    COMMON_STRIDE: 8
    CONVS_DIM: 128
    IN_FEATURES:
    - p3
    - p4
    - p5
    LOSS_ON: false
    LOSS_WEIGHT: 0.3
    NAME: ProtoNet
    NORM: SyncBN
    NUM_BASES: 4
    NUM_CLASSES: 80
    NUM_CONVS: 3
  BATEXT:
    CANONICAL_SIZE: 96
    CONV_DIM: 256
    CUSTOM_DICT: ''
    IN_FEATURES:
    - p2
    - p3
    - p4
    NUM_CHARS: 25
    NUM_CONV: 2
    POOLER_RESOLUTION:
    - 8
    - 32
    POOLER_SCALES:
    - 0.25
    - 0.125
    - 0.0625
    RECOGNITION_LOSS: ctc
    RECOGNIZER: attn
    SAMPLING_RATIO: 1
    USE_AET: false
    USE_COORDCONV: false
    VOC_SIZE: 96
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: false
  META_ARCHITECTURE: TransformerPureDetector
  MOBILENET: false
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_INTERVAL: 1
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  SWIN:
    DROP_PATH_RATE: 0.2
    TYPE: tiny
  TOP_MODULE:
    DIM: 16
    NAME: conv
  TRANSFORMER:
    AUX_LOSS: true
    BOUNDARY_HEAD: true
    CUSTOM_DICT: ''
    DEC_LAYERS: 6
    DEC_N_POINTS: 4
    DIM_FEEDFORWARD: 1024
    DROPOUT: 0.0
    ENABLED: true
    ENC_LAYERS: 6
    ENC_N_POINTS: 4
    HIDDEN_DIM: 256
    INFERENCE_TH_TEST: 0.01
    LOSS:
      AUX_LOSS: true
      BEZIER_CLASS_WEIGHT: 1.0
      BEZIER_COORD_WEIGHT: 1.0
      BEZIER_SAMPLE_POINTS: 25
      BOUNDARY_WEIGHT: 0.5
      FOCAL_ALPHA: 0.25
      FOCAL_GAMMA: 2.0
      POINT_CLASS_WEIGHT: 1.0
      POINT_COORD_WEIGHT: 1.0
      POINT_TEXT_WEIGHT: 0.5
    NHEADS: 8
    NUM_FEATURE_LEVELS: 4
    NUM_POINTS: 25
    NUM_QUERIES: 100
    POSITION_EMBEDDING_SCALE: 6.283185307179586
    TEMPERATURE: 10000
    VOC_SIZE: 37
  ViTAEv2:
    DROP_PATH_RATE: 0.2
    TYPE: vitaev2_s
  WEIGHTS: output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth
OUTPUT_DIR: output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15
SEED: 42
SOLVER:
  AMP:
    ENABLED: true
  BASE_LR: 1.0e-05
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.1
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 2
  LR_BACKBONE: 1.0e-06
  LR_BACKBONE_NAMES:
  - backbone.0
  LR_LINEAR_PROJ_MULT: 1.0
  LR_LINEAR_PROJ_NAMES:
  - reference_points
  - sampling_offsets
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 30000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 15000
  - 25000
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 250
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  LEXICON_TYPE: 3
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/29 16:26:35] detectron2 INFO: Full config saved to output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/config.yaml
[04/29 16:26:37] d2.engine.defaults INFO: Model:
TransformerPureDetector(
  (detection_transformer): DETECTION_TRANSFORMER(
    (backbone): Joiner(
      (0): MaskedBackbone(
        (backbone): ResNet(
          (stem): BasicStem(
            (conv1): Conv2d(
              3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
          )
          (res2): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv1): Conv2d(
                64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
          )
          (res3): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv1): Conv2d(
                256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
          )
          (res4): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
              (conv1): Conv2d(
                512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (4): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (5): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
          )
          (res5): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
              (conv1): Conv2d(
                1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
          )
        )
      )
      (1): PositionalEncoding2D()
    )
    (point_embed): Embedding(2500, 256)
    (transformer): DeformableTransformer(
      (encoder): DeformableTransformerEncoder(
        (layers): ModuleList(
          (0): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (decoder): DeformableCompositeTransformerDecoder(
        (layers): ModuleList(
          (0): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ctrl_point_coord): ModuleList(
          (0): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (1): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (2): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (3): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (4): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (5): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
        )
      )
      (enc_output): Linear(in_features=256, out_features=256, bias=True)
      (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (bezier_coord_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=8, bias=True)
        )
      )
      (bezier_class_embed): Linear(in_features=256, out_features=1, bias=True)
    )
    (input_proj): ModuleList(
      (0): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (1): Sequential(
        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (2): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (3): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (bezier_proposal_coord): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=8, bias=True)
      )
    )
    (bezier_proposal_class): Linear(in_features=256, out_features=1, bias=True)
    (ctrl_point_coord): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
    )
    (ctrl_point_class): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (ctrl_point_text): ModuleList(
      (0): Linear(in_features=256, out_features=38, bias=True)
      (1): Linear(in_features=256, out_features=38, bias=True)
      (2): Linear(in_features=256, out_features=38, bias=True)
      (3): Linear(in_features=256, out_features=38, bias=True)
      (4): Linear(in_features=256, out_features=38, bias=True)
      (5): Linear(in_features=256, out_features=38, bias=True)
    )
    (boundary_offset): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (enc_matcher): BezierHungarianMatcher(
      (bezier_sampler): BezierSampler()
    )
    (dec_matcher): CtrlPointHungarianMatcher()
    (bezier_sampler): BezierSampler()
  )
)
[04/29 16:26:37] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth ...
[04/29 16:26:37] fvcore.common.checkpoint INFO: [Checkpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth ...
[04/29 16:26:37] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1440, 1440), max_size=4000, sample_style='choice')]
[04/29 16:26:37] adet.data.dataset_mapper INFO: Rebuilding the augmentations. The previous augmentations will be overridden.
[04/29 16:26:37] adet.data.datasets.text INFO: Loaded 500 images in COCO format from datasets/ic15/test.json
[04/29 16:26:37] d2.data.build INFO: Distribution of instances among all 1 categories:
[36m|  category  | #instances   |
|:----------:|:-------------|
|    text    | 0            |
|            |              |[0m
[04/29 16:26:37] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[04/29 16:26:37] d2.data.common INFO: Serializing 500 elements to byte tensors and concatenating them all ...
[04/29 16:26:37] d2.data.common INFO: Serialized dataset takes 0.06 MiB
[04/29 16:26:37] d2.evaluation.evaluator INFO: Start inference on 500 batches
[04/29 16:26:43] d2.evaluation.evaluator INFO: Inference done 11/500. Dataloading: 0.0011 s/iter. Inference: 0.3863 s/iter. Eval: 0.0031 s/iter. Total: 0.3905 s/iter. ETA=0:03:10
[04/29 16:26:48] d2.evaluation.evaluator INFO: Inference done 24/500. Dataloading: 0.0012 s/iter. Inference: 0.3866 s/iter. Eval: 0.0022 s/iter. Total: 0.3900 s/iter. ETA=0:03:05
[04/29 16:26:53] d2.evaluation.evaluator INFO: Inference done 37/500. Dataloading: 0.0013 s/iter. Inference: 0.3880 s/iter. Eval: 0.0020 s/iter. Total: 0.3914 s/iter. ETA=0:03:01
[04/29 16:26:58] d2.evaluation.evaluator INFO: Inference done 50/500. Dataloading: 0.0014 s/iter. Inference: 0.3895 s/iter. Eval: 0.0021 s/iter. Total: 0.3931 s/iter. ETA=0:02:56
[04/29 16:27:03] d2.evaluation.evaluator INFO: Inference done 63/500. Dataloading: 0.0014 s/iter. Inference: 0.3895 s/iter. Eval: 0.0019 s/iter. Total: 0.3928 s/iter. ETA=0:02:51
[04/29 16:27:08] d2.evaluation.evaluator INFO: Inference done 76/500. Dataloading: 0.0014 s/iter. Inference: 0.3917 s/iter. Eval: 0.0018 s/iter. Total: 0.3949 s/iter. ETA=0:02:47
[04/29 16:27:13] d2.evaluation.evaluator INFO: Inference done 89/500. Dataloading: 0.0014 s/iter. Inference: 0.3928 s/iter. Eval: 0.0018 s/iter. Total: 0.3960 s/iter. ETA=0:02:42
[04/29 16:27:19] d2.evaluation.evaluator INFO: Inference done 102/500. Dataloading: 0.0014 s/iter. Inference: 0.3933 s/iter. Eval: 0.0018 s/iter. Total: 0.3965 s/iter. ETA=0:02:37
[04/29 16:27:24] d2.evaluation.evaluator INFO: Inference done 115/500. Dataloading: 0.0014 s/iter. Inference: 0.3944 s/iter. Eval: 0.0018 s/iter. Total: 0.3976 s/iter. ETA=0:02:33
[04/29 16:27:29] d2.evaluation.evaluator INFO: Inference done 128/500. Dataloading: 0.0014 s/iter. Inference: 0.3942 s/iter. Eval: 0.0018 s/iter. Total: 0.3974 s/iter. ETA=0:02:27
[04/29 16:27:34] d2.evaluation.evaluator INFO: Inference done 141/500. Dataloading: 0.0014 s/iter. Inference: 0.3949 s/iter. Eval: 0.0018 s/iter. Total: 0.3980 s/iter. ETA=0:02:22
[04/29 16:27:40] d2.evaluation.evaluator INFO: Inference done 154/500. Dataloading: 0.0014 s/iter. Inference: 0.3949 s/iter. Eval: 0.0019 s/iter. Total: 0.3982 s/iter. ETA=0:02:17
[04/29 16:27:45] d2.evaluation.evaluator INFO: Inference done 167/500. Dataloading: 0.0014 s/iter. Inference: 0.3945 s/iter. Eval: 0.0019 s/iter. Total: 0.3978 s/iter. ETA=0:02:12
[04/29 16:27:50] d2.evaluation.evaluator INFO: Inference done 180/500. Dataloading: 0.0014 s/iter. Inference: 0.3942 s/iter. Eval: 0.0020 s/iter. Total: 0.3976 s/iter. ETA=0:02:07
[04/29 16:27:55] d2.evaluation.evaluator INFO: Inference done 192/500. Dataloading: 0.0014 s/iter. Inference: 0.3954 s/iter. Eval: 0.0020 s/iter. Total: 0.3989 s/iter. ETA=0:02:02
[04/29 16:28:00] d2.evaluation.evaluator INFO: Inference done 204/500. Dataloading: 0.0014 s/iter. Inference: 0.3969 s/iter. Eval: 0.0020 s/iter. Total: 0.4004 s/iter. ETA=0:01:58
[04/29 16:28:05] d2.evaluation.evaluator INFO: Inference done 216/500. Dataloading: 0.0014 s/iter. Inference: 0.3988 s/iter. Eval: 0.0020 s/iter. Total: 0.4022 s/iter. ETA=0:01:54
[04/29 16:28:10] d2.evaluation.evaluator INFO: Inference done 228/500. Dataloading: 0.0014 s/iter. Inference: 0.4011 s/iter. Eval: 0.0020 s/iter. Total: 0.4045 s/iter. ETA=0:01:50
[04/29 16:28:16] d2.evaluation.evaluator INFO: Inference done 240/500. Dataloading: 0.0014 s/iter. Inference: 0.4031 s/iter. Eval: 0.0020 s/iter. Total: 0.4066 s/iter. ETA=0:01:45
[04/29 16:28:21] d2.evaluation.evaluator INFO: Inference done 252/500. Dataloading: 0.0014 s/iter. Inference: 0.4052 s/iter. Eval: 0.0020 s/iter. Total: 0.4086 s/iter. ETA=0:01:41
[04/29 16:28:27] d2.evaluation.evaluator INFO: Inference done 264/500. Dataloading: 0.0014 s/iter. Inference: 0.4070 s/iter. Eval: 0.0020 s/iter. Total: 0.4104 s/iter. ETA=0:01:36
[04/29 16:28:32] d2.evaluation.evaluator INFO: Inference done 276/500. Dataloading: 0.0014 s/iter. Inference: 0.4081 s/iter. Eval: 0.0020 s/iter. Total: 0.4116 s/iter. ETA=0:01:32
[04/29 16:28:37] d2.evaluation.evaluator INFO: Inference done 288/500. Dataloading: 0.0014 s/iter. Inference: 0.4090 s/iter. Eval: 0.0020 s/iter. Total: 0.4125 s/iter. ETA=0:01:27
[04/29 16:28:42] d2.evaluation.evaluator INFO: Inference done 300/500. Dataloading: 0.0014 s/iter. Inference: 0.4095 s/iter. Eval: 0.0020 s/iter. Total: 0.4129 s/iter. ETA=0:01:22
[04/29 16:28:47] d2.evaluation.evaluator INFO: Inference done 312/500. Dataloading: 0.0014 s/iter. Inference: 0.4099 s/iter. Eval: 0.0020 s/iter. Total: 0.4133 s/iter. ETA=0:01:17
[04/29 16:28:52] d2.evaluation.evaluator INFO: Inference done 324/500. Dataloading: 0.0014 s/iter. Inference: 0.4102 s/iter. Eval: 0.0020 s/iter. Total: 0.4136 s/iter. ETA=0:01:12
[04/29 16:28:57] d2.evaluation.evaluator INFO: Inference done 336/500. Dataloading: 0.0014 s/iter. Inference: 0.4106 s/iter. Eval: 0.0020 s/iter. Total: 0.4140 s/iter. ETA=0:01:07
[04/29 16:29:02] d2.evaluation.evaluator INFO: Inference done 348/500. Dataloading: 0.0014 s/iter. Inference: 0.4109 s/iter. Eval: 0.0021 s/iter. Total: 0.4145 s/iter. ETA=0:01:02
[04/29 16:29:07] d2.evaluation.evaluator INFO: Inference done 360/500. Dataloading: 0.0014 s/iter. Inference: 0.4112 s/iter. Eval: 0.0021 s/iter. Total: 0.4148 s/iter. ETA=0:00:58
[04/29 16:29:13] d2.evaluation.evaluator INFO: Inference done 372/500. Dataloading: 0.0014 s/iter. Inference: 0.4115 s/iter. Eval: 0.0022 s/iter. Total: 0.4151 s/iter. ETA=0:00:53
[04/29 16:29:18] d2.evaluation.evaluator INFO: Inference done 384/500. Dataloading: 0.0014 s/iter. Inference: 0.4118 s/iter. Eval: 0.0022 s/iter. Total: 0.4154 s/iter. ETA=0:00:48
[04/29 16:29:23] d2.evaluation.evaluator INFO: Inference done 396/500. Dataloading: 0.0014 s/iter. Inference: 0.4121 s/iter. Eval: 0.0022 s/iter. Total: 0.4157 s/iter. ETA=0:00:43
[04/29 16:29:28] d2.evaluation.evaluator INFO: Inference done 408/500. Dataloading: 0.0014 s/iter. Inference: 0.4123 s/iter. Eval: 0.0022 s/iter. Total: 0.4159 s/iter. ETA=0:00:38
[04/29 16:29:33] d2.evaluation.evaluator INFO: Inference done 420/500. Dataloading: 0.0014 s/iter. Inference: 0.4125 s/iter. Eval: 0.0022 s/iter. Total: 0.4161 s/iter. ETA=0:00:33
[04/29 16:29:38] d2.evaluation.evaluator INFO: Inference done 432/500. Dataloading: 0.0014 s/iter. Inference: 0.4127 s/iter. Eval: 0.0022 s/iter. Total: 0.4164 s/iter. ETA=0:00:28
[04/29 16:29:43] d2.evaluation.evaluator INFO: Inference done 444/500. Dataloading: 0.0014 s/iter. Inference: 0.4130 s/iter. Eval: 0.0022 s/iter. Total: 0.4166 s/iter. ETA=0:00:23
[04/29 16:29:48] d2.evaluation.evaluator INFO: Inference done 456/500. Dataloading: 0.0014 s/iter. Inference: 0.4131 s/iter. Eval: 0.0021 s/iter. Total: 0.4167 s/iter. ETA=0:00:18
[04/29 16:29:53] d2.evaluation.evaluator INFO: Inference done 468/500. Dataloading: 0.0014 s/iter. Inference: 0.4133 s/iter. Eval: 0.0021 s/iter. Total: 0.4169 s/iter. ETA=0:00:13
[04/29 16:29:58] d2.evaluation.evaluator INFO: Inference done 480/500. Dataloading: 0.0014 s/iter. Inference: 0.4135 s/iter. Eval: 0.0021 s/iter. Total: 0.4171 s/iter. ETA=0:00:08
[04/29 16:30:03] d2.evaluation.evaluator INFO: Inference done 492/500. Dataloading: 0.0014 s/iter. Inference: 0.4137 s/iter. Eval: 0.0021 s/iter. Total: 0.4172 s/iter. ETA=0:00:03
[04/29 16:30:07] d2.evaluation.evaluator INFO: Total inference time: 0:03:26.591644 (0.417357 s / iter per device, on 1 devices)
[04/29 16:30:07] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:03:24 (0.413746 s / iter per device, on 1 devices)
[04/29 16:30:07] adet.evaluation.text_evaluation_all INFO: Saving results to output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/inference/text_results.json
[04/29 16:36:34] detectron2 INFO: Rank of current process: 0. World size: 1
[04/29 16:36:34] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/kylee/DeepSolo_LangPriors/detectron2/detectron2
Compiler                         GCC 9.5
CUDA compiler                    CUDA 11.3
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.12.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 3060 (arch=8.6)
Driver version                   550.120
CUDA_HOME                        /usr
Pillow                           9.0.1
torchvision                      0.13.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[04/29 16:36:34] detectron2 INFO: Command line arguments: Namespace(config_file='configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50152', opts=['MODEL.WEIGHTS', 'output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth'])
[04/29 16:36:34] detectron2 INFO: Contents of args.config_file=configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml:
_BASE_: "../Base_det.yaml"

MODEL:
  WEIGHTS: "output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth"
  TRANSFORMER:
    INFERENCE_TH_TEST: 0.1

DATASETS:
  TRAIN: ("ic15_train",)
  TEST: ("ic15_test",)

INPUT:
  MIN_SIZE_TRAIN: (640, 700, 760, 820, 880, 940, 1000)
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MAX_SIZE_TEST: 4000
  CROP:
    ENABLED: False
  ROTATE: False

SOLVER:
  AMP:
    ENABLED: True  # Enable Automatic Mixed Precision
  IMS_PER_BATCH: 2
  BASE_LR: 1e-5
  LR_BACKBONE: 1e-6
  WARMUP_ITERS: 250
  STEPS: (15000, 25000)  # no step
  MAX_ITER: 30000
  GAMMA: 0.1
  CHECKPOINT_PERIOD: 1000

TEST:
  EVAL_PERIOD: 1000
  # 1 - Generic, 2 - Weak, 3 - Strong (for icdar2015)
  LEXICON_TYPE: 3

OUTPUT_DIR: "output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15"

[04/29 16:36:34] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ic15_test
  TRAIN:
  - ic15_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    CROP_INSTANCE: false
    ENABLED: false
    SIZE:
    - 0.1
    - 0.1
    TYPE: relative_range
  FORMAT: RGB
  HFLIP_TRAIN: false
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 4000
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MIN_SIZE_TRAIN:
  - 640
  - 700
  - 760
  - 820
  - 880
  - 940
  - 1000
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  ROTATE: false
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    ANTI_ALIAS: false
    FREEZE_AT: 2
    NAME: build_resnet_backbone
  BASIS_MODULE:
    ANN_SET: coco
    COMMON_STRIDE: 8
    CONVS_DIM: 128
    IN_FEATURES:
    - p3
    - p4
    - p5
    LOSS_ON: false
    LOSS_WEIGHT: 0.3
    NAME: ProtoNet
    NORM: SyncBN
    NUM_BASES: 4
    NUM_CLASSES: 80
    NUM_CONVS: 3
  BATEXT:
    CANONICAL_SIZE: 96
    CONV_DIM: 256
    CUSTOM_DICT: ''
    IN_FEATURES:
    - p2
    - p3
    - p4
    NUM_CHARS: 25
    NUM_CONV: 2
    POOLER_RESOLUTION:
    - 8
    - 32
    POOLER_SCALES:
    - 0.25
    - 0.125
    - 0.0625
    RECOGNITION_LOSS: ctc
    RECOGNIZER: attn
    SAMPLING_RATIO: 1
    USE_AET: false
    USE_COORDCONV: false
    VOC_SIZE: 96
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: false
  META_ARCHITECTURE: TransformerPureDetector
  MOBILENET: false
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_INTERVAL: 1
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  SWIN:
    DROP_PATH_RATE: 0.2
    TYPE: tiny
  TOP_MODULE:
    DIM: 16
    NAME: conv
  TRANSFORMER:
    AUX_LOSS: true
    BOUNDARY_HEAD: true
    CUSTOM_DICT: ''
    DEC_LAYERS: 6
    DEC_N_POINTS: 4
    DIM_FEEDFORWARD: 1024
    DROPOUT: 0.0
    ENABLED: true
    ENC_LAYERS: 6
    ENC_N_POINTS: 4
    HIDDEN_DIM: 256
    INFERENCE_TH_TEST: 0.1
    LOSS:
      AUX_LOSS: true
      BEZIER_CLASS_WEIGHT: 1.0
      BEZIER_COORD_WEIGHT: 1.0
      BEZIER_SAMPLE_POINTS: 25
      BOUNDARY_WEIGHT: 0.5
      FOCAL_ALPHA: 0.25
      FOCAL_GAMMA: 2.0
      POINT_CLASS_WEIGHT: 1.0
      POINT_COORD_WEIGHT: 1.0
      POINT_TEXT_WEIGHT: 0.5
    NHEADS: 8
    NUM_FEATURE_LEVELS: 4
    NUM_POINTS: 25
    NUM_QUERIES: 100
    POSITION_EMBEDDING_SCALE: 6.283185307179586
    TEMPERATURE: 10000
    VOC_SIZE: 37
  ViTAEv2:
    DROP_PATH_RATE: 0.2
    TYPE: vitaev2_s
  WEIGHTS: output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth
OUTPUT_DIR: output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15
SEED: 42
SOLVER:
  AMP:
    ENABLED: true
  BASE_LR: 1.0e-05
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.1
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 2
  LR_BACKBONE: 1.0e-06
  LR_BACKBONE_NAMES:
  - backbone.0
  LR_LINEAR_PROJ_MULT: 1.0
  LR_LINEAR_PROJ_NAMES:
  - reference_points
  - sampling_offsets
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 30000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 15000
  - 25000
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 250
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  LEXICON_TYPE: 3
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/29 16:36:34] detectron2 INFO: Full config saved to output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/config.yaml
[04/29 16:36:36] d2.engine.defaults INFO: Model:
TransformerPureDetector(
  (detection_transformer): DETECTION_TRANSFORMER(
    (backbone): Joiner(
      (0): MaskedBackbone(
        (backbone): ResNet(
          (stem): BasicStem(
            (conv1): Conv2d(
              3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
          )
          (res2): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv1): Conv2d(
                64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
          )
          (res3): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv1): Conv2d(
                256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
          )
          (res4): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
              (conv1): Conv2d(
                512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (4): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (5): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
          )
          (res5): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
              (conv1): Conv2d(
                1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
          )
        )
      )
      (1): PositionalEncoding2D()
    )
    (point_embed): Embedding(2500, 256)
    (transformer): DeformableTransformer(
      (encoder): DeformableTransformerEncoder(
        (layers): ModuleList(
          (0): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (decoder): DeformableCompositeTransformerDecoder(
        (layers): ModuleList(
          (0): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ctrl_point_coord): ModuleList(
          (0): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (1): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (2): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (3): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (4): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (5): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
        )
      )
      (enc_output): Linear(in_features=256, out_features=256, bias=True)
      (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (bezier_coord_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=8, bias=True)
        )
      )
      (bezier_class_embed): Linear(in_features=256, out_features=1, bias=True)
    )
    (input_proj): ModuleList(
      (0): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (1): Sequential(
        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (2): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (3): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (bezier_proposal_coord): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=8, bias=True)
      )
    )
    (bezier_proposal_class): Linear(in_features=256, out_features=1, bias=True)
    (ctrl_point_coord): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
    )
    (ctrl_point_class): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (ctrl_point_text): ModuleList(
      (0): Linear(in_features=256, out_features=38, bias=True)
      (1): Linear(in_features=256, out_features=38, bias=True)
      (2): Linear(in_features=256, out_features=38, bias=True)
      (3): Linear(in_features=256, out_features=38, bias=True)
      (4): Linear(in_features=256, out_features=38, bias=True)
      (5): Linear(in_features=256, out_features=38, bias=True)
    )
    (boundary_offset): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (enc_matcher): BezierHungarianMatcher(
      (bezier_sampler): BezierSampler()
    )
    (dec_matcher): CtrlPointHungarianMatcher()
    (bezier_sampler): BezierSampler()
  )
)
[04/29 16:36:36] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth ...
[04/29 16:36:36] fvcore.common.checkpoint INFO: [Checkpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth ...
[04/29 16:36:36] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1440, 1440), max_size=4000, sample_style='choice')]
[04/29 16:36:36] adet.data.dataset_mapper INFO: Rebuilding the augmentations. The previous augmentations will be overridden.
[04/29 16:36:36] adet.data.datasets.text INFO: Loaded 500 images in COCO format from datasets/ic15/test.json
[04/29 16:36:36] d2.data.build INFO: Distribution of instances among all 1 categories:
[36m|  category  | #instances   |
|:----------:|:-------------|
|    text    | 0            |
|            |              |[0m
[04/29 16:36:36] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[04/29 16:36:36] d2.data.common INFO: Serializing 500 elements to byte tensors and concatenating them all ...
[04/29 16:36:36] d2.data.common INFO: Serialized dataset takes 0.06 MiB
[04/29 16:36:36] d2.evaluation.evaluator INFO: Start inference on 500 batches
[04/29 16:36:42] d2.evaluation.evaluator INFO: Inference done 11/500. Dataloading: 0.0010 s/iter. Inference: 0.3841 s/iter. Eval: 0.0001 s/iter. Total: 0.3851 s/iter. ETA=0:03:08
[04/29 16:37:08] detectron2 INFO: Rank of current process: 0. World size: 1
[04/29 16:37:08] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/kylee/DeepSolo_LangPriors/detectron2/detectron2
Compiler                         GCC 9.5
CUDA compiler                    CUDA 11.3
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.12.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 3060 (arch=8.6)
Driver version                   550.120
CUDA_HOME                        /usr
Pillow                           9.0.1
torchvision                      0.13.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[04/29 16:37:08] detectron2 INFO: Command line arguments: Namespace(config_file='configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50152', opts=['MODEL.WEIGHTS', 'output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth'])
[04/29 16:37:08] detectron2 INFO: Contents of args.config_file=configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml:
_BASE_: "../Base_det.yaml"

MODEL:
  WEIGHTS: "output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth"
  TRANSFORMER:
    INFERENCE_TH_TEST: 0.05

DATASETS:
  TRAIN: ("ic15_train",)
  TEST: ("ic15_test",)

INPUT:
  MIN_SIZE_TRAIN: (640, 700, 760, 820, 880, 940, 1000)
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MAX_SIZE_TEST: 4000
  CROP:
    ENABLED: False
  ROTATE: False

SOLVER:
  AMP:
    ENABLED: True  # Enable Automatic Mixed Precision
  IMS_PER_BATCH: 2
  BASE_LR: 1e-5
  LR_BACKBONE: 1e-6
  WARMUP_ITERS: 250
  STEPS: (15000, 25000)  # no step
  MAX_ITER: 30000
  GAMMA: 0.1
  CHECKPOINT_PERIOD: 1000

TEST:
  EVAL_PERIOD: 1000
  # 1 - Generic, 2 - Weak, 3 - Strong (for icdar2015)
  LEXICON_TYPE: 3

OUTPUT_DIR: "output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15"

[04/29 16:37:08] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ic15_test
  TRAIN:
  - ic15_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    CROP_INSTANCE: false
    ENABLED: false
    SIZE:
    - 0.1
    - 0.1
    TYPE: relative_range
  FORMAT: RGB
  HFLIP_TRAIN: false
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 4000
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MIN_SIZE_TRAIN:
  - 640
  - 700
  - 760
  - 820
  - 880
  - 940
  - 1000
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  ROTATE: false
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    ANTI_ALIAS: false
    FREEZE_AT: 2
    NAME: build_resnet_backbone
  BASIS_MODULE:
    ANN_SET: coco
    COMMON_STRIDE: 8
    CONVS_DIM: 128
    IN_FEATURES:
    - p3
    - p4
    - p5
    LOSS_ON: false
    LOSS_WEIGHT: 0.3
    NAME: ProtoNet
    NORM: SyncBN
    NUM_BASES: 4
    NUM_CLASSES: 80
    NUM_CONVS: 3
  BATEXT:
    CANONICAL_SIZE: 96
    CONV_DIM: 256
    CUSTOM_DICT: ''
    IN_FEATURES:
    - p2
    - p3
    - p4
    NUM_CHARS: 25
    NUM_CONV: 2
    POOLER_RESOLUTION:
    - 8
    - 32
    POOLER_SCALES:
    - 0.25
    - 0.125
    - 0.0625
    RECOGNITION_LOSS: ctc
    RECOGNIZER: attn
    SAMPLING_RATIO: 1
    USE_AET: false
    USE_COORDCONV: false
    VOC_SIZE: 96
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: false
  META_ARCHITECTURE: TransformerPureDetector
  MOBILENET: false
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_INTERVAL: 1
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  SWIN:
    DROP_PATH_RATE: 0.2
    TYPE: tiny
  TOP_MODULE:
    DIM: 16
    NAME: conv
  TRANSFORMER:
    AUX_LOSS: true
    BOUNDARY_HEAD: true
    CUSTOM_DICT: ''
    DEC_LAYERS: 6
    DEC_N_POINTS: 4
    DIM_FEEDFORWARD: 1024
    DROPOUT: 0.0
    ENABLED: true
    ENC_LAYERS: 6
    ENC_N_POINTS: 4
    HIDDEN_DIM: 256
    INFERENCE_TH_TEST: 0.05
    LOSS:
      AUX_LOSS: true
      BEZIER_CLASS_WEIGHT: 1.0
      BEZIER_COORD_WEIGHT: 1.0
      BEZIER_SAMPLE_POINTS: 25
      BOUNDARY_WEIGHT: 0.5
      FOCAL_ALPHA: 0.25
      FOCAL_GAMMA: 2.0
      POINT_CLASS_WEIGHT: 1.0
      POINT_COORD_WEIGHT: 1.0
      POINT_TEXT_WEIGHT: 0.5
    NHEADS: 8
    NUM_FEATURE_LEVELS: 4
    NUM_POINTS: 25
    NUM_QUERIES: 100
    POSITION_EMBEDDING_SCALE: 6.283185307179586
    TEMPERATURE: 10000
    VOC_SIZE: 37
  ViTAEv2:
    DROP_PATH_RATE: 0.2
    TYPE: vitaev2_s
  WEIGHTS: output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth
OUTPUT_DIR: output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15
SEED: 42
SOLVER:
  AMP:
    ENABLED: true
  BASE_LR: 1.0e-05
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.1
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 2
  LR_BACKBONE: 1.0e-06
  LR_BACKBONE_NAMES:
  - backbone.0
  LR_LINEAR_PROJ_MULT: 1.0
  LR_LINEAR_PROJ_NAMES:
  - reference_points
  - sampling_offsets
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 30000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 15000
  - 25000
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 250
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  LEXICON_TYPE: 3
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/29 16:37:08] detectron2 INFO: Full config saved to output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/config.yaml
[04/29 16:37:10] d2.engine.defaults INFO: Model:
TransformerPureDetector(
  (detection_transformer): DETECTION_TRANSFORMER(
    (backbone): Joiner(
      (0): MaskedBackbone(
        (backbone): ResNet(
          (stem): BasicStem(
            (conv1): Conv2d(
              3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
          )
          (res2): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv1): Conv2d(
                64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
          )
          (res3): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv1): Conv2d(
                256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
          )
          (res4): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
              (conv1): Conv2d(
                512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (4): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (5): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
          )
          (res5): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
              (conv1): Conv2d(
                1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
          )
        )
      )
      (1): PositionalEncoding2D()
    )
    (point_embed): Embedding(2500, 256)
    (transformer): DeformableTransformer(
      (encoder): DeformableTransformerEncoder(
        (layers): ModuleList(
          (0): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (decoder): DeformableCompositeTransformerDecoder(
        (layers): ModuleList(
          (0): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ctrl_point_coord): ModuleList(
          (0): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (1): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (2): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (3): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (4): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (5): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
        )
      )
      (enc_output): Linear(in_features=256, out_features=256, bias=True)
      (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (bezier_coord_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=8, bias=True)
        )
      )
      (bezier_class_embed): Linear(in_features=256, out_features=1, bias=True)
    )
    (input_proj): ModuleList(
      (0): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (1): Sequential(
        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (2): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (3): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (bezier_proposal_coord): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=8, bias=True)
      )
    )
    (bezier_proposal_class): Linear(in_features=256, out_features=1, bias=True)
    (ctrl_point_coord): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
    )
    (ctrl_point_class): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (ctrl_point_text): ModuleList(
      (0): Linear(in_features=256, out_features=38, bias=True)
      (1): Linear(in_features=256, out_features=38, bias=True)
      (2): Linear(in_features=256, out_features=38, bias=True)
      (3): Linear(in_features=256, out_features=38, bias=True)
      (4): Linear(in_features=256, out_features=38, bias=True)
      (5): Linear(in_features=256, out_features=38, bias=True)
    )
    (boundary_offset): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (enc_matcher): BezierHungarianMatcher(
      (bezier_sampler): BezierSampler()
    )
    (dec_matcher): CtrlPointHungarianMatcher()
    (bezier_sampler): BezierSampler()
  )
)
[04/29 16:37:10] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth ...
[04/29 16:37:10] fvcore.common.checkpoint INFO: [Checkpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth ...
[04/29 16:37:10] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1440, 1440), max_size=4000, sample_style='choice')]
[04/29 16:37:10] adet.data.dataset_mapper INFO: Rebuilding the augmentations. The previous augmentations will be overridden.
[04/29 16:37:10] adet.data.datasets.text INFO: Loaded 500 images in COCO format from datasets/ic15/test.json
[04/29 16:37:10] d2.data.build INFO: Distribution of instances among all 1 categories:
[36m|  category  | #instances   |
|:----------:|:-------------|
|    text    | 0            |
|            |              |[0m
[04/29 16:37:10] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[04/29 16:37:10] d2.data.common INFO: Serializing 500 elements to byte tensors and concatenating them all ...
[04/29 16:37:10] d2.data.common INFO: Serialized dataset takes 0.06 MiB
[04/29 16:37:10] d2.evaluation.evaluator INFO: Start inference on 500 batches
[04/29 16:37:29] detectron2 INFO: Rank of current process: 0. World size: 1
[04/29 16:37:30] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/kylee/DeepSolo_LangPriors/detectron2/detectron2
Compiler                         GCC 9.5
CUDA compiler                    CUDA 11.3
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.12.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 3060 (arch=8.6)
Driver version                   550.120
CUDA_HOME                        /usr
Pillow                           9.0.1
torchvision                      0.13.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[04/29 16:37:30] detectron2 INFO: Command line arguments: Namespace(config_file='configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50152', opts=['MODEL.WEIGHTS', 'output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth'])
[04/29 16:37:30] detectron2 INFO: Contents of args.config_file=configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml:
_BASE_: "../Base_det.yaml"

MODEL:
  WEIGHTS: "output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth"
  TRANSFORMER:
    INFERENCE_TH_TEST: 0.03

DATASETS:
  TRAIN: ("ic15_train",)
  TEST: ("ic15_test",)

INPUT:
  MIN_SIZE_TRAIN: (640, 700, 760, 820, 880, 940, 1000)
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MAX_SIZE_TEST: 4000
  CROP:
    ENABLED: False
  ROTATE: False

SOLVER:
  AMP:
    ENABLED: True  # Enable Automatic Mixed Precision
  IMS_PER_BATCH: 2
  BASE_LR: 1e-5
  LR_BACKBONE: 1e-6
  WARMUP_ITERS: 250
  STEPS: (15000, 25000)  # no step
  MAX_ITER: 30000
  GAMMA: 0.1
  CHECKPOINT_PERIOD: 1000

TEST:
  EVAL_PERIOD: 1000
  # 1 - Generic, 2 - Weak, 3 - Strong (for icdar2015)
  LEXICON_TYPE: 3

OUTPUT_DIR: "output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15"

[04/29 16:37:30] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ic15_test
  TRAIN:
  - ic15_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    CROP_INSTANCE: false
    ENABLED: false
    SIZE:
    - 0.1
    - 0.1
    TYPE: relative_range
  FORMAT: RGB
  HFLIP_TRAIN: false
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 4000
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MIN_SIZE_TRAIN:
  - 640
  - 700
  - 760
  - 820
  - 880
  - 940
  - 1000
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  ROTATE: false
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    ANTI_ALIAS: false
    FREEZE_AT: 2
    NAME: build_resnet_backbone
  BASIS_MODULE:
    ANN_SET: coco
    COMMON_STRIDE: 8
    CONVS_DIM: 128
    IN_FEATURES:
    - p3
    - p4
    - p5
    LOSS_ON: false
    LOSS_WEIGHT: 0.3
    NAME: ProtoNet
    NORM: SyncBN
    NUM_BASES: 4
    NUM_CLASSES: 80
    NUM_CONVS: 3
  BATEXT:
    CANONICAL_SIZE: 96
    CONV_DIM: 256
    CUSTOM_DICT: ''
    IN_FEATURES:
    - p2
    - p3
    - p4
    NUM_CHARS: 25
    NUM_CONV: 2
    POOLER_RESOLUTION:
    - 8
    - 32
    POOLER_SCALES:
    - 0.25
    - 0.125
    - 0.0625
    RECOGNITION_LOSS: ctc
    RECOGNIZER: attn
    SAMPLING_RATIO: 1
    USE_AET: false
    USE_COORDCONV: false
    VOC_SIZE: 96
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: false
  META_ARCHITECTURE: TransformerPureDetector
  MOBILENET: false
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_INTERVAL: 1
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  SWIN:
    DROP_PATH_RATE: 0.2
    TYPE: tiny
  TOP_MODULE:
    DIM: 16
    NAME: conv
  TRANSFORMER:
    AUX_LOSS: true
    BOUNDARY_HEAD: true
    CUSTOM_DICT: ''
    DEC_LAYERS: 6
    DEC_N_POINTS: 4
    DIM_FEEDFORWARD: 1024
    DROPOUT: 0.0
    ENABLED: true
    ENC_LAYERS: 6
    ENC_N_POINTS: 4
    HIDDEN_DIM: 256
    INFERENCE_TH_TEST: 0.03
    LOSS:
      AUX_LOSS: true
      BEZIER_CLASS_WEIGHT: 1.0
      BEZIER_COORD_WEIGHT: 1.0
      BEZIER_SAMPLE_POINTS: 25
      BOUNDARY_WEIGHT: 0.5
      FOCAL_ALPHA: 0.25
      FOCAL_GAMMA: 2.0
      POINT_CLASS_WEIGHT: 1.0
      POINT_COORD_WEIGHT: 1.0
      POINT_TEXT_WEIGHT: 0.5
    NHEADS: 8
    NUM_FEATURE_LEVELS: 4
    NUM_POINTS: 25
    NUM_QUERIES: 100
    POSITION_EMBEDDING_SCALE: 6.283185307179586
    TEMPERATURE: 10000
    VOC_SIZE: 37
  ViTAEv2:
    DROP_PATH_RATE: 0.2
    TYPE: vitaev2_s
  WEIGHTS: output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth
OUTPUT_DIR: output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15
SEED: 42
SOLVER:
  AMP:
    ENABLED: true
  BASE_LR: 1.0e-05
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.1
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 2
  LR_BACKBONE: 1.0e-06
  LR_BACKBONE_NAMES:
  - backbone.0
  LR_LINEAR_PROJ_MULT: 1.0
  LR_LINEAR_PROJ_NAMES:
  - reference_points
  - sampling_offsets
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 30000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 15000
  - 25000
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 250
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  LEXICON_TYPE: 3
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/29 16:37:30] detectron2 INFO: Full config saved to output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/config.yaml
[04/29 16:37:31] d2.engine.defaults INFO: Model:
TransformerPureDetector(
  (detection_transformer): DETECTION_TRANSFORMER(
    (backbone): Joiner(
      (0): MaskedBackbone(
        (backbone): ResNet(
          (stem): BasicStem(
            (conv1): Conv2d(
              3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
          )
          (res2): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv1): Conv2d(
                64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
          )
          (res3): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv1): Conv2d(
                256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
          )
          (res4): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
              (conv1): Conv2d(
                512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (4): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (5): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
          )
          (res5): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
              (conv1): Conv2d(
                1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
          )
        )
      )
      (1): PositionalEncoding2D()
    )
    (point_embed): Embedding(2500, 256)
    (transformer): DeformableTransformer(
      (encoder): DeformableTransformerEncoder(
        (layers): ModuleList(
          (0): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (decoder): DeformableCompositeTransformerDecoder(
        (layers): ModuleList(
          (0): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ctrl_point_coord): ModuleList(
          (0): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (1): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (2): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (3): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (4): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (5): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
        )
      )
      (enc_output): Linear(in_features=256, out_features=256, bias=True)
      (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (bezier_coord_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=8, bias=True)
        )
      )
      (bezier_class_embed): Linear(in_features=256, out_features=1, bias=True)
    )
    (input_proj): ModuleList(
      (0): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (1): Sequential(
        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (2): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (3): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (bezier_proposal_coord): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=8, bias=True)
      )
    )
    (bezier_proposal_class): Linear(in_features=256, out_features=1, bias=True)
    (ctrl_point_coord): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
    )
    (ctrl_point_class): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (ctrl_point_text): ModuleList(
      (0): Linear(in_features=256, out_features=38, bias=True)
      (1): Linear(in_features=256, out_features=38, bias=True)
      (2): Linear(in_features=256, out_features=38, bias=True)
      (3): Linear(in_features=256, out_features=38, bias=True)
      (4): Linear(in_features=256, out_features=38, bias=True)
      (5): Linear(in_features=256, out_features=38, bias=True)
    )
    (boundary_offset): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (enc_matcher): BezierHungarianMatcher(
      (bezier_sampler): BezierSampler()
    )
    (dec_matcher): CtrlPointHungarianMatcher()
    (bezier_sampler): BezierSampler()
  )
)
[04/29 16:37:31] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth ...
[04/29 16:37:31] fvcore.common.checkpoint INFO: [Checkpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth ...
[04/29 16:37:31] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1440, 1440), max_size=4000, sample_style='choice')]
[04/29 16:37:31] adet.data.dataset_mapper INFO: Rebuilding the augmentations. The previous augmentations will be overridden.
[04/29 16:37:31] adet.data.datasets.text INFO: Loaded 500 images in COCO format from datasets/ic15/test.json
[04/29 16:37:31] d2.data.build INFO: Distribution of instances among all 1 categories:
[36m|  category  | #instances   |
|:----------:|:-------------|
|    text    | 0            |
|            |              |[0m
[04/29 16:37:31] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[04/29 16:37:31] d2.data.common INFO: Serializing 500 elements to byte tensors and concatenating them all ...
[04/29 16:37:31] d2.data.common INFO: Serialized dataset takes 0.06 MiB
[04/29 16:37:31] d2.evaluation.evaluator INFO: Start inference on 500 batches
[04/29 16:37:54] detectron2 INFO: Rank of current process: 0. World size: 1
[04/29 16:37:54] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/kylee/DeepSolo_LangPriors/detectron2/detectron2
Compiler                         GCC 9.5
CUDA compiler                    CUDA 11.3
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.12.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 3060 (arch=8.6)
Driver version                   550.120
CUDA_HOME                        /usr
Pillow                           9.0.1
torchvision                      0.13.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[04/29 16:37:54] detectron2 INFO: Command line arguments: Namespace(config_file='configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50152', opts=['MODEL.WEIGHTS', 'output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth'])
[04/29 16:37:54] detectron2 INFO: Contents of args.config_file=configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml:
_BASE_: "../Base_det.yaml"

MODEL:
  WEIGHTS: "output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth"
  TRANSFORMER:
    INFERENCE_TH_TEST: 0.02

DATASETS:
  TRAIN: ("ic15_train",)
  TEST: ("ic15_test",)

INPUT:
  MIN_SIZE_TRAIN: (640, 700, 760, 820, 880, 940, 1000)
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MAX_SIZE_TEST: 4000
  CROP:
    ENABLED: False
  ROTATE: False

SOLVER:
  AMP:
    ENABLED: True  # Enable Automatic Mixed Precision
  IMS_PER_BATCH: 2
  BASE_LR: 1e-5
  LR_BACKBONE: 1e-6
  WARMUP_ITERS: 250
  STEPS: (15000, 25000)  # no step
  MAX_ITER: 30000
  GAMMA: 0.1
  CHECKPOINT_PERIOD: 1000

TEST:
  EVAL_PERIOD: 1000
  # 1 - Generic, 2 - Weak, 3 - Strong (for icdar2015)
  LEXICON_TYPE: 3

OUTPUT_DIR: "output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15"

[04/29 16:37:54] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ic15_test
  TRAIN:
  - ic15_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    CROP_INSTANCE: false
    ENABLED: false
    SIZE:
    - 0.1
    - 0.1
    TYPE: relative_range
  FORMAT: RGB
  HFLIP_TRAIN: false
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 4000
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MIN_SIZE_TRAIN:
  - 640
  - 700
  - 760
  - 820
  - 880
  - 940
  - 1000
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  ROTATE: false
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    ANTI_ALIAS: false
    FREEZE_AT: 2
    NAME: build_resnet_backbone
  BASIS_MODULE:
    ANN_SET: coco
    COMMON_STRIDE: 8
    CONVS_DIM: 128
    IN_FEATURES:
    - p3
    - p4
    - p5
    LOSS_ON: false
    LOSS_WEIGHT: 0.3
    NAME: ProtoNet
    NORM: SyncBN
    NUM_BASES: 4
    NUM_CLASSES: 80
    NUM_CONVS: 3
  BATEXT:
    CANONICAL_SIZE: 96
    CONV_DIM: 256
    CUSTOM_DICT: ''
    IN_FEATURES:
    - p2
    - p3
    - p4
    NUM_CHARS: 25
    NUM_CONV: 2
    POOLER_RESOLUTION:
    - 8
    - 32
    POOLER_SCALES:
    - 0.25
    - 0.125
    - 0.0625
    RECOGNITION_LOSS: ctc
    RECOGNIZER: attn
    SAMPLING_RATIO: 1
    USE_AET: false
    USE_COORDCONV: false
    VOC_SIZE: 96
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: false
  META_ARCHITECTURE: TransformerPureDetector
  MOBILENET: false
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_INTERVAL: 1
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  SWIN:
    DROP_PATH_RATE: 0.2
    TYPE: tiny
  TOP_MODULE:
    DIM: 16
    NAME: conv
  TRANSFORMER:
    AUX_LOSS: true
    BOUNDARY_HEAD: true
    CUSTOM_DICT: ''
    DEC_LAYERS: 6
    DEC_N_POINTS: 4
    DIM_FEEDFORWARD: 1024
    DROPOUT: 0.0
    ENABLED: true
    ENC_LAYERS: 6
    ENC_N_POINTS: 4
    HIDDEN_DIM: 256
    INFERENCE_TH_TEST: 0.02
    LOSS:
      AUX_LOSS: true
      BEZIER_CLASS_WEIGHT: 1.0
      BEZIER_COORD_WEIGHT: 1.0
      BEZIER_SAMPLE_POINTS: 25
      BOUNDARY_WEIGHT: 0.5
      FOCAL_ALPHA: 0.25
      FOCAL_GAMMA: 2.0
      POINT_CLASS_WEIGHT: 1.0
      POINT_COORD_WEIGHT: 1.0
      POINT_TEXT_WEIGHT: 0.5
    NHEADS: 8
    NUM_FEATURE_LEVELS: 4
    NUM_POINTS: 25
    NUM_QUERIES: 100
    POSITION_EMBEDDING_SCALE: 6.283185307179586
    TEMPERATURE: 10000
    VOC_SIZE: 37
  ViTAEv2:
    DROP_PATH_RATE: 0.2
    TYPE: vitaev2_s
  WEIGHTS: output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth
OUTPUT_DIR: output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15
SEED: 42
SOLVER:
  AMP:
    ENABLED: true
  BASE_LR: 1.0e-05
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.1
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 2
  LR_BACKBONE: 1.0e-06
  LR_BACKBONE_NAMES:
  - backbone.0
  LR_LINEAR_PROJ_MULT: 1.0
  LR_LINEAR_PROJ_NAMES:
  - reference_points
  - sampling_offsets
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 30000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 15000
  - 25000
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 250
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  LEXICON_TYPE: 3
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/29 16:37:54] detectron2 INFO: Full config saved to output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/config.yaml
[04/29 16:37:55] d2.engine.defaults INFO: Model:
TransformerPureDetector(
  (detection_transformer): DETECTION_TRANSFORMER(
    (backbone): Joiner(
      (0): MaskedBackbone(
        (backbone): ResNet(
          (stem): BasicStem(
            (conv1): Conv2d(
              3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
          )
          (res2): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv1): Conv2d(
                64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
          )
          (res3): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv1): Conv2d(
                256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
          )
          (res4): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
              (conv1): Conv2d(
                512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (4): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (5): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
          )
          (res5): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
              (conv1): Conv2d(
                1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
          )
        )
      )
      (1): PositionalEncoding2D()
    )
    (point_embed): Embedding(2500, 256)
    (transformer): DeformableTransformer(
      (encoder): DeformableTransformerEncoder(
        (layers): ModuleList(
          (0): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (decoder): DeformableCompositeTransformerDecoder(
        (layers): ModuleList(
          (0): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ctrl_point_coord): ModuleList(
          (0): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (1): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (2): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (3): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (4): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (5): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
        )
      )
      (enc_output): Linear(in_features=256, out_features=256, bias=True)
      (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (bezier_coord_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=8, bias=True)
        )
      )
      (bezier_class_embed): Linear(in_features=256, out_features=1, bias=True)
    )
    (input_proj): ModuleList(
      (0): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (1): Sequential(
        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (2): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (3): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (bezier_proposal_coord): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=8, bias=True)
      )
    )
    (bezier_proposal_class): Linear(in_features=256, out_features=1, bias=True)
    (ctrl_point_coord): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
    )
    (ctrl_point_class): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (ctrl_point_text): ModuleList(
      (0): Linear(in_features=256, out_features=38, bias=True)
      (1): Linear(in_features=256, out_features=38, bias=True)
      (2): Linear(in_features=256, out_features=38, bias=True)
      (3): Linear(in_features=256, out_features=38, bias=True)
      (4): Linear(in_features=256, out_features=38, bias=True)
      (5): Linear(in_features=256, out_features=38, bias=True)
    )
    (boundary_offset): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (enc_matcher): BezierHungarianMatcher(
      (bezier_sampler): BezierSampler()
    )
    (dec_matcher): CtrlPointHungarianMatcher()
    (bezier_sampler): BezierSampler()
  )
)
[04/29 16:37:55] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth ...
[04/29 16:37:55] fvcore.common.checkpoint INFO: [Checkpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth ...
[04/29 16:37:56] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1440, 1440), max_size=4000, sample_style='choice')]
[04/29 16:37:56] adet.data.dataset_mapper INFO: Rebuilding the augmentations. The previous augmentations will be overridden.
[04/29 16:37:56] adet.data.datasets.text INFO: Loaded 500 images in COCO format from datasets/ic15/test.json
[04/29 16:37:56] d2.data.build INFO: Distribution of instances among all 1 categories:
[36m|  category  | #instances   |
|:----------:|:-------------|
|    text    | 0            |
|            |              |[0m
[04/29 16:37:56] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[04/29 16:37:56] d2.data.common INFO: Serializing 500 elements to byte tensors and concatenating them all ...
[04/29 16:37:56] d2.data.common INFO: Serialized dataset takes 0.06 MiB
[04/29 16:37:56] d2.evaluation.evaluator INFO: Start inference on 500 batches
[04/29 16:38:10] detectron2 INFO: Rank of current process: 0. World size: 1
[04/29 16:38:10] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/kylee/DeepSolo_LangPriors/detectron2/detectron2
Compiler                         GCC 9.5
CUDA compiler                    CUDA 11.3
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.12.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 3060 (arch=8.6)
Driver version                   550.120
CUDA_HOME                        /usr
Pillow                           9.0.1
torchvision                      0.13.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[04/29 16:38:10] detectron2 INFO: Command line arguments: Namespace(config_file='configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50152', opts=['MODEL.WEIGHTS', 'output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth'])
[04/29 16:38:10] detectron2 INFO: Contents of args.config_file=configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml:
_BASE_: "../Base_det.yaml"

MODEL:
  WEIGHTS: "output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth"
  TRANSFORMER:
    INFERENCE_TH_TEST: 0.01

DATASETS:
  TRAIN: ("ic15_train",)
  TEST: ("ic15_test",)

INPUT:
  MIN_SIZE_TRAIN: (640, 700, 760, 820, 880, 940, 1000)
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MAX_SIZE_TEST: 4000
  CROP:
    ENABLED: False
  ROTATE: False

SOLVER:
  AMP:
    ENABLED: True  # Enable Automatic Mixed Precision
  IMS_PER_BATCH: 2
  BASE_LR: 1e-5
  LR_BACKBONE: 1e-6
  WARMUP_ITERS: 250
  STEPS: (15000, 25000)  # no step
  MAX_ITER: 30000
  GAMMA: 0.1
  CHECKPOINT_PERIOD: 1000

TEST:
  EVAL_PERIOD: 1000
  # 1 - Generic, 2 - Weak, 3 - Strong (for icdar2015)
  LEXICON_TYPE: 3

OUTPUT_DIR: "output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15"

[04/29 16:38:11] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ic15_test
  TRAIN:
  - ic15_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    CROP_INSTANCE: false
    ENABLED: false
    SIZE:
    - 0.1
    - 0.1
    TYPE: relative_range
  FORMAT: RGB
  HFLIP_TRAIN: false
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 4000
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MIN_SIZE_TRAIN:
  - 640
  - 700
  - 760
  - 820
  - 880
  - 940
  - 1000
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  ROTATE: false
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    ANTI_ALIAS: false
    FREEZE_AT: 2
    NAME: build_resnet_backbone
  BASIS_MODULE:
    ANN_SET: coco
    COMMON_STRIDE: 8
    CONVS_DIM: 128
    IN_FEATURES:
    - p3
    - p4
    - p5
    LOSS_ON: false
    LOSS_WEIGHT: 0.3
    NAME: ProtoNet
    NORM: SyncBN
    NUM_BASES: 4
    NUM_CLASSES: 80
    NUM_CONVS: 3
  BATEXT:
    CANONICAL_SIZE: 96
    CONV_DIM: 256
    CUSTOM_DICT: ''
    IN_FEATURES:
    - p2
    - p3
    - p4
    NUM_CHARS: 25
    NUM_CONV: 2
    POOLER_RESOLUTION:
    - 8
    - 32
    POOLER_SCALES:
    - 0.25
    - 0.125
    - 0.0625
    RECOGNITION_LOSS: ctc
    RECOGNIZER: attn
    SAMPLING_RATIO: 1
    USE_AET: false
    USE_COORDCONV: false
    VOC_SIZE: 96
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: false
  META_ARCHITECTURE: TransformerPureDetector
  MOBILENET: false
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_INTERVAL: 1
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  SWIN:
    DROP_PATH_RATE: 0.2
    TYPE: tiny
  TOP_MODULE:
    DIM: 16
    NAME: conv
  TRANSFORMER:
    AUX_LOSS: true
    BOUNDARY_HEAD: true
    CUSTOM_DICT: ''
    DEC_LAYERS: 6
    DEC_N_POINTS: 4
    DIM_FEEDFORWARD: 1024
    DROPOUT: 0.0
    ENABLED: true
    ENC_LAYERS: 6
    ENC_N_POINTS: 4
    HIDDEN_DIM: 256
    INFERENCE_TH_TEST: 0.01
    LOSS:
      AUX_LOSS: true
      BEZIER_CLASS_WEIGHT: 1.0
      BEZIER_COORD_WEIGHT: 1.0
      BEZIER_SAMPLE_POINTS: 25
      BOUNDARY_WEIGHT: 0.5
      FOCAL_ALPHA: 0.25
      FOCAL_GAMMA: 2.0
      POINT_CLASS_WEIGHT: 1.0
      POINT_COORD_WEIGHT: 1.0
      POINT_TEXT_WEIGHT: 0.5
    NHEADS: 8
    NUM_FEATURE_LEVELS: 4
    NUM_POINTS: 25
    NUM_QUERIES: 100
    POSITION_EMBEDDING_SCALE: 6.283185307179586
    TEMPERATURE: 10000
    VOC_SIZE: 37
  ViTAEv2:
    DROP_PATH_RATE: 0.2
    TYPE: vitaev2_s
  WEIGHTS: output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth
OUTPUT_DIR: output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15
SEED: 42
SOLVER:
  AMP:
    ENABLED: true
  BASE_LR: 1.0e-05
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.1
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 2
  LR_BACKBONE: 1.0e-06
  LR_BACKBONE_NAMES:
  - backbone.0
  LR_LINEAR_PROJ_MULT: 1.0
  LR_LINEAR_PROJ_NAMES:
  - reference_points
  - sampling_offsets
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 30000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 15000
  - 25000
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 250
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  LEXICON_TYPE: 3
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/29 16:38:11] detectron2 INFO: Full config saved to output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/config.yaml
[04/29 16:38:12] d2.engine.defaults INFO: Model:
TransformerPureDetector(
  (detection_transformer): DETECTION_TRANSFORMER(
    (backbone): Joiner(
      (0): MaskedBackbone(
        (backbone): ResNet(
          (stem): BasicStem(
            (conv1): Conv2d(
              3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
          )
          (res2): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv1): Conv2d(
                64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
          )
          (res3): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv1): Conv2d(
                256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
          )
          (res4): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
              (conv1): Conv2d(
                512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (4): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (5): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
          )
          (res5): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
              (conv1): Conv2d(
                1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
          )
        )
      )
      (1): PositionalEncoding2D()
    )
    (point_embed): Embedding(2500, 256)
    (transformer): DeformableTransformer(
      (encoder): DeformableTransformerEncoder(
        (layers): ModuleList(
          (0): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (decoder): DeformableCompositeTransformerDecoder(
        (layers): ModuleList(
          (0): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ctrl_point_coord): ModuleList(
          (0): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (1): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (2): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (3): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (4): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (5): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
        )
      )
      (enc_output): Linear(in_features=256, out_features=256, bias=True)
      (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (bezier_coord_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=8, bias=True)
        )
      )
      (bezier_class_embed): Linear(in_features=256, out_features=1, bias=True)
    )
    (input_proj): ModuleList(
      (0): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (1): Sequential(
        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (2): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (3): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (bezier_proposal_coord): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=8, bias=True)
      )
    )
    (bezier_proposal_class): Linear(in_features=256, out_features=1, bias=True)
    (ctrl_point_coord): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
    )
    (ctrl_point_class): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (ctrl_point_text): ModuleList(
      (0): Linear(in_features=256, out_features=38, bias=True)
      (1): Linear(in_features=256, out_features=38, bias=True)
      (2): Linear(in_features=256, out_features=38, bias=True)
      (3): Linear(in_features=256, out_features=38, bias=True)
      (4): Linear(in_features=256, out_features=38, bias=True)
      (5): Linear(in_features=256, out_features=38, bias=True)
    )
    (boundary_offset): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (enc_matcher): BezierHungarianMatcher(
      (bezier_sampler): BezierSampler()
    )
    (dec_matcher): CtrlPointHungarianMatcher()
    (bezier_sampler): BezierSampler()
  )
)
[04/29 16:38:12] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth ...
[04/29 16:38:12] fvcore.common.checkpoint INFO: [Checkpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth ...
[04/29 16:38:12] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1440, 1440), max_size=4000, sample_style='choice')]
[04/29 16:38:12] adet.data.dataset_mapper INFO: Rebuilding the augmentations. The previous augmentations will be overridden.
[04/29 16:38:12] adet.data.datasets.text INFO: Loaded 500 images in COCO format from datasets/ic15/test.json
[04/29 16:38:12] d2.data.build INFO: Distribution of instances among all 1 categories:
[36m|  category  | #instances   |
|:----------:|:-------------|
|    text    | 0            |
|            |              |[0m
[04/29 16:38:12] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[04/29 16:38:12] d2.data.common INFO: Serializing 500 elements to byte tensors and concatenating them all ...
[04/29 16:38:12] d2.data.common INFO: Serialized dataset takes 0.06 MiB
[04/29 16:38:12] d2.evaluation.evaluator INFO: Start inference on 500 batches
[04/29 16:57:22] detectron2 INFO: Rank of current process: 0. World size: 1
[04/29 16:57:22] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/kylee/DeepSolo_LangPriors/detectron2/detectron2
Compiler                         GCC 9.5
CUDA compiler                    CUDA 11.3
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.12.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 3060 (arch=8.6)
Driver version                   550.120
CUDA_HOME                        /usr
Pillow                           9.0.1
torchvision                      0.13.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[04/29 16:57:22] detectron2 INFO: Command line arguments: Namespace(config_file='configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50152', opts=['MODEL.WEIGHTS', 'output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth'])
[04/29 16:57:22] detectron2 INFO: Contents of args.config_file=configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml:
_BASE_: "../Base_det.yaml"

MODEL:
  WEIGHTS: "output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth"
  TRANSFORMER:
    INFERENCE_TH_TEST: 0.01

DATASETS:
  TRAIN: ("ic15_train",)
  TEST: ("ic15_test",)

INPUT:
  MIN_SIZE_TRAIN: (640, 700, 760, 820, 880, 940, 1000)
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MAX_SIZE_TEST: 4000
  CROP:
    ENABLED: False
  ROTATE: False

SOLVER:
  AMP:
    ENABLED: True  # Enable Automatic Mixed Precision
  IMS_PER_BATCH: 2
  BASE_LR: 1e-5
  LR_BACKBONE: 1e-6
  WARMUP_ITERS: 250
  STEPS: (15000, 25000)  # no step
  MAX_ITER: 30000
  GAMMA: 0.1
  CHECKPOINT_PERIOD: 1000

TEST:
  EVAL_PERIOD: 1000
  # 1 - Generic, 2 - Weak, 3 - Strong (for icdar2015)
  LEXICON_TYPE: 3

OUTPUT_DIR: "output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15"

[04/29 16:57:22] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ic15_test
  TRAIN:
  - ic15_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    CROP_INSTANCE: false
    ENABLED: false
    SIZE:
    - 0.1
    - 0.1
    TYPE: relative_range
  FORMAT: RGB
  HFLIP_TRAIN: false
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 4000
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MIN_SIZE_TRAIN:
  - 640
  - 700
  - 760
  - 820
  - 880
  - 940
  - 1000
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  ROTATE: false
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    ANTI_ALIAS: false
    FREEZE_AT: 2
    NAME: build_resnet_backbone
  BASIS_MODULE:
    ANN_SET: coco
    COMMON_STRIDE: 8
    CONVS_DIM: 128
    IN_FEATURES:
    - p3
    - p4
    - p5
    LOSS_ON: false
    LOSS_WEIGHT: 0.3
    NAME: ProtoNet
    NORM: SyncBN
    NUM_BASES: 4
    NUM_CLASSES: 80
    NUM_CONVS: 3
  BATEXT:
    CANONICAL_SIZE: 96
    CONV_DIM: 256
    CUSTOM_DICT: ''
    IN_FEATURES:
    - p2
    - p3
    - p4
    NUM_CHARS: 25
    NUM_CONV: 2
    POOLER_RESOLUTION:
    - 8
    - 32
    POOLER_SCALES:
    - 0.25
    - 0.125
    - 0.0625
    RECOGNITION_LOSS: ctc
    RECOGNIZER: attn
    SAMPLING_RATIO: 1
    USE_AET: false
    USE_COORDCONV: false
    VOC_SIZE: 96
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: false
  META_ARCHITECTURE: TransformerPureDetector
  MOBILENET: false
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_INTERVAL: 1
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  SWIN:
    DROP_PATH_RATE: 0.2
    TYPE: tiny
  TOP_MODULE:
    DIM: 16
    NAME: conv
  TRANSFORMER:
    AUX_LOSS: true
    BOUNDARY_HEAD: true
    CUSTOM_DICT: ''
    DEC_LAYERS: 6
    DEC_N_POINTS: 4
    DIM_FEEDFORWARD: 1024
    DROPOUT: 0.0
    ENABLED: true
    ENC_LAYERS: 6
    ENC_N_POINTS: 4
    HIDDEN_DIM: 256
    INFERENCE_TH_TEST: 0.01
    LOSS:
      AUX_LOSS: true
      BEZIER_CLASS_WEIGHT: 1.0
      BEZIER_COORD_WEIGHT: 1.0
      BEZIER_SAMPLE_POINTS: 25
      BOUNDARY_WEIGHT: 0.5
      FOCAL_ALPHA: 0.25
      FOCAL_GAMMA: 2.0
      POINT_CLASS_WEIGHT: 1.0
      POINT_COORD_WEIGHT: 1.0
      POINT_TEXT_WEIGHT: 0.5
    NHEADS: 8
    NUM_FEATURE_LEVELS: 4
    NUM_POINTS: 25
    NUM_QUERIES: 100
    POSITION_EMBEDDING_SCALE: 6.283185307179586
    TEMPERATURE: 10000
    VOC_SIZE: 37
  ViTAEv2:
    DROP_PATH_RATE: 0.2
    TYPE: vitaev2_s
  WEIGHTS: output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth
OUTPUT_DIR: output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15
SEED: 42
SOLVER:
  AMP:
    ENABLED: true
  BASE_LR: 1.0e-05
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.1
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 2
  LR_BACKBONE: 1.0e-06
  LR_BACKBONE_NAMES:
  - backbone.0
  LR_LINEAR_PROJ_MULT: 1.0
  LR_LINEAR_PROJ_NAMES:
  - reference_points
  - sampling_offsets
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 30000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 15000
  - 25000
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 250
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  LEXICON_TYPE: 3
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/29 16:57:22] detectron2 INFO: Full config saved to output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/config.yaml
[04/29 16:57:24] d2.engine.defaults INFO: Model:
TransformerPureDetector(
  (detection_transformer): DETECTION_TRANSFORMER(
    (backbone): Joiner(
      (0): MaskedBackbone(
        (backbone): ResNet(
          (stem): BasicStem(
            (conv1): Conv2d(
              3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
          )
          (res2): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv1): Conv2d(
                64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
          )
          (res3): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv1): Conv2d(
                256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
          )
          (res4): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
              (conv1): Conv2d(
                512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (4): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (5): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
          )
          (res5): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
              (conv1): Conv2d(
                1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
          )
        )
      )
      (1): PositionalEncoding2D()
    )
    (point_embed): Embedding(2500, 256)
    (transformer): DeformableTransformer(
      (encoder): DeformableTransformerEncoder(
        (layers): ModuleList(
          (0): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (decoder): DeformableCompositeTransformerDecoder(
        (layers): ModuleList(
          (0): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ctrl_point_coord): ModuleList(
          (0): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (1): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (2): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (3): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (4): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (5): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
        )
      )
      (enc_output): Linear(in_features=256, out_features=256, bias=True)
      (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (bezier_coord_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=8, bias=True)
        )
      )
      (bezier_class_embed): Linear(in_features=256, out_features=1, bias=True)
    )
    (input_proj): ModuleList(
      (0): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (1): Sequential(
        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (2): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (3): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (bezier_proposal_coord): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=8, bias=True)
      )
    )
    (bezier_proposal_class): Linear(in_features=256, out_features=1, bias=True)
    (ctrl_point_coord): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
    )
    (ctrl_point_class): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (ctrl_point_text): ModuleList(
      (0): Linear(in_features=256, out_features=38, bias=True)
      (1): Linear(in_features=256, out_features=38, bias=True)
      (2): Linear(in_features=256, out_features=38, bias=True)
      (3): Linear(in_features=256, out_features=38, bias=True)
      (4): Linear(in_features=256, out_features=38, bias=True)
      (5): Linear(in_features=256, out_features=38, bias=True)
    )
    (boundary_offset): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (enc_matcher): BezierHungarianMatcher(
      (bezier_sampler): BezierSampler()
    )
    (dec_matcher): CtrlPointHungarianMatcher()
    (bezier_sampler): BezierSampler()
  )
)
[04/29 16:57:24] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth ...
[04/29 16:57:24] fvcore.common.checkpoint INFO: [Checkpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth ...
[04/29 16:57:24] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1440, 1440), max_size=4000, sample_style='choice')]
[04/29 16:57:24] adet.data.dataset_mapper INFO: Rebuilding the augmentations. The previous augmentations will be overridden.
[04/29 16:57:24] adet.data.datasets.text INFO: Loaded 500 images in COCO format from datasets/ic15/test.json
[04/29 16:57:24] d2.data.build INFO: Distribution of instances among all 1 categories:
[36m|  category  | #instances   |
|:----------:|:-------------|
|    text    | 0            |
|            |              |[0m
[04/29 16:57:24] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[04/29 16:57:24] d2.data.common INFO: Serializing 500 elements to byte tensors and concatenating them all ...
[04/29 16:57:24] d2.data.common INFO: Serialized dataset takes 0.06 MiB
[04/29 16:57:24] d2.evaluation.evaluator INFO: Start inference on 500 batches
[04/29 16:57:30] d2.evaluation.evaluator INFO: Inference done 11/500. Dataloading: 0.0009 s/iter. Inference: 0.3884 s/iter. Eval: 0.0030 s/iter. Total: 0.3923 s/iter. ETA=0:03:11
[04/29 16:57:35] d2.evaluation.evaluator INFO: Inference done 24/500. Dataloading: 0.0011 s/iter. Inference: 0.3869 s/iter. Eval: 0.0021 s/iter. Total: 0.3902 s/iter. ETA=0:03:05
[04/29 16:57:40] d2.evaluation.evaluator INFO: Inference done 37/500. Dataloading: 0.0012 s/iter. Inference: 0.3896 s/iter. Eval: 0.0020 s/iter. Total: 0.3929 s/iter. ETA=0:03:01
[04/29 16:57:45] d2.evaluation.evaluator INFO: Inference done 50/500. Dataloading: 0.0012 s/iter. Inference: 0.3913 s/iter. Eval: 0.0021 s/iter. Total: 0.3947 s/iter. ETA=0:02:57
[04/29 16:57:51] d2.evaluation.evaluator INFO: Inference done 63/500. Dataloading: 0.0013 s/iter. Inference: 0.3928 s/iter. Eval: 0.0019 s/iter. Total: 0.3960 s/iter. ETA=0:02:53
[04/29 16:57:56] d2.evaluation.evaluator INFO: Inference done 76/500. Dataloading: 0.0013 s/iter. Inference: 0.3940 s/iter. Eval: 0.0017 s/iter. Total: 0.3971 s/iter. ETA=0:02:48
[04/29 16:58:01] d2.evaluation.evaluator INFO: Inference done 89/500. Dataloading: 0.0013 s/iter. Inference: 0.3932 s/iter. Eval: 0.0017 s/iter. Total: 0.3963 s/iter. ETA=0:02:42
[04/29 16:58:06] d2.evaluation.evaluator INFO: Inference done 102/500. Dataloading: 0.0013 s/iter. Inference: 0.3927 s/iter. Eval: 0.0017 s/iter. Total: 0.3958 s/iter. ETA=0:02:37
[04/29 16:58:11] d2.evaluation.evaluator INFO: Inference done 115/500. Dataloading: 0.0014 s/iter. Inference: 0.3918 s/iter. Eval: 0.0017 s/iter. Total: 0.3949 s/iter. ETA=0:02:32
[04/29 16:58:16] d2.evaluation.evaluator INFO: Inference done 128/500. Dataloading: 0.0014 s/iter. Inference: 0.3918 s/iter. Eval: 0.0017 s/iter. Total: 0.3949 s/iter. ETA=0:02:26
[04/29 16:58:21] d2.evaluation.evaluator INFO: Inference done 141/500. Dataloading: 0.0014 s/iter. Inference: 0.3914 s/iter. Eval: 0.0017 s/iter. Total: 0.3945 s/iter. ETA=0:02:21
[04/29 16:58:26] d2.evaluation.evaluator INFO: Inference done 154/500. Dataloading: 0.0014 s/iter. Inference: 0.3913 s/iter. Eval: 0.0018 s/iter. Total: 0.3945 s/iter. ETA=0:02:16
[04/29 16:58:31] d2.evaluation.evaluator INFO: Inference done 167/500. Dataloading: 0.0014 s/iter. Inference: 0.3910 s/iter. Eval: 0.0018 s/iter. Total: 0.3942 s/iter. ETA=0:02:11
[04/29 16:58:37] d2.evaluation.evaluator INFO: Inference done 180/500. Dataloading: 0.0014 s/iter. Inference: 0.3910 s/iter. Eval: 0.0019 s/iter. Total: 0.3943 s/iter. ETA=0:02:06
[04/29 16:58:42] d2.evaluation.evaluator INFO: Inference done 193/500. Dataloading: 0.0014 s/iter. Inference: 0.3910 s/iter. Eval: 0.0019 s/iter. Total: 0.3943 s/iter. ETA=0:02:01
[04/29 16:58:47] d2.evaluation.evaluator INFO: Inference done 206/500. Dataloading: 0.0014 s/iter. Inference: 0.3914 s/iter. Eval: 0.0019 s/iter. Total: 0.3947 s/iter. ETA=0:01:56
[04/29 16:58:52] d2.evaluation.evaluator INFO: Inference done 219/500. Dataloading: 0.0014 s/iter. Inference: 0.3915 s/iter. Eval: 0.0019 s/iter. Total: 0.3948 s/iter. ETA=0:01:50
[04/29 16:58:57] d2.evaluation.evaluator INFO: Inference done 232/500. Dataloading: 0.0014 s/iter. Inference: 0.3914 s/iter. Eval: 0.0019 s/iter. Total: 0.3947 s/iter. ETA=0:01:45
[04/29 16:59:02] d2.evaluation.evaluator INFO: Inference done 245/500. Dataloading: 0.0014 s/iter. Inference: 0.3914 s/iter. Eval: 0.0018 s/iter. Total: 0.3947 s/iter. ETA=0:01:40
[04/29 16:59:07] d2.evaluation.evaluator INFO: Inference done 258/500. Dataloading: 0.0014 s/iter. Inference: 0.3914 s/iter. Eval: 0.0018 s/iter. Total: 0.3947 s/iter. ETA=0:01:35
[04/29 16:59:13] d2.evaluation.evaluator INFO: Inference done 271/500. Dataloading: 0.0014 s/iter. Inference: 0.3920 s/iter. Eval: 0.0019 s/iter. Total: 0.3953 s/iter. ETA=0:01:30
[04/29 16:59:18] d2.evaluation.evaluator INFO: Inference done 284/500. Dataloading: 0.0014 s/iter. Inference: 0.3924 s/iter. Eval: 0.0019 s/iter. Total: 0.3958 s/iter. ETA=0:01:25
[04/29 16:59:23] d2.evaluation.evaluator INFO: Inference done 297/500. Dataloading: 0.0014 s/iter. Inference: 0.3928 s/iter. Eval: 0.0019 s/iter. Total: 0.3961 s/iter. ETA=0:01:20
[04/29 16:59:28] d2.evaluation.evaluator INFO: Inference done 310/500. Dataloading: 0.0014 s/iter. Inference: 0.3929 s/iter. Eval: 0.0018 s/iter. Total: 0.3962 s/iter. ETA=0:01:15
[04/29 16:59:34] d2.evaluation.evaluator INFO: Inference done 323/500. Dataloading: 0.0014 s/iter. Inference: 0.3931 s/iter. Eval: 0.0018 s/iter. Total: 0.3964 s/iter. ETA=0:01:10
[04/29 16:59:39] d2.evaluation.evaluator INFO: Inference done 336/500. Dataloading: 0.0014 s/iter. Inference: 0.3929 s/iter. Eval: 0.0018 s/iter. Total: 0.3962 s/iter. ETA=0:01:04
[04/29 16:59:44] d2.evaluation.evaluator INFO: Inference done 349/500. Dataloading: 0.0014 s/iter. Inference: 0.3931 s/iter. Eval: 0.0020 s/iter. Total: 0.3965 s/iter. ETA=0:00:59
[04/29 16:59:49] d2.evaluation.evaluator INFO: Inference done 362/500. Dataloading: 0.0014 s/iter. Inference: 0.3932 s/iter. Eval: 0.0020 s/iter. Total: 0.3967 s/iter. ETA=0:00:54
[04/29 16:59:54] d2.evaluation.evaluator INFO: Inference done 375/500. Dataloading: 0.0014 s/iter. Inference: 0.3936 s/iter. Eval: 0.0020 s/iter. Total: 0.3970 s/iter. ETA=0:00:49
[04/29 17:04:25] detectron2 INFO: Rank of current process: 0. World size: 1
[04/29 17:04:25] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/kylee/DeepSolo_LangPriors/detectron2/detectron2
Compiler                         GCC 9.5
CUDA compiler                    CUDA 11.3
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.12.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 3060 (arch=8.6)
Driver version                   550.120
CUDA_HOME                        /usr
Pillow                           9.0.1
torchvision                      0.13.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[04/29 17:04:25] detectron2 INFO: Command line arguments: Namespace(config_file='configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50152', opts=[])
[04/29 17:04:25] detectron2 INFO: Contents of args.config_file=configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml:
_BASE_: "../Base_det.yaml"

MODEL:
  WEIGHTS: "output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth"
  TRANSFORMER:
    INFERENCE_TH_TEST: 0.01

DATASETS:
  TRAIN: ("ic15_train",)
  TEST: ("ic15_test",)

INPUT:
  MIN_SIZE_TRAIN: (640, 700, 760, 820, 880, 940, 1000)
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MAX_SIZE_TEST: 4000
  CROP:
    ENABLED: False
  ROTATE: False

SOLVER:
  AMP:
    ENABLED: True  # Enable Automatic Mixed Precision
  IMS_PER_BATCH: 2
  BASE_LR: 1e-5
  LR_BACKBONE: 1e-6
  WARMUP_ITERS: 250
  STEPS: (15000, 25000)  # no step
  MAX_ITER: 30000
  GAMMA: 0.1
  CHECKPOINT_PERIOD: 1000

TEST:
  EVAL_PERIOD: 1000
  # 1 - Generic, 2 - Weak, 3 - Strong (for icdar2015)
  LEXICON_TYPE: 3

OUTPUT_DIR: "output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15"

[04/29 17:04:25] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ic15_test
  TRAIN:
  - ic15_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    CROP_INSTANCE: false
    ENABLED: false
    SIZE:
    - 0.1
    - 0.1
    TYPE: relative_range
  FORMAT: RGB
  HFLIP_TRAIN: false
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 4000
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MIN_SIZE_TRAIN:
  - 640
  - 700
  - 760
  - 820
  - 880
  - 940
  - 1000
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  ROTATE: false
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    ANTI_ALIAS: false
    FREEZE_AT: 2
    NAME: build_resnet_backbone
  BASIS_MODULE:
    ANN_SET: coco
    COMMON_STRIDE: 8
    CONVS_DIM: 128
    IN_FEATURES:
    - p3
    - p4
    - p5
    LOSS_ON: false
    LOSS_WEIGHT: 0.3
    NAME: ProtoNet
    NORM: SyncBN
    NUM_BASES: 4
    NUM_CLASSES: 80
    NUM_CONVS: 3
  BATEXT:
    CANONICAL_SIZE: 96
    CONV_DIM: 256
    CUSTOM_DICT: ''
    IN_FEATURES:
    - p2
    - p3
    - p4
    NUM_CHARS: 25
    NUM_CONV: 2
    POOLER_RESOLUTION:
    - 8
    - 32
    POOLER_SCALES:
    - 0.25
    - 0.125
    - 0.0625
    RECOGNITION_LOSS: ctc
    RECOGNIZER: attn
    SAMPLING_RATIO: 1
    USE_AET: false
    USE_COORDCONV: false
    VOC_SIZE: 96
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: false
  META_ARCHITECTURE: TransformerPureDetector
  MOBILENET: false
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_INTERVAL: 1
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  SWIN:
    DROP_PATH_RATE: 0.2
    TYPE: tiny
  TOP_MODULE:
    DIM: 16
    NAME: conv
  TRANSFORMER:
    AUX_LOSS: true
    BOUNDARY_HEAD: true
    CUSTOM_DICT: ''
    DEC_LAYERS: 6
    DEC_N_POINTS: 4
    DIM_FEEDFORWARD: 1024
    DROPOUT: 0.0
    ENABLED: true
    ENC_LAYERS: 6
    ENC_N_POINTS: 4
    HIDDEN_DIM: 256
    INFERENCE_TH_TEST: 0.01
    LOSS:
      AUX_LOSS: true
      BEZIER_CLASS_WEIGHT: 1.0
      BEZIER_COORD_WEIGHT: 1.0
      BEZIER_SAMPLE_POINTS: 25
      BOUNDARY_WEIGHT: 0.5
      FOCAL_ALPHA: 0.25
      FOCAL_GAMMA: 2.0
      POINT_CLASS_WEIGHT: 1.0
      POINT_COORD_WEIGHT: 1.0
      POINT_TEXT_WEIGHT: 0.5
    NHEADS: 8
    NUM_FEATURE_LEVELS: 4
    NUM_POINTS: 25
    NUM_QUERIES: 100
    POSITION_EMBEDDING_SCALE: 6.283185307179586
    TEMPERATURE: 10000
    VOC_SIZE: 37
  ViTAEv2:
    DROP_PATH_RATE: 0.2
    TYPE: vitaev2_s
  WEIGHTS: output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth
OUTPUT_DIR: output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15
SEED: 42
SOLVER:
  AMP:
    ENABLED: true
  BASE_LR: 1.0e-05
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.1
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 2
  LR_BACKBONE: 1.0e-06
  LR_BACKBONE_NAMES:
  - backbone.0
  LR_LINEAR_PROJ_MULT: 1.0
  LR_LINEAR_PROJ_NAMES:
  - reference_points
  - sampling_offsets
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 30000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 15000
  - 25000
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 250
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  LEXICON_TYPE: 3
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/29 17:04:25] detectron2 INFO: Full config saved to output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/config.yaml
[04/29 17:04:27] d2.engine.defaults INFO: Model:
TransformerPureDetector(
  (detection_transformer): DETECTION_TRANSFORMER(
    (backbone): Joiner(
      (0): MaskedBackbone(
        (backbone): ResNet(
          (stem): BasicStem(
            (conv1): Conv2d(
              3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
          )
          (res2): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv1): Conv2d(
                64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
          )
          (res3): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv1): Conv2d(
                256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
          )
          (res4): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
              (conv1): Conv2d(
                512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (4): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (5): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
          )
          (res5): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
              (conv1): Conv2d(
                1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
          )
        )
      )
      (1): PositionalEncoding2D()
    )
    (point_embed): Embedding(2500, 256)
    (transformer): DeformableTransformer(
      (encoder): DeformableTransformerEncoder(
        (layers): ModuleList(
          (0): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (decoder): DeformableCompositeTransformerDecoder(
        (layers): ModuleList(
          (0): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ctrl_point_coord): ModuleList(
          (0): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (1): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (2): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (3): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (4): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (5): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
        )
      )
      (enc_output): Linear(in_features=256, out_features=256, bias=True)
      (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (bezier_coord_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=8, bias=True)
        )
      )
      (bezier_class_embed): Linear(in_features=256, out_features=1, bias=True)
    )
    (input_proj): ModuleList(
      (0): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (1): Sequential(
        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (2): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (3): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (bezier_proposal_coord): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=8, bias=True)
      )
    )
    (bezier_proposal_class): Linear(in_features=256, out_features=1, bias=True)
    (ctrl_point_coord): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
    )
    (ctrl_point_class): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (ctrl_point_text): ModuleList(
      (0): Linear(in_features=256, out_features=38, bias=True)
      (1): Linear(in_features=256, out_features=38, bias=True)
      (2): Linear(in_features=256, out_features=38, bias=True)
      (3): Linear(in_features=256, out_features=38, bias=True)
      (4): Linear(in_features=256, out_features=38, bias=True)
      (5): Linear(in_features=256, out_features=38, bias=True)
    )
    (boundary_offset): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (enc_matcher): BezierHungarianMatcher(
      (bezier_sampler): BezierSampler()
    )
    (dec_matcher): CtrlPointHungarianMatcher()
    (bezier_sampler): BezierSampler()
  )
)
[04/29 17:04:27] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 700, 760, 820, 880, 940, 1000), max_size=2000, sample_style='choice'), RandomFlip()]
[04/29 17:04:27] adet.data.dataset_mapper INFO: Rebuilding the augmentations. The previous augmentations will be overridden.
[04/29 17:04:27] adet.data.detection_utils INFO: Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 700, 760, 820, 880, 940, 1000), max_size=2000, sample_style='choice'), RandomContrast(intensity_min=0.3, intensity_max=1.7), RandomBrightness(intensity_min=0.3, intensity_max=1.7), RandomLighting(scale=1.1394267984578836), RandomSaturation(intensity_min=0.3, intensity_max=1.7)]
[04/29 17:04:27] adet.data.datasets.text INFO: Loaded 1000 images in COCO format from datasets/ic15/train_37voc.json
[04/29 17:04:27] d2.data.build INFO: Removed 0 images with no usable annotations. 979 images left.
[04/29 17:04:27] d2.data.build INFO: Distribution of instances among all 1 categories:
[36m|  category  | #instances   |
|:----------:|:-------------|
|    text    | 4441         |
|            |              |[0m
[04/29 17:04:27] d2.data.build INFO: Using training sampler TrainingSampler
[04/29 17:04:27] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[04/29 17:04:27] d2.data.common INFO: Serializing 979 elements to byte tensors and concatenating them all ...
[04/29 17:04:27] d2.data.common INFO: Serialized dataset takes 6.51 MiB
[04/29 17:04:27] d2.data.build INFO: Making batched data loader with batch_size=2
[04/29 17:04:27] d2.engine.defaults INFO: Model:
TransformerPureDetector(
  (detection_transformer): DETECTION_TRANSFORMER(
    (backbone): Joiner(
      (0): MaskedBackbone(
        (backbone): ResNet(
          (stem): BasicStem(
            (conv1): Conv2d(
              3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
          )
          (res2): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv1): Conv2d(
                64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
          )
          (res3): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv1): Conv2d(
                256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
          )
          (res4): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
              (conv1): Conv2d(
                512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (4): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (5): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
          )
          (res5): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
              (conv1): Conv2d(
                1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
          )
        )
      )
      (1): PositionalEncoding2D()
    )
    (point_embed): Embedding(2500, 256)
    (transformer): DeformableTransformer(
      (encoder): DeformableTransformerEncoder(
        (layers): ModuleList(
          (0): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (decoder): DeformableCompositeTransformerDecoder(
        (layers): ModuleList(
          (0): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ctrl_point_coord): ModuleList(
          (0): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (1): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (2): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (3): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (4): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (5): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
        )
      )
      (enc_output): Linear(in_features=256, out_features=256, bias=True)
      (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (bezier_coord_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=8, bias=True)
        )
      )
      (bezier_class_embed): Linear(in_features=256, out_features=1, bias=True)
    )
    (input_proj): ModuleList(
      (0): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (1): Sequential(
        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (2): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (3): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (bezier_proposal_coord): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=8, bias=True)
      )
    )
    (bezier_proposal_class): Linear(in_features=256, out_features=1, bias=True)
    (ctrl_point_coord): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
    )
    (ctrl_point_class): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (ctrl_point_text): ModuleList(
      (0): Linear(in_features=256, out_features=38, bias=True)
      (1): Linear(in_features=256, out_features=38, bias=True)
      (2): Linear(in_features=256, out_features=38, bias=True)
      (3): Linear(in_features=256, out_features=38, bias=True)
      (4): Linear(in_features=256, out_features=38, bias=True)
      (5): Linear(in_features=256, out_features=38, bias=True)
    )
    (boundary_offset): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (enc_matcher): BezierHungarianMatcher(
      (bezier_sampler): BezierSampler()
    )
    (dec_matcher): CtrlPointHungarianMatcher()
    (bezier_sampler): BezierSampler()
  )
)
[04/29 17:04:27] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 700, 760, 820, 880, 940, 1000), max_size=2000, sample_style='choice'), RandomFlip()]
[04/29 17:04:27] adet.data.dataset_mapper INFO: Rebuilding the augmentations. The previous augmentations will be overridden.
[04/29 17:04:27] adet.data.detection_utils INFO: Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 700, 760, 820, 880, 940, 1000), max_size=2000, sample_style='choice'), RandomContrast(intensity_min=0.3, intensity_max=1.7), RandomBrightness(intensity_min=0.3, intensity_max=1.7), RandomLighting(scale=0.5250107552226669), RandomSaturation(intensity_min=0.3, intensity_max=1.7)]
[04/29 17:04:27] adet.data.datasets.text INFO: Loaded 1000 images in COCO format from datasets/ic15/train_37voc.json
[04/29 17:04:28] d2.data.build INFO: Removed 0 images with no usable annotations. 979 images left.
[04/29 17:04:28] d2.data.build INFO: Using training sampler TrainingSampler
[04/29 17:04:28] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[04/29 17:04:28] d2.data.common INFO: Serializing 979 elements to byte tensors and concatenating them all ...
[04/29 17:04:28] d2.data.common INFO: Serialized dataset takes 6.51 MiB
[04/29 17:04:28] d2.data.build INFO: Making batched data loader with batch_size=2
[04/29 17:04:28] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth ...
[04/29 17:04:28] fvcore.common.checkpoint INFO: [Checkpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth ...
[04/29 17:09:37] detectron2 INFO: Rank of current process: 0. World size: 1
[04/29 17:09:37] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/kylee/DeepSolo_LangPriors/detectron2/detectron2
Compiler                         GCC 9.5
CUDA compiler                    CUDA 11.3
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.12.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 3060 (arch=8.6)
Driver version                   550.120
CUDA_HOME                        /usr
Pillow                           9.0.1
torchvision                      0.13.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[04/29 17:09:37] detectron2 INFO: Command line arguments: Namespace(config_file='configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50152', opts=[])
[04/29 17:09:37] detectron2 INFO: Contents of args.config_file=configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml:
_BASE_: "../Base_det.yaml"

MODEL:
  WEIGHTS: "output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth"
  TRANSFORMER:
    INFERENCE_TH_TEST: 0.01

DATASETS:
  TRAIN: ("ic15_train",)
  TEST: ("ic15_test",)

INPUT:
  MIN_SIZE_TRAIN: (640, 700, 760, 820, 880, 940, 1000)
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MAX_SIZE_TEST: 4000
  CROP:
    ENABLED: False
  ROTATE: False

SOLVER:
  AMP:
    ENABLED: True  # Enable Automatic Mixed Precision
  IMS_PER_BATCH: 2
  BASE_LR: 1e-5
  LR_BACKBONE: 1e-6
  WARMUP_ITERS: 250
  STEPS: (15000, 25000)  # no step
  MAX_ITER: 30000
  GAMMA: 0.1
  CHECKPOINT_PERIOD: 1000

TEST:
  EVAL_PERIOD: 1000
  # 1 - Generic, 2 - Weak, 3 - Strong (for icdar2015)
  LEXICON_TYPE: 3

OUTPUT_DIR: "output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15"

[04/29 17:09:37] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ic15_test
  TRAIN:
  - ic15_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    CROP_INSTANCE: false
    ENABLED: false
    SIZE:
    - 0.1
    - 0.1
    TYPE: relative_range
  FORMAT: RGB
  HFLIP_TRAIN: false
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 4000
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MIN_SIZE_TRAIN:
  - 640
  - 700
  - 760
  - 820
  - 880
  - 940
  - 1000
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  ROTATE: false
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    ANTI_ALIAS: false
    FREEZE_AT: 2
    NAME: build_resnet_backbone
  BASIS_MODULE:
    ANN_SET: coco
    COMMON_STRIDE: 8
    CONVS_DIM: 128
    IN_FEATURES:
    - p3
    - p4
    - p5
    LOSS_ON: false
    LOSS_WEIGHT: 0.3
    NAME: ProtoNet
    NORM: SyncBN
    NUM_BASES: 4
    NUM_CLASSES: 80
    NUM_CONVS: 3
  BATEXT:
    CANONICAL_SIZE: 96
    CONV_DIM: 256
    CUSTOM_DICT: ''
    IN_FEATURES:
    - p2
    - p3
    - p4
    NUM_CHARS: 25
    NUM_CONV: 2
    POOLER_RESOLUTION:
    - 8
    - 32
    POOLER_SCALES:
    - 0.25
    - 0.125
    - 0.0625
    RECOGNITION_LOSS: ctc
    RECOGNIZER: attn
    SAMPLING_RATIO: 1
    USE_AET: false
    USE_COORDCONV: false
    VOC_SIZE: 96
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: false
  META_ARCHITECTURE: TransformerPureDetector
  MOBILENET: false
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_INTERVAL: 1
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  SWIN:
    DROP_PATH_RATE: 0.2
    TYPE: tiny
  TOP_MODULE:
    DIM: 16
    NAME: conv
  TRANSFORMER:
    AUX_LOSS: true
    BOUNDARY_HEAD: true
    CUSTOM_DICT: ''
    DEC_LAYERS: 6
    DEC_N_POINTS: 4
    DIM_FEEDFORWARD: 1024
    DROPOUT: 0.0
    ENABLED: true
    ENC_LAYERS: 6
    ENC_N_POINTS: 4
    HIDDEN_DIM: 256
    INFERENCE_TH_TEST: 0.01
    LOSS:
      AUX_LOSS: true
      BEZIER_CLASS_WEIGHT: 1.0
      BEZIER_COORD_WEIGHT: 1.0
      BEZIER_SAMPLE_POINTS: 25
      BOUNDARY_WEIGHT: 0.5
      FOCAL_ALPHA: 0.25
      FOCAL_GAMMA: 2.0
      POINT_CLASS_WEIGHT: 1.0
      POINT_COORD_WEIGHT: 1.0
      POINT_TEXT_WEIGHT: 0.5
    NHEADS: 8
    NUM_FEATURE_LEVELS: 4
    NUM_POINTS: 25
    NUM_QUERIES: 100
    POSITION_EMBEDDING_SCALE: 6.283185307179586
    TEMPERATURE: 10000
    VOC_SIZE: 37
  ViTAEv2:
    DROP_PATH_RATE: 0.2
    TYPE: vitaev2_s
  WEIGHTS: output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth
OUTPUT_DIR: output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15
SEED: 42
SOLVER:
  AMP:
    ENABLED: true
  BASE_LR: 1.0e-05
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.1
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 2
  LR_BACKBONE: 1.0e-06
  LR_BACKBONE_NAMES:
  - backbone.0
  LR_LINEAR_PROJ_MULT: 1.0
  LR_LINEAR_PROJ_NAMES:
  - reference_points
  - sampling_offsets
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 30000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 15000
  - 25000
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 250
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  LEXICON_TYPE: 3
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/29 17:09:37] detectron2 INFO: Full config saved to output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/config.yaml
[04/29 17:09:39] d2.engine.defaults INFO: Model:
TransformerPureDetector(
  (detection_transformer): DETECTION_TRANSFORMER(
    (backbone): Joiner(
      (0): MaskedBackbone(
        (backbone): ResNet(
          (stem): BasicStem(
            (conv1): Conv2d(
              3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
          )
          (res2): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv1): Conv2d(
                64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
          )
          (res3): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv1): Conv2d(
                256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
          )
          (res4): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
              (conv1): Conv2d(
                512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (4): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (5): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
          )
          (res5): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
              (conv1): Conv2d(
                1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
          )
        )
      )
      (1): PositionalEncoding2D()
    )
    (point_embed): Embedding(2500, 256)
    (transformer): DeformableTransformer(
      (encoder): DeformableTransformerEncoder(
        (layers): ModuleList(
          (0): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (decoder): DeformableCompositeTransformerDecoder(
        (layers): ModuleList(
          (0): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ctrl_point_coord): ModuleList(
          (0): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (1): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (2): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (3): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (4): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (5): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
        )
      )
      (enc_output): Linear(in_features=256, out_features=256, bias=True)
      (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (bezier_coord_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=8, bias=True)
        )
      )
      (bezier_class_embed): Linear(in_features=256, out_features=1, bias=True)
    )
    (input_proj): ModuleList(
      (0): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (1): Sequential(
        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (2): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (3): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (bezier_proposal_coord): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=8, bias=True)
      )
    )
    (bezier_proposal_class): Linear(in_features=256, out_features=1, bias=True)
    (ctrl_point_coord): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
    )
    (ctrl_point_class): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (ctrl_point_text): ModuleList(
      (0): Linear(in_features=256, out_features=38, bias=True)
      (1): Linear(in_features=256, out_features=38, bias=True)
      (2): Linear(in_features=256, out_features=38, bias=True)
      (3): Linear(in_features=256, out_features=38, bias=True)
      (4): Linear(in_features=256, out_features=38, bias=True)
      (5): Linear(in_features=256, out_features=38, bias=True)
    )
    (boundary_offset): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (enc_matcher): BezierHungarianMatcher(
      (bezier_sampler): BezierSampler()
    )
    (dec_matcher): CtrlPointHungarianMatcher()
    (bezier_sampler): BezierSampler()
  )
)
[04/29 17:09:39] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 700, 760, 820, 880, 940, 1000), max_size=2000, sample_style='choice'), RandomFlip()]
[04/29 17:09:39] adet.data.dataset_mapper INFO: Rebuilding the augmentations. The previous augmentations will be overridden.
[04/29 17:09:39] adet.data.detection_utils INFO: Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 700, 760, 820, 880, 940, 1000), max_size=2000, sample_style='choice'), RandomContrast(intensity_min=0.3, intensity_max=1.7), RandomBrightness(intensity_min=0.3, intensity_max=1.7), RandomLighting(scale=1.1394267984578836), RandomSaturation(intensity_min=0.3, intensity_max=1.7)]
[04/29 17:09:39] adet.data.datasets.text INFO: Loaded 1000 images in COCO format from datasets/ic15/train_37voc.json
[04/29 17:09:39] d2.data.build INFO: Removed 0 images with no usable annotations. 979 images left.
[04/29 17:09:39] d2.data.build INFO: Distribution of instances among all 1 categories:
[36m|  category  | #instances   |
|:----------:|:-------------|
|    text    | 4441         |
|            |              |[0m
[04/29 17:09:39] d2.data.build INFO: Using training sampler TrainingSampler
[04/29 17:09:39] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[04/29 17:09:39] d2.data.common INFO: Serializing 979 elements to byte tensors and concatenating them all ...
[04/29 17:09:39] d2.data.common INFO: Serialized dataset takes 6.51 MiB
[04/29 17:09:39] d2.data.build INFO: Making batched data loader with batch_size=2
[04/29 17:09:39] d2.engine.defaults INFO: Model:
TransformerPureDetector(
  (detection_transformer): DETECTION_TRANSFORMER(
    (backbone): Joiner(
      (0): MaskedBackbone(
        (backbone): ResNet(
          (stem): BasicStem(
            (conv1): Conv2d(
              3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
          )
          (res2): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv1): Conv2d(
                64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
          )
          (res3): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv1): Conv2d(
                256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
          )
          (res4): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
              (conv1): Conv2d(
                512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (4): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (5): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
          )
          (res5): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
              (conv1): Conv2d(
                1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
          )
        )
      )
      (1): PositionalEncoding2D()
    )
    (point_embed): Embedding(2500, 256)
    (transformer): DeformableTransformer(
      (encoder): DeformableTransformerEncoder(
        (layers): ModuleList(
          (0): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (decoder): DeformableCompositeTransformerDecoder(
        (layers): ModuleList(
          (0): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ctrl_point_coord): ModuleList(
          (0): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (1): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (2): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (3): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (4): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (5): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
        )
      )
      (enc_output): Linear(in_features=256, out_features=256, bias=True)
      (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (bezier_coord_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=8, bias=True)
        )
      )
      (bezier_class_embed): Linear(in_features=256, out_features=1, bias=True)
    )
    (input_proj): ModuleList(
      (0): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (1): Sequential(
        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (2): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (3): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (bezier_proposal_coord): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=8, bias=True)
      )
    )
    (bezier_proposal_class): Linear(in_features=256, out_features=1, bias=True)
    (ctrl_point_coord): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
    )
    (ctrl_point_class): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (ctrl_point_text): ModuleList(
      (0): Linear(in_features=256, out_features=38, bias=True)
      (1): Linear(in_features=256, out_features=38, bias=True)
      (2): Linear(in_features=256, out_features=38, bias=True)
      (3): Linear(in_features=256, out_features=38, bias=True)
      (4): Linear(in_features=256, out_features=38, bias=True)
      (5): Linear(in_features=256, out_features=38, bias=True)
    )
    (boundary_offset): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (enc_matcher): BezierHungarianMatcher(
      (bezier_sampler): BezierSampler()
    )
    (dec_matcher): CtrlPointHungarianMatcher()
    (bezier_sampler): BezierSampler()
  )
)
[04/29 17:09:40] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 700, 760, 820, 880, 940, 1000), max_size=2000, sample_style='choice'), RandomFlip()]
[04/29 17:09:40] adet.data.dataset_mapper INFO: Rebuilding the augmentations. The previous augmentations will be overridden.
[04/29 17:09:40] adet.data.detection_utils INFO: Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 700, 760, 820, 880, 940, 1000), max_size=2000, sample_style='choice'), RandomContrast(intensity_min=0.3, intensity_max=1.7), RandomBrightness(intensity_min=0.3, intensity_max=1.7), RandomLighting(scale=0.5250107552226669), RandomSaturation(intensity_min=0.3, intensity_max=1.7)]
[04/29 17:09:40] adet.data.datasets.text INFO: Loaded 1000 images in COCO format from datasets/ic15/train_37voc.json
[04/29 17:09:40] d2.data.build INFO: Removed 0 images with no usable annotations. 979 images left.
[04/29 17:09:40] d2.data.build INFO: Using training sampler TrainingSampler
[04/29 17:09:40] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[04/29 17:09:40] d2.data.common INFO: Serializing 979 elements to byte tensors and concatenating them all ...
[04/29 17:09:40] d2.data.common INFO: Serialized dataset takes 6.51 MiB
[04/29 17:09:40] d2.data.build INFO: Making batched data loader with batch_size=2
[04/29 17:09:40] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth ...
[04/29 17:09:40] fvcore.common.checkpoint INFO: [Checkpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth ...
[04/29 17:09:40] adet.trainer INFO: Starting training from iteration 0
[04/29 17:11:58] d2.utils.events INFO:  eta: 2 days, 9:48:50  iter: 19  loss_ce: 0.1484  loss_texts: 0.2085  loss_ctrl_points: 0.1134  loss_bd_points: 0.1527  loss_ce_0: 0.1713  loss_texts_0: 0.3088  loss_ctrl_points_0: 0.1185  loss_bd_points_0: 0.1675  loss_ce_1: 0.157  loss_texts_1: 0.2489  loss_ctrl_points_1: 0.1187  loss_bd_points_1: 0.1634  loss_ce_2: 0.1519  loss_texts_2: 0.234  loss_ctrl_points_2: 0.1172  loss_bd_points_2: 0.162  loss_ce_3: 0.1475  loss_texts_3: 0.2156  loss_ctrl_points_3: 0.1174  loss_bd_points_3: 0.1572  loss_ce_4: 0.1464  loss_texts_4: 0.2079  loss_ctrl_points_4: 0.1138  loss_bd_points_4: 0.1527  loss_ce_enc: 0.1626  loss_bezier_enc: 0.1128  total_loss: 4.458    time: 6.8958  last_time: 6.1885  data_time: 0.0071  last_data_time: 0.0034   lr: 1.5285e-06  max_mem: 6977M
[04/29 17:11:59] d2.utils.events INFO:  eta: 2 days, 9:48:56  iter: 19  loss_ce: 0.1484  loss_texts: 0.2085  loss_ctrl_points: 0.1134  loss_bd_points: 0.1527  loss_ce_0: 0.1713  loss_texts_0: 0.3088  loss_ctrl_points_0: 0.1185  loss_bd_points_0: 0.1675  loss_ce_1: 0.157  loss_texts_1: 0.2489  loss_ctrl_points_1: 0.1187  loss_bd_points_1: 0.1634  loss_ce_2: 0.1519  loss_texts_2: 0.234  loss_ctrl_points_2: 0.1172  loss_bd_points_2: 0.162  loss_ce_3: 0.1475  loss_texts_3: 0.2156  loss_ctrl_points_3: 0.1174  loss_bd_points_3: 0.1572  loss_ce_4: 0.1464  loss_texts_4: 0.2079  loss_ctrl_points_4: 0.1138  loss_bd_points_4: 0.1527  loss_ce_enc: 0.1626  loss_bezier_enc: 0.1128  total_loss: 4.458    time: 6.9093  last_time: 7.3789  data_time: 0.0071  last_data_time: 0.0034   lr: 1.5684e-06  max_mem: 6977M
[04/29 17:14:26] detectron2 INFO: Rank of current process: 0. World size: 1
[04/29 17:14:27] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/kylee/DeepSolo_LangPriors/detectron2/detectron2
Compiler                         GCC 9.5
CUDA compiler                    CUDA 11.3
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.12.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 3060 (arch=8.6)
Driver version                   550.120
CUDA_HOME                        /usr
Pillow                           9.0.1
torchvision                      0.13.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[04/29 17:14:27] detectron2 INFO: Command line arguments: Namespace(config_file='configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50152', opts=[])
[04/29 17:14:27] detectron2 INFO: Contents of args.config_file=configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml:
_BASE_: "../Base_det.yaml"

MODEL:
  #WEIGHTS: "output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth"
  WEIGHTS: "output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth"
  TRANSFORMER:
    INFERENCE_TH_TEST: 0.01

DATASETS:
  TRAIN: ("ic15_train",)
  TEST: ("ic15_test",)

INPUT:
  MIN_SIZE_TRAIN: (640, 700, 760, 820, 880, 940, 1000)
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MAX_SIZE_TEST: 4000
  CROP:
    ENABLED: False
  ROTATE: False

SOLVER:
  AMP:
    ENABLED: True  # Enable Automatic Mixed Precision
  IMS_PER_BATCH: 2
  BASE_LR: 1e-5
  LR_BACKBONE: 1e-6
  WARMUP_ITERS: 250
  STEPS: (15000, 25000)  # no step
  MAX_ITER: 30000
  GAMMA: 0.1
  CHECKPOINT_PERIOD: 1000

TEST:
  EVAL_PERIOD: 1000
  # 1 - Generic, 2 - Weak, 3 - Strong (for icdar2015)
  LEXICON_TYPE: 3

OUTPUT_DIR: "output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15"

[04/29 17:14:27] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ic15_test
  TRAIN:
  - ic15_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    CROP_INSTANCE: false
    ENABLED: false
    SIZE:
    - 0.1
    - 0.1
    TYPE: relative_range
  FORMAT: RGB
  HFLIP_TRAIN: false
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 4000
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MIN_SIZE_TRAIN:
  - 640
  - 700
  - 760
  - 820
  - 880
  - 940
  - 1000
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  ROTATE: false
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    ANTI_ALIAS: false
    FREEZE_AT: 2
    NAME: build_resnet_backbone
  BASIS_MODULE:
    ANN_SET: coco
    COMMON_STRIDE: 8
    CONVS_DIM: 128
    IN_FEATURES:
    - p3
    - p4
    - p5
    LOSS_ON: false
    LOSS_WEIGHT: 0.3
    NAME: ProtoNet
    NORM: SyncBN
    NUM_BASES: 4
    NUM_CLASSES: 80
    NUM_CONVS: 3
  BATEXT:
    CANONICAL_SIZE: 96
    CONV_DIM: 256
    CUSTOM_DICT: ''
    IN_FEATURES:
    - p2
    - p3
    - p4
    NUM_CHARS: 25
    NUM_CONV: 2
    POOLER_RESOLUTION:
    - 8
    - 32
    POOLER_SCALES:
    - 0.25
    - 0.125
    - 0.0625
    RECOGNITION_LOSS: ctc
    RECOGNIZER: attn
    SAMPLING_RATIO: 1
    USE_AET: false
    USE_COORDCONV: false
    VOC_SIZE: 96
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: false
  META_ARCHITECTURE: TransformerPureDetector
  MOBILENET: false
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_INTERVAL: 1
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  SWIN:
    DROP_PATH_RATE: 0.2
    TYPE: tiny
  TOP_MODULE:
    DIM: 16
    NAME: conv
  TRANSFORMER:
    AUX_LOSS: true
    BOUNDARY_HEAD: true
    CUSTOM_DICT: ''
    DEC_LAYERS: 6
    DEC_N_POINTS: 4
    DIM_FEEDFORWARD: 1024
    DROPOUT: 0.0
    ENABLED: true
    ENC_LAYERS: 6
    ENC_N_POINTS: 4
    HIDDEN_DIM: 256
    INFERENCE_TH_TEST: 0.01
    LOSS:
      AUX_LOSS: true
      BEZIER_CLASS_WEIGHT: 1.0
      BEZIER_COORD_WEIGHT: 1.0
      BEZIER_SAMPLE_POINTS: 25
      BOUNDARY_WEIGHT: 0.5
      FOCAL_ALPHA: 0.25
      FOCAL_GAMMA: 2.0
      POINT_CLASS_WEIGHT: 1.0
      POINT_COORD_WEIGHT: 1.0
      POINT_TEXT_WEIGHT: 0.5
    NHEADS: 8
    NUM_FEATURE_LEVELS: 4
    NUM_POINTS: 25
    NUM_QUERIES: 100
    POSITION_EMBEDDING_SCALE: 6.283185307179586
    TEMPERATURE: 10000
    VOC_SIZE: 37
  ViTAEv2:
    DROP_PATH_RATE: 0.2
    TYPE: vitaev2_s
  WEIGHTS: output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth
OUTPUT_DIR: output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15
SEED: 42
SOLVER:
  AMP:
    ENABLED: true
  BASE_LR: 1.0e-05
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.1
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 2
  LR_BACKBONE: 1.0e-06
  LR_BACKBONE_NAMES:
  - backbone.0
  LR_LINEAR_PROJ_MULT: 1.0
  LR_LINEAR_PROJ_NAMES:
  - reference_points
  - sampling_offsets
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 30000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 15000
  - 25000
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 250
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  LEXICON_TYPE: 3
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/29 17:14:27] detectron2 INFO: Full config saved to output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/config.yaml
[04/29 17:14:28] d2.engine.defaults INFO: Model:
TransformerPureDetector(
  (detection_transformer): DETECTION_TRANSFORMER(
    (backbone): Joiner(
      (0): MaskedBackbone(
        (backbone): ResNet(
          (stem): BasicStem(
            (conv1): Conv2d(
              3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
          )
          (res2): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv1): Conv2d(
                64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
          )
          (res3): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv1): Conv2d(
                256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
          )
          (res4): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
              (conv1): Conv2d(
                512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (4): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (5): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
          )
          (res5): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
              (conv1): Conv2d(
                1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
          )
        )
      )
      (1): PositionalEncoding2D()
    )
    (point_embed): Embedding(2500, 256)
    (transformer): DeformableTransformer(
      (encoder): DeformableTransformerEncoder(
        (layers): ModuleList(
          (0): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (decoder): DeformableCompositeTransformerDecoder(
        (layers): ModuleList(
          (0): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ctrl_point_coord): ModuleList(
          (0): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (1): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (2): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (3): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (4): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (5): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
        )
      )
      (enc_output): Linear(in_features=256, out_features=256, bias=True)
      (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (bezier_coord_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=8, bias=True)
        )
      )
      (bezier_class_embed): Linear(in_features=256, out_features=1, bias=True)
    )
    (input_proj): ModuleList(
      (0): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (1): Sequential(
        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (2): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (3): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (bezier_proposal_coord): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=8, bias=True)
      )
    )
    (bezier_proposal_class): Linear(in_features=256, out_features=1, bias=True)
    (ctrl_point_coord): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
    )
    (ctrl_point_class): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (ctrl_point_text): ModuleList(
      (0): Linear(in_features=256, out_features=38, bias=True)
      (1): Linear(in_features=256, out_features=38, bias=True)
      (2): Linear(in_features=256, out_features=38, bias=True)
      (3): Linear(in_features=256, out_features=38, bias=True)
      (4): Linear(in_features=256, out_features=38, bias=True)
      (5): Linear(in_features=256, out_features=38, bias=True)
    )
    (boundary_offset): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (enc_matcher): BezierHungarianMatcher(
      (bezier_sampler): BezierSampler()
    )
    (dec_matcher): CtrlPointHungarianMatcher()
    (bezier_sampler): BezierSampler()
  )
)
[04/29 17:14:28] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 700, 760, 820, 880, 940, 1000), max_size=2000, sample_style='choice'), RandomFlip()]
[04/29 17:14:28] adet.data.dataset_mapper INFO: Rebuilding the augmentations. The previous augmentations will be overridden.
[04/29 17:14:28] adet.data.detection_utils INFO: Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 700, 760, 820, 880, 940, 1000), max_size=2000, sample_style='choice'), RandomContrast(intensity_min=0.3, intensity_max=1.7), RandomBrightness(intensity_min=0.3, intensity_max=1.7), RandomLighting(scale=1.1394267984578836), RandomSaturation(intensity_min=0.3, intensity_max=1.7)]
[04/29 17:14:28] adet.data.datasets.text INFO: Loaded 1000 images in COCO format from datasets/ic15/train_37voc.json
[04/29 17:14:29] d2.data.build INFO: Removed 0 images with no usable annotations. 979 images left.
[04/29 17:14:29] d2.data.build INFO: Distribution of instances among all 1 categories:
[36m|  category  | #instances   |
|:----------:|:-------------|
|    text    | 4441         |
|            |              |[0m
[04/29 17:14:29] d2.data.build INFO: Using training sampler TrainingSampler
[04/29 17:14:29] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[04/29 17:14:29] d2.data.common INFO: Serializing 979 elements to byte tensors and concatenating them all ...
[04/29 17:14:29] d2.data.common INFO: Serialized dataset takes 6.51 MiB
[04/29 17:14:29] d2.data.build INFO: Making batched data loader with batch_size=2
[04/29 17:14:29] d2.engine.defaults INFO: Model:
TransformerPureDetector(
  (detection_transformer): DETECTION_TRANSFORMER(
    (backbone): Joiner(
      (0): MaskedBackbone(
        (backbone): ResNet(
          (stem): BasicStem(
            (conv1): Conv2d(
              3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
          )
          (res2): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv1): Conv2d(
                64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
          )
          (res3): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv1): Conv2d(
                256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
          )
          (res4): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
              (conv1): Conv2d(
                512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (4): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (5): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
          )
          (res5): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
              (conv1): Conv2d(
                1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
          )
        )
      )
      (1): PositionalEncoding2D()
    )
    (point_embed): Embedding(2500, 256)
    (transformer): DeformableTransformer(
      (encoder): DeformableTransformerEncoder(
        (layers): ModuleList(
          (0): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (decoder): DeformableCompositeTransformerDecoder(
        (layers): ModuleList(
          (0): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ctrl_point_coord): ModuleList(
          (0): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (1): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (2): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (3): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (4): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (5): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
        )
      )
      (enc_output): Linear(in_features=256, out_features=256, bias=True)
      (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (bezier_coord_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=8, bias=True)
        )
      )
      (bezier_class_embed): Linear(in_features=256, out_features=1, bias=True)
    )
    (input_proj): ModuleList(
      (0): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (1): Sequential(
        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (2): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (3): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (bezier_proposal_coord): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=8, bias=True)
      )
    )
    (bezier_proposal_class): Linear(in_features=256, out_features=1, bias=True)
    (ctrl_point_coord): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
    )
    (ctrl_point_class): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (ctrl_point_text): ModuleList(
      (0): Linear(in_features=256, out_features=38, bias=True)
      (1): Linear(in_features=256, out_features=38, bias=True)
      (2): Linear(in_features=256, out_features=38, bias=True)
      (3): Linear(in_features=256, out_features=38, bias=True)
      (4): Linear(in_features=256, out_features=38, bias=True)
      (5): Linear(in_features=256, out_features=38, bias=True)
    )
    (boundary_offset): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (enc_matcher): BezierHungarianMatcher(
      (bezier_sampler): BezierSampler()
    )
    (dec_matcher): CtrlPointHungarianMatcher()
    (bezier_sampler): BezierSampler()
  )
)
[04/29 17:14:29] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 700, 760, 820, 880, 940, 1000), max_size=2000, sample_style='choice'), RandomFlip()]
[04/29 17:14:29] adet.data.dataset_mapper INFO: Rebuilding the augmentations. The previous augmentations will be overridden.
[04/29 17:14:29] adet.data.detection_utils INFO: Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 700, 760, 820, 880, 940, 1000), max_size=2000, sample_style='choice'), RandomContrast(intensity_min=0.3, intensity_max=1.7), RandomBrightness(intensity_min=0.3, intensity_max=1.7), RandomLighting(scale=0.5250107552226669), RandomSaturation(intensity_min=0.3, intensity_max=1.7)]
[04/29 17:14:29] adet.data.datasets.text INFO: Loaded 1000 images in COCO format from datasets/ic15/train_37voc.json
[04/29 17:14:29] d2.data.build INFO: Removed 0 images with no usable annotations. 979 images left.
[04/29 17:14:29] d2.data.build INFO: Using training sampler TrainingSampler
[04/29 17:14:29] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[04/29 17:14:29] d2.data.common INFO: Serializing 979 elements to byte tensors and concatenating them all ...
[04/29 17:14:29] d2.data.common INFO: Serialized dataset takes 6.51 MiB
[04/29 17:14:29] d2.data.build INFO: Making batched data loader with batch_size=2
[04/29 17:14:29] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth ...
[04/29 17:14:29] fvcore.common.checkpoint INFO: [Checkpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth ...
[04/29 17:14:29] adet.trainer INFO: Starting training from iteration 0
[04/29 17:25:15] detectron2 INFO: Rank of current process: 0. World size: 1
[04/29 17:25:15] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/kylee/DeepSolo_LangPriors/detectron2/detectron2
Compiler                         GCC 9.5
CUDA compiler                    CUDA 11.3
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.12.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 3060 (arch=8.6)
Driver version                   550.120
CUDA_HOME                        /usr
Pillow                           9.0.1
torchvision                      0.13.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[04/29 17:25:15] detectron2 INFO: Command line arguments: Namespace(config_file='configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50152', opts=[])
[04/29 17:25:15] detectron2 INFO: Contents of args.config_file=configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml:
_BASE_: "../Base_det.yaml"

MODEL:
  WEIGHTS: "output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth"
  #WEIGHTS: "output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth"
  TRANSFORMER:
    INFERENCE_TH_TEST: 0.2

DATASETS:
  TRAIN: ("ic15_train",)
  TEST: ("ic15_test",)

INPUT:
  MIN_SIZE_TRAIN: (640, 700, 760, 820, 880, 940, 1000)
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MAX_SIZE_TEST: 4000
  CROP:
    ENABLED: False
  ROTATE: False

SOLVER:
  AMP:
    ENABLED: True  # Enable Automatic Mixed Precision
  IMS_PER_BATCH: 2
  BASE_LR: 1e-5
  LR_BACKBONE: 1e-6
  WARMUP_ITERS: 250
  STEPS: (15000, 25000)  # no step
  MAX_ITER: 30000
  GAMMA: 0.1
  CHECKPOINT_PERIOD: 1000

TEST:
  EVAL_PERIOD: 1000
  # 1 - Generic, 2 - Weak, 3 - Strong (for icdar2015)
  LEXICON_TYPE: 3

OUTPUT_DIR: "output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15"

[04/29 17:25:15] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ic15_test
  TRAIN:
  - ic15_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    CROP_INSTANCE: false
    ENABLED: false
    SIZE:
    - 0.1
    - 0.1
    TYPE: relative_range
  FORMAT: RGB
  HFLIP_TRAIN: false
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 4000
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MIN_SIZE_TRAIN:
  - 640
  - 700
  - 760
  - 820
  - 880
  - 940
  - 1000
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  ROTATE: false
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    ANTI_ALIAS: false
    FREEZE_AT: 2
    NAME: build_resnet_backbone
  BASIS_MODULE:
    ANN_SET: coco
    COMMON_STRIDE: 8
    CONVS_DIM: 128
    IN_FEATURES:
    - p3
    - p4
    - p5
    LOSS_ON: false
    LOSS_WEIGHT: 0.3
    NAME: ProtoNet
    NORM: SyncBN
    NUM_BASES: 4
    NUM_CLASSES: 80
    NUM_CONVS: 3
  BATEXT:
    CANONICAL_SIZE: 96
    CONV_DIM: 256
    CUSTOM_DICT: ''
    IN_FEATURES:
    - p2
    - p3
    - p4
    NUM_CHARS: 25
    NUM_CONV: 2
    POOLER_RESOLUTION:
    - 8
    - 32
    POOLER_SCALES:
    - 0.25
    - 0.125
    - 0.0625
    RECOGNITION_LOSS: ctc
    RECOGNIZER: attn
    SAMPLING_RATIO: 1
    USE_AET: false
    USE_COORDCONV: false
    VOC_SIZE: 96
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: false
  META_ARCHITECTURE: TransformerPureDetector
  MOBILENET: false
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_INTERVAL: 1
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  SWIN:
    DROP_PATH_RATE: 0.2
    TYPE: tiny
  TOP_MODULE:
    DIM: 16
    NAME: conv
  TRANSFORMER:
    AUX_LOSS: true
    BOUNDARY_HEAD: true
    CUSTOM_DICT: ''
    DEC_LAYERS: 6
    DEC_N_POINTS: 4
    DIM_FEEDFORWARD: 1024
    DROPOUT: 0.0
    ENABLED: true
    ENC_LAYERS: 6
    ENC_N_POINTS: 4
    HIDDEN_DIM: 256
    INFERENCE_TH_TEST: 0.2
    LOSS:
      AUX_LOSS: true
      BEZIER_CLASS_WEIGHT: 1.0
      BEZIER_COORD_WEIGHT: 1.0
      BEZIER_SAMPLE_POINTS: 25
      BOUNDARY_WEIGHT: 0.5
      FOCAL_ALPHA: 0.25
      FOCAL_GAMMA: 2.0
      POINT_CLASS_WEIGHT: 1.0
      POINT_COORD_WEIGHT: 1.0
      POINT_TEXT_WEIGHT: 0.5
    NHEADS: 8
    NUM_FEATURE_LEVELS: 4
    NUM_POINTS: 25
    NUM_QUERIES: 100
    POSITION_EMBEDDING_SCALE: 6.283185307179586
    TEMPERATURE: 10000
    VOC_SIZE: 37
  ViTAEv2:
    DROP_PATH_RATE: 0.2
    TYPE: vitaev2_s
  WEIGHTS: output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth
OUTPUT_DIR: output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15
SEED: 42
SOLVER:
  AMP:
    ENABLED: true
  BASE_LR: 1.0e-05
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.1
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 2
  LR_BACKBONE: 1.0e-06
  LR_BACKBONE_NAMES:
  - backbone.0
  LR_LINEAR_PROJ_MULT: 1.0
  LR_LINEAR_PROJ_NAMES:
  - reference_points
  - sampling_offsets
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 30000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 15000
  - 25000
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 250
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  LEXICON_TYPE: 3
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/29 17:25:15] detectron2 INFO: Full config saved to output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/config.yaml
[04/29 17:25:17] d2.engine.defaults INFO: Model:
TransformerPureDetector(
  (detection_transformer): DETECTION_TRANSFORMER(
    (backbone): Joiner(
      (0): MaskedBackbone(
        (backbone): ResNet(
          (stem): BasicStem(
            (conv1): Conv2d(
              3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
          )
          (res2): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv1): Conv2d(
                64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
          )
          (res3): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv1): Conv2d(
                256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
          )
          (res4): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
              (conv1): Conv2d(
                512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (4): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (5): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
          )
          (res5): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
              (conv1): Conv2d(
                1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
          )
        )
      )
      (1): PositionalEncoding2D()
    )
    (point_embed): Embedding(2500, 256)
    (transformer): DeformableTransformer(
      (encoder): DeformableTransformerEncoder(
        (layers): ModuleList(
          (0): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (decoder): DeformableCompositeTransformerDecoder(
        (layers): ModuleList(
          (0): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ctrl_point_coord): ModuleList(
          (0): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (1): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (2): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (3): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (4): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (5): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
        )
      )
      (enc_output): Linear(in_features=256, out_features=256, bias=True)
      (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (bezier_coord_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=8, bias=True)
        )
      )
      (bezier_class_embed): Linear(in_features=256, out_features=1, bias=True)
    )
    (input_proj): ModuleList(
      (0): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (1): Sequential(
        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (2): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (3): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (bezier_proposal_coord): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=8, bias=True)
      )
    )
    (bezier_proposal_class): Linear(in_features=256, out_features=1, bias=True)
    (ctrl_point_coord): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
    )
    (ctrl_point_class): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (ctrl_point_text): ModuleList(
      (0): Linear(in_features=256, out_features=38, bias=True)
      (1): Linear(in_features=256, out_features=38, bias=True)
      (2): Linear(in_features=256, out_features=38, bias=True)
      (3): Linear(in_features=256, out_features=38, bias=True)
      (4): Linear(in_features=256, out_features=38, bias=True)
      (5): Linear(in_features=256, out_features=38, bias=True)
    )
    (boundary_offset): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (enc_matcher): BezierHungarianMatcher(
      (bezier_sampler): BezierSampler()
    )
    (dec_matcher): CtrlPointHungarianMatcher()
    (bezier_sampler): BezierSampler()
  )
)
[04/29 17:25:17] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 700, 760, 820, 880, 940, 1000), max_size=2000, sample_style='choice'), RandomFlip()]
[04/29 17:25:17] adet.data.dataset_mapper INFO: Rebuilding the augmentations. The previous augmentations will be overridden.
[04/29 17:25:17] adet.data.detection_utils INFO: Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 700, 760, 820, 880, 940, 1000), max_size=2000, sample_style='choice'), RandomContrast(intensity_min=0.3, intensity_max=1.7), RandomBrightness(intensity_min=0.3, intensity_max=1.7), RandomLighting(scale=1.1394267984578836), RandomSaturation(intensity_min=0.3, intensity_max=1.7)]
[04/29 17:25:17] adet.data.datasets.text INFO: Loaded 1000 images in COCO format from datasets/ic15/train_37voc.json
[04/29 17:25:17] d2.data.build INFO: Removed 0 images with no usable annotations. 979 images left.
[04/29 17:25:17] d2.data.build INFO: Distribution of instances among all 1 categories:
[36m|  category  | #instances   |
|:----------:|:-------------|
|    text    | 4441         |
|            |              |[0m
[04/29 17:25:17] d2.data.build INFO: Using training sampler TrainingSampler
[04/29 17:25:17] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[04/29 17:25:17] d2.data.common INFO: Serializing 979 elements to byte tensors and concatenating them all ...
[04/29 17:25:17] d2.data.common INFO: Serialized dataset takes 6.51 MiB
[04/29 17:25:17] d2.data.build INFO: Making batched data loader with batch_size=2
[04/29 17:25:17] d2.engine.defaults INFO: Model:
TransformerPureDetector(
  (detection_transformer): DETECTION_TRANSFORMER(
    (backbone): Joiner(
      (0): MaskedBackbone(
        (backbone): ResNet(
          (stem): BasicStem(
            (conv1): Conv2d(
              3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
          )
          (res2): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv1): Conv2d(
                64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
          )
          (res3): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv1): Conv2d(
                256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
          )
          (res4): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
              (conv1): Conv2d(
                512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (4): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (5): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
          )
          (res5): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
              (conv1): Conv2d(
                1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
          )
        )
      )
      (1): PositionalEncoding2D()
    )
    (point_embed): Embedding(2500, 256)
    (transformer): DeformableTransformer(
      (encoder): DeformableTransformerEncoder(
        (layers): ModuleList(
          (0): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (decoder): DeformableCompositeTransformerDecoder(
        (layers): ModuleList(
          (0): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ctrl_point_coord): ModuleList(
          (0): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (1): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (2): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (3): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (4): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (5): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
        )
      )
      (enc_output): Linear(in_features=256, out_features=256, bias=True)
      (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (bezier_coord_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=8, bias=True)
        )
      )
      (bezier_class_embed): Linear(in_features=256, out_features=1, bias=True)
    )
    (input_proj): ModuleList(
      (0): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (1): Sequential(
        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (2): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (3): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (bezier_proposal_coord): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=8, bias=True)
      )
    )
    (bezier_proposal_class): Linear(in_features=256, out_features=1, bias=True)
    (ctrl_point_coord): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
    )
    (ctrl_point_class): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (ctrl_point_text): ModuleList(
      (0): Linear(in_features=256, out_features=38, bias=True)
      (1): Linear(in_features=256, out_features=38, bias=True)
      (2): Linear(in_features=256, out_features=38, bias=True)
      (3): Linear(in_features=256, out_features=38, bias=True)
      (4): Linear(in_features=256, out_features=38, bias=True)
      (5): Linear(in_features=256, out_features=38, bias=True)
    )
    (boundary_offset): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (enc_matcher): BezierHungarianMatcher(
      (bezier_sampler): BezierSampler()
    )
    (dec_matcher): CtrlPointHungarianMatcher()
    (bezier_sampler): BezierSampler()
  )
)
[04/29 17:25:17] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 700, 760, 820, 880, 940, 1000), max_size=2000, sample_style='choice'), RandomFlip()]
[04/29 17:25:17] adet.data.dataset_mapper INFO: Rebuilding the augmentations. The previous augmentations will be overridden.
[04/29 17:25:17] adet.data.detection_utils INFO: Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 700, 760, 820, 880, 940, 1000), max_size=2000, sample_style='choice'), RandomContrast(intensity_min=0.3, intensity_max=1.7), RandomBrightness(intensity_min=0.3, intensity_max=1.7), RandomLighting(scale=0.5250107552226669), RandomSaturation(intensity_min=0.3, intensity_max=1.7)]
[04/29 17:25:17] adet.data.datasets.text INFO: Loaded 1000 images in COCO format from datasets/ic15/train_37voc.json
[04/29 17:25:17] d2.data.build INFO: Removed 0 images with no usable annotations. 979 images left.
[04/29 17:25:17] d2.data.build INFO: Using training sampler TrainingSampler
[04/29 17:25:17] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[04/29 17:25:17] d2.data.common INFO: Serializing 979 elements to byte tensors and concatenating them all ...
[04/29 17:25:17] d2.data.common INFO: Serialized dataset takes 6.51 MiB
[04/29 17:25:17] d2.data.build INFO: Making batched data loader with batch_size=2
[04/29 17:25:17] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth ...
[04/29 17:25:17] fvcore.common.checkpoint INFO: [Checkpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth ...
[04/29 17:25:18] adet.trainer INFO: Starting training from iteration 0
[04/29 17:27:42] d2.utils.events INFO:  eta: 2 days, 12:22:15  iter: 19  loss_ce: 0.1484  loss_texts: 0.2085  loss_ctrl_points: 0.1134  loss_bd_points: 0.1527  loss_ce_0: 0.1713  loss_texts_0: 0.3088  loss_ctrl_points_0: 0.1185  loss_bd_points_0: 0.1675  loss_ce_1: 0.157  loss_texts_1: 0.2489  loss_ctrl_points_1: 0.1187  loss_bd_points_1: 0.1634  loss_ce_2: 0.1519  loss_texts_2: 0.234  loss_ctrl_points_2: 0.1172  loss_bd_points_2: 0.162  loss_ce_3: 0.1475  loss_texts_3: 0.2156  loss_ctrl_points_3: 0.1174  loss_bd_points_3: 0.1572  loss_ce_4: 0.1464  loss_texts_4: 0.2079  loss_ctrl_points_4: 0.1138  loss_bd_points_4: 0.1527  loss_ce_enc: 0.1626  loss_bezier_enc: 0.1128  total_loss: 4.458    time: 7.2065  last_time: 5.8006  data_time: 0.0072  last_data_time: 0.0036   lr: 1.5285e-06  max_mem: 6977M
[04/29 17:27:42] d2.utils.events INFO:  eta: 2 days, 11:50:47  iter: 19  loss_ce: 0.1484  loss_texts: 0.2085  loss_ctrl_points: 0.1134  loss_bd_points: 0.1527  loss_ce_0: 0.1713  loss_texts_0: 0.3088  loss_ctrl_points_0: 0.1185  loss_bd_points_0: 0.1675  loss_ce_1: 0.157  loss_texts_1: 0.2489  loss_ctrl_points_1: 0.1187  loss_bd_points_1: 0.1634  loss_ce_2: 0.1519  loss_texts_2: 0.234  loss_ctrl_points_2: 0.1172  loss_bd_points_2: 0.162  loss_ce_3: 0.1475  loss_texts_3: 0.2156  loss_ctrl_points_3: 0.1174  loss_bd_points_3: 0.1572  loss_ce_4: 0.1464  loss_texts_4: 0.2079  loss_ctrl_points_4: 0.1138  loss_bd_points_4: 0.1527  loss_ce_enc: 0.1626  loss_bezier_enc: 0.1128  total_loss: 4.458    time: 7.1701  last_time: 5.8939  data_time: 0.0072  last_data_time: 0.0036   lr: 1.5684e-06  max_mem: 6977M
[04/29 17:29:58] d2.utils.events INFO:  eta: 2 days, 10:08:48  iter: 39  loss_ce: 0.1612  loss_texts: 0.2215  loss_ctrl_points: 0.1365  loss_bd_points: 0.1932  loss_ce_0: 0.1991  loss_texts_0: 0.3283  loss_ctrl_points_0: 0.1361  loss_bd_points_0: 0.2055  loss_ce_1: 0.1746  loss_texts_1: 0.2733  loss_ctrl_points_1: 0.1401  loss_bd_points_1: 0.2036  loss_ce_2: 0.1694  loss_texts_2: 0.2335  loss_ctrl_points_2: 0.1395  loss_bd_points_2: 0.203  loss_ce_3: 0.1669  loss_texts_3: 0.2274  loss_ctrl_points_3: 0.1361  loss_bd_points_3: 0.1932  loss_ce_4: 0.1629  loss_texts_4: 0.2242  loss_ctrl_points_4: 0.1371  loss_bd_points_4: 0.1934  loss_ce_enc: 0.1927  loss_bezier_enc: 0.1137  total_loss: 4.887    time: 6.9558  last_time: 6.8939  data_time: 0.0036  last_data_time: 0.0037   lr: 3.1269e-06  max_mem: 6977M
[04/29 17:29:58] d2.utils.events INFO:  eta: 2 days, 9:58:50  iter: 39  loss_ce: 0.1612  loss_texts: 0.2215  loss_ctrl_points: 0.1365  loss_bd_points: 0.1932  loss_ce_0: 0.1991  loss_texts_0: 0.3283  loss_ctrl_points_0: 0.1361  loss_bd_points_0: 0.2055  loss_ce_1: 0.1746  loss_texts_1: 0.2733  loss_ctrl_points_1: 0.1401  loss_bd_points_1: 0.2036  loss_ce_2: 0.1694  loss_texts_2: 0.2335  loss_ctrl_points_2: 0.1395  loss_bd_points_2: 0.203  loss_ce_3: 0.1669  loss_texts_3: 0.2274  loss_ctrl_points_3: 0.1361  loss_bd_points_3: 0.1932  loss_ce_4: 0.1629  loss_texts_4: 0.2242  loss_ctrl_points_4: 0.1371  loss_bd_points_4: 0.1934  loss_ce_enc: 0.1927  loss_bezier_enc: 0.1137  total_loss: 4.887    time: 6.9557  last_time: 6.9470  data_time: 0.0036  last_data_time: 0.0037   lr: 3.1668e-06  max_mem: 6977M
[04/29 17:32:23] d2.utils.events INFO:  eta: 2 days, 10:57:24  iter: 59  loss_ce: 0.1559  loss_texts: 0.1757  loss_ctrl_points: 0.1123  loss_bd_points: 0.15  loss_ce_0: 0.1839  loss_texts_0: 0.2959  loss_ctrl_points_0: 0.1167  loss_bd_points_0: 0.162  loss_ce_1: 0.1758  loss_texts_1: 0.2378  loss_ctrl_points_1: 0.1112  loss_bd_points_1: 0.157  loss_ce_2: 0.1637  loss_texts_2: 0.2093  loss_ctrl_points_2: 0.1135  loss_bd_points_2: 0.1552  loss_ce_3: 0.1592  loss_texts_3: 0.1854  loss_ctrl_points_3: 0.1122  loss_bd_points_3: 0.1534  loss_ce_4: 0.1548  loss_texts_4: 0.1756  loss_ctrl_points_4: 0.1131  loss_bd_points_4: 0.1485  loss_ce_enc: 0.1716  loss_bezier_enc: 0.1096  total_loss: 4.432    time: 7.0505  last_time: 8.1007  data_time: 0.0038  last_data_time: 0.0038   lr: 4.7253e-06  max_mem: 6977M
[04/29 17:32:23] d2.utils.events INFO:  eta: 2 days, 11:00:51  iter: 59  loss_ce: 0.1559  loss_texts: 0.1757  loss_ctrl_points: 0.1123  loss_bd_points: 0.15  loss_ce_0: 0.1839  loss_texts_0: 0.2959  loss_ctrl_points_0: 0.1167  loss_bd_points_0: 0.162  loss_ce_1: 0.1758  loss_texts_1: 0.2378  loss_ctrl_points_1: 0.1112  loss_bd_points_1: 0.157  loss_ce_2: 0.1637  loss_texts_2: 0.2093  loss_ctrl_points_2: 0.1135  loss_bd_points_2: 0.1552  loss_ce_3: 0.1592  loss_texts_3: 0.1854  loss_ctrl_points_3: 0.1122  loss_bd_points_3: 0.1534  loss_ce_4: 0.1548  loss_texts_4: 0.1756  loss_ctrl_points_4: 0.1131  loss_bd_points_4: 0.1485  loss_ce_enc: 0.1716  loss_bezier_enc: 0.1096  total_loss: 4.432    time: 7.0599  last_time: 8.1416  data_time: 0.0038  last_data_time: 0.0038   lr: 4.7652e-06  max_mem: 6977M
[04/29 17:34:55] d2.utils.events INFO:  eta: 2 days, 12:15:12  iter: 79  loss_ce: 0.1569  loss_texts: 0.2264  loss_ctrl_points: 0.1188  loss_bd_points: 0.1561  loss_ce_0: 0.178  loss_texts_0: 0.3103  loss_ctrl_points_0: 0.1265  loss_bd_points_0: 0.1549  loss_ce_1: 0.1689  loss_texts_1: 0.2781  loss_ctrl_points_1: 0.1206  loss_bd_points_1: 0.15  loss_ce_2: 0.1686  loss_texts_2: 0.2508  loss_ctrl_points_2: 0.1155  loss_bd_points_2: 0.1502  loss_ce_3: 0.1644  loss_texts_3: 0.246  loss_ctrl_points_3: 0.1203  loss_bd_points_3: 0.157  loss_ce_4: 0.1603  loss_texts_4: 0.2268  loss_ctrl_points_4: 0.119  loss_bd_points_4: 0.1567  loss_ce_enc: 0.1682  loss_bezier_enc: 0.1066  total_loss: 4.459    time: 7.1963  last_time: 7.4629  data_time: 0.0038  last_data_time: 0.0035   lr: 6.3237e-06  max_mem: 6977M
[04/29 17:34:55] d2.utils.events INFO:  eta: 2 days, 12:16:58  iter: 79  loss_ce: 0.1569  loss_texts: 0.2264  loss_ctrl_points: 0.1188  loss_bd_points: 0.1561  loss_ce_0: 0.178  loss_texts_0: 0.3103  loss_ctrl_points_0: 0.1265  loss_bd_points_0: 0.1549  loss_ce_1: 0.1689  loss_texts_1: 0.2781  loss_ctrl_points_1: 0.1206  loss_bd_points_1: 0.15  loss_ce_2: 0.1686  loss_texts_2: 0.2508  loss_ctrl_points_2: 0.1155  loss_bd_points_2: 0.1502  loss_ce_3: 0.1644  loss_texts_3: 0.246  loss_ctrl_points_3: 0.1203  loss_bd_points_3: 0.157  loss_ce_4: 0.1603  loss_texts_4: 0.2268  loss_ctrl_points_4: 0.119  loss_bd_points_4: 0.1567  loss_ce_enc: 0.1682  loss_bezier_enc: 0.1066  total_loss: 4.459    time: 7.1986  last_time: 7.5472  data_time: 0.0038  last_data_time: 0.0035   lr: 6.3636e-06  max_mem: 6977M
[04/29 17:37:23] d2.utils.events INFO:  eta: 2 days, 12:30:50  iter: 99  loss_ce: 0.1536  loss_texts: 0.1709  loss_ctrl_points: 0.1006  loss_bd_points: 0.1431  loss_ce_0: 0.1742  loss_texts_0: 0.2681  loss_ctrl_points_0: 0.1066  loss_bd_points_0: 0.1517  loss_ce_1: 0.1668  loss_texts_1: 0.2214  loss_ctrl_points_1: 0.1043  loss_bd_points_1: 0.1445  loss_ce_2: 0.1637  loss_texts_2: 0.1884  loss_ctrl_points_2: 0.1032  loss_bd_points_2: 0.1447  loss_ce_3: 0.1598  loss_texts_3: 0.1762  loss_ctrl_points_3: 0.1022  loss_bd_points_3: 0.1427  loss_ce_4: 0.1554  loss_texts_4: 0.169  loss_ctrl_points_4: 0.102  loss_bd_points_4: 0.144  loss_ce_enc: 0.1608  loss_bezier_enc: 0.105  total_loss: 3.903    time: 7.2452  last_time: 6.3398  data_time: 0.0038  last_data_time: 0.0038   lr: 7.9221e-06  max_mem: 6977M
[04/29 17:37:23] d2.utils.events INFO:  eta: 2 days, 12:28:36  iter: 99  loss_ce: 0.1536  loss_texts: 0.1709  loss_ctrl_points: 0.1006  loss_bd_points: 0.1431  loss_ce_0: 0.1742  loss_texts_0: 0.2681  loss_ctrl_points_0: 0.1066  loss_bd_points_0: 0.1517  loss_ce_1: 0.1668  loss_texts_1: 0.2214  loss_ctrl_points_1: 0.1043  loss_bd_points_1: 0.1445  loss_ce_2: 0.1637  loss_texts_2: 0.1884  loss_ctrl_points_2: 0.1032  loss_bd_points_2: 0.1447  loss_ce_3: 0.1598  loss_texts_3: 0.1762  loss_ctrl_points_3: 0.1022  loss_bd_points_3: 0.1427  loss_ce_4: 0.1554  loss_texts_4: 0.169  loss_ctrl_points_4: 0.102  loss_bd_points_4: 0.144  loss_ce_enc: 0.1608  loss_bezier_enc: 0.105  total_loss: 3.903    time: 7.2409  last_time: 6.4023  data_time: 0.0038  last_data_time: 0.0038   lr: 7.962e-06  max_mem: 6977M
[04/29 17:39:55] d2.utils.events INFO:  eta: 2 days, 13:27:38  iter: 119  loss_ce: 0.1526  loss_texts: 0.2007  loss_ctrl_points: 0.1197  loss_bd_points: 0.1652  loss_ce_0: 0.1738  loss_texts_0: 0.3059  loss_ctrl_points_0: 0.1162  loss_bd_points_0: 0.1545  loss_ce_1: 0.1719  loss_texts_1: 0.2559  loss_ctrl_points_1: 0.1222  loss_bd_points_1: 0.17  loss_ce_2: 0.1592  loss_texts_2: 0.2241  loss_ctrl_points_2: 0.1243  loss_bd_points_2: 0.1842  loss_ce_3: 0.1556  loss_texts_3: 0.206  loss_ctrl_points_3: 0.1208  loss_bd_points_3: 0.1716  loss_ce_4: 0.1534  loss_texts_4: 0.2012  loss_ctrl_points_4: 0.1201  loss_bd_points_4: 0.1652  loss_ce_enc: 0.1698  loss_bezier_enc: 0.1105  total_loss: 4.508    time: 7.2959  last_time: 7.5777  data_time: 0.0038  last_data_time: 0.0040   lr: 9.5205e-06  max_mem: 6977M
[04/29 17:39:55] d2.utils.events INFO:  eta: 2 days, 13:33:02  iter: 119  loss_ce: 0.1526  loss_texts: 0.2007  loss_ctrl_points: 0.1197  loss_bd_points: 0.1652  loss_ce_0: 0.1738  loss_texts_0: 0.3059  loss_ctrl_points_0: 0.1162  loss_bd_points_0: 0.1545  loss_ce_1: 0.1719  loss_texts_1: 0.2559  loss_ctrl_points_1: 0.1222  loss_bd_points_1: 0.17  loss_ce_2: 0.1592  loss_texts_2: 0.2241  loss_ctrl_points_2: 0.1243  loss_bd_points_2: 0.1842  loss_ce_3: 0.1556  loss_texts_3: 0.206  loss_ctrl_points_3: 0.1208  loss_bd_points_3: 0.1716  loss_ce_4: 0.1534  loss_texts_4: 0.2012  loss_ctrl_points_4: 0.1201  loss_bd_points_4: 0.1652  loss_ce_enc: 0.1698  loss_bezier_enc: 0.1105  total_loss: 4.508    time: 7.2973  last_time: 7.6311  data_time: 0.0038  last_data_time: 0.0040   lr: 9.5604e-06  max_mem: 6977M
[04/29 17:42:24] d2.utils.events INFO:  eta: 2 days, 13:37:04  iter: 139  loss_ce: 0.1567  loss_texts: 0.1846  loss_ctrl_points: 0.1071  loss_bd_points: 0.1399  loss_ce_0: 0.1782  loss_texts_0: 0.277  loss_ctrl_points_0: 0.1104  loss_bd_points_0: 0.1526  loss_ce_1: 0.1742  loss_texts_1: 0.2338  loss_ctrl_points_1: 0.1105  loss_bd_points_1: 0.1404  loss_ce_2: 0.1673  loss_texts_2: 0.206  loss_ctrl_points_2: 0.1081  loss_bd_points_2: 0.141  loss_ce_3: 0.1664  loss_texts_3: 0.1893  loss_ctrl_points_3: 0.1134  loss_bd_points_3: 0.142  loss_ce_4: 0.1639  loss_texts_4: 0.1804  loss_ctrl_points_4: 0.1034  loss_bd_points_4: 0.1379  loss_ce_enc: 0.178  loss_bezier_enc: 0.1007  total_loss: 4.198    time: 7.3190  last_time: 7.5689  data_time: 0.0038  last_data_time: 0.0035   lr: 1e-05  max_mem: 6977M
[04/29 17:42:24] d2.utils.events INFO:  eta: 2 days, 13:38:40  iter: 139  loss_ce: 0.1567  loss_texts: 0.1846  loss_ctrl_points: 0.1071  loss_bd_points: 0.1399  loss_ce_0: 0.1782  loss_texts_0: 0.277  loss_ctrl_points_0: 0.1104  loss_bd_points_0: 0.1526  loss_ce_1: 0.1742  loss_texts_1: 0.2338  loss_ctrl_points_1: 0.1105  loss_bd_points_1: 0.1404  loss_ce_2: 0.1673  loss_texts_2: 0.206  loss_ctrl_points_2: 0.1081  loss_bd_points_2: 0.141  loss_ce_3: 0.1664  loss_texts_3: 0.1893  loss_ctrl_points_3: 0.1134  loss_bd_points_3: 0.142  loss_ce_4: 0.1639  loss_texts_4: 0.1804  loss_ctrl_points_4: 0.1034  loss_bd_points_4: 0.1379  loss_ce_enc: 0.178  loss_bezier_enc: 0.1007  total_loss: 4.198    time: 7.3201  last_time: 7.6256  data_time: 0.0038  last_data_time: 0.0035   lr: 1e-05  max_mem: 6977M
[04/29 17:44:53] d2.utils.events INFO:  eta: 2 days, 13:38:28  iter: 159  loss_ce: 0.1602  loss_texts: 0.213  loss_ctrl_points: 0.1075  loss_bd_points: 0.1387  loss_ce_0: 0.1758  loss_texts_0: 0.3028  loss_ctrl_points_0: 0.111  loss_bd_points_0: 0.1625  loss_ce_1: 0.1686  loss_texts_1: 0.2736  loss_ctrl_points_1: 0.1088  loss_bd_points_1: 0.1541  loss_ce_2: 0.1678  loss_texts_2: 0.24  loss_ctrl_points_2: 0.1075  loss_bd_points_2: 0.1499  loss_ce_3: 0.1699  loss_texts_3: 0.2242  loss_ctrl_points_3: 0.112  loss_bd_points_3: 0.1498  loss_ce_4: 0.1655  loss_texts_4: 0.216  loss_ctrl_points_4: 0.111  loss_bd_points_4: 0.152  loss_ce_enc: 0.1633  loss_bezier_enc: 0.1075  total_loss: 4.432    time: 7.3313  last_time: 7.8334  data_time: 0.0037  last_data_time: 0.0037   lr: 1e-05  max_mem: 6977M
[04/29 17:44:53] d2.utils.events INFO:  eta: 2 days, 13:39:01  iter: 159  loss_ce: 0.1602  loss_texts: 0.213  loss_ctrl_points: 0.1075  loss_bd_points: 0.1387  loss_ce_0: 0.1758  loss_texts_0: 0.3028  loss_ctrl_points_0: 0.111  loss_bd_points_0: 0.1625  loss_ce_1: 0.1686  loss_texts_1: 0.2736  loss_ctrl_points_1: 0.1088  loss_bd_points_1: 0.1541  loss_ce_2: 0.1678  loss_texts_2: 0.24  loss_ctrl_points_2: 0.1075  loss_bd_points_2: 0.1499  loss_ce_3: 0.1699  loss_texts_3: 0.2242  loss_ctrl_points_3: 0.112  loss_bd_points_3: 0.1498  loss_ce_4: 0.1655  loss_texts_4: 0.216  loss_ctrl_points_4: 0.111  loss_bd_points_4: 0.152  loss_ce_enc: 0.1633  loss_bezier_enc: 0.1075  total_loss: 4.432    time: 7.3331  last_time: 7.8862  data_time: 0.0037  last_data_time: 0.0037   lr: 1e-05  max_mem: 6977M
[04/29 17:47:21] d2.utils.events INFO:  eta: 2 days, 13:35:19  iter: 179  loss_ce: 0.1543  loss_texts: 0.1959  loss_ctrl_points: 0.1105  loss_bd_points: 0.1459  loss_ce_0: 0.1782  loss_texts_0: 0.314  loss_ctrl_points_0: 0.1229  loss_bd_points_0: 0.1524  loss_ce_1: 0.1674  loss_texts_1: 0.2631  loss_ctrl_points_1: 0.1125  loss_bd_points_1: 0.1516  loss_ce_2: 0.1592  loss_texts_2: 0.2181  loss_ctrl_points_2: 0.1116  loss_bd_points_2: 0.148  loss_ce_3: 0.1601  loss_texts_3: 0.2048  loss_ctrl_points_3: 0.1103  loss_bd_points_3: 0.1475  loss_ce_4: 0.1543  loss_texts_4: 0.1975  loss_ctrl_points_4: 0.1089  loss_bd_points_4: 0.1449  loss_ce_enc: 0.1678  loss_bezier_enc: 0.1068  total_loss: 4.258    time: 7.3456  last_time: 6.6314  data_time: 0.0037  last_data_time: 0.0037   lr: 1e-05  max_mem: 6977M
[04/29 17:47:21] d2.utils.events INFO:  eta: 2 days, 13:33:43  iter: 179  loss_ce: 0.1543  loss_texts: 0.1959  loss_ctrl_points: 0.1105  loss_bd_points: 0.1459  loss_ce_0: 0.1782  loss_texts_0: 0.314  loss_ctrl_points_0: 0.1229  loss_bd_points_0: 0.1524  loss_ce_1: 0.1674  loss_texts_1: 0.2631  loss_ctrl_points_1: 0.1125  loss_bd_points_1: 0.1516  loss_ce_2: 0.1592  loss_texts_2: 0.2181  loss_ctrl_points_2: 0.1116  loss_bd_points_2: 0.148  loss_ce_3: 0.1601  loss_texts_3: 0.2048  loss_ctrl_points_3: 0.1103  loss_bd_points_3: 0.1475  loss_ce_4: 0.1543  loss_texts_4: 0.1975  loss_ctrl_points_4: 0.1089  loss_bd_points_4: 0.1449  loss_ce_enc: 0.1678  loss_bezier_enc: 0.1068  total_loss: 4.258    time: 7.3437  last_time: 6.6826  data_time: 0.0037  last_data_time: 0.0037   lr: 1e-05  max_mem: 6977M
[04/29 17:49:52] d2.utils.events INFO:  eta: 2 days, 13:33:03  iter: 199  loss_ce: 0.1566  loss_texts: 0.1838  loss_ctrl_points: 0.1196  loss_bd_points: 0.1736  loss_ce_0: 0.1725  loss_texts_0: 0.2978  loss_ctrl_points_0: 0.1337  loss_bd_points_0: 0.1769  loss_ce_1: 0.1682  loss_texts_1: 0.2545  loss_ctrl_points_1: 0.1275  loss_bd_points_1: 0.1733  loss_ce_2: 0.1649  loss_texts_2: 0.2088  loss_ctrl_points_2: 0.1202  loss_bd_points_2: 0.1761  loss_ce_3: 0.1624  loss_texts_3: 0.1952  loss_ctrl_points_3: 0.1244  loss_bd_points_3: 0.1757  loss_ce_4: 0.1595  loss_texts_4: 0.1925  loss_ctrl_points_4: 0.1147  loss_bd_points_4: 0.1663  loss_ce_enc: 0.1689  loss_bezier_enc: 0.1053  total_loss: 4.224    time: 7.3630  last_time: 7.5463  data_time: 0.0038  last_data_time: 0.0039   lr: 1e-05  max_mem: 6977M
[04/29 17:49:52] d2.utils.events INFO:  eta: 2 days, 13:33:12  iter: 199  loss_ce: 0.1566  loss_texts: 0.1838  loss_ctrl_points: 0.1196  loss_bd_points: 0.1736  loss_ce_0: 0.1725  loss_texts_0: 0.2978  loss_ctrl_points_0: 0.1337  loss_bd_points_0: 0.1769  loss_ce_1: 0.1682  loss_texts_1: 0.2545  loss_ctrl_points_1: 0.1275  loss_bd_points_1: 0.1733  loss_ce_2: 0.1649  loss_texts_2: 0.2088  loss_ctrl_points_2: 0.1202  loss_bd_points_2: 0.1761  loss_ce_3: 0.1624  loss_texts_3: 0.1952  loss_ctrl_points_3: 0.1244  loss_bd_points_3: 0.1757  loss_ce_4: 0.1595  loss_texts_4: 0.1925  loss_ctrl_points_4: 0.1147  loss_bd_points_4: 0.1663  loss_ce_enc: 0.1689  loss_bezier_enc: 0.1053  total_loss: 4.224    time: 7.3637  last_time: 7.6156  data_time: 0.0038  last_data_time: 0.0039   lr: 1e-05  max_mem: 6977M
[04/29 17:52:22] d2.utils.events INFO:  eta: 2 days, 13:27:10  iter: 219  loss_ce: 0.1577  loss_texts: 0.2072  loss_ctrl_points: 0.1134  loss_bd_points: 0.1462  loss_ce_0: 0.182  loss_texts_0: 0.3162  loss_ctrl_points_0: 0.1089  loss_bd_points_0: 0.1403  loss_ce_1: 0.1717  loss_texts_1: 0.2736  loss_ctrl_points_1: 0.1098  loss_bd_points_1: 0.145  loss_ce_2: 0.1636  loss_texts_2: 0.2331  loss_ctrl_points_2: 0.1151  loss_bd_points_2: 0.1493  loss_ce_3: 0.16  loss_texts_3: 0.2164  loss_ctrl_points_3: 0.1148  loss_bd_points_3: 0.1468  loss_ce_4: 0.1604  loss_texts_4: 0.21  loss_ctrl_points_4: 0.1145  loss_bd_points_4: 0.1467  loss_ce_enc: 0.1696  loss_bezier_enc: 0.1052  total_loss: 4.427    time: 7.3740  last_time: 7.4372  data_time: 0.0038  last_data_time: 0.0036   lr: 1e-05  max_mem: 6977M
[04/29 17:52:22] d2.utils.events INFO:  eta: 2 days, 13:28:46  iter: 219  loss_ce: 0.1577  loss_texts: 0.2072  loss_ctrl_points: 0.1134  loss_bd_points: 0.1462  loss_ce_0: 0.182  loss_texts_0: 0.3162  loss_ctrl_points_0: 0.1089  loss_bd_points_0: 0.1403  loss_ce_1: 0.1717  loss_texts_1: 0.2736  loss_ctrl_points_1: 0.1098  loss_bd_points_1: 0.145  loss_ce_2: 0.1636  loss_texts_2: 0.2331  loss_ctrl_points_2: 0.1151  loss_bd_points_2: 0.1493  loss_ce_3: 0.16  loss_texts_3: 0.2164  loss_ctrl_points_3: 0.1148  loss_bd_points_3: 0.1468  loss_ce_4: 0.1604  loss_texts_4: 0.21  loss_ctrl_points_4: 0.1145  loss_bd_points_4: 0.1467  loss_ce_enc: 0.1696  loss_bezier_enc: 0.1052  total_loss: 4.427    time: 7.3742  last_time: 7.4951  data_time: 0.0038  last_data_time: 0.0036   lr: 1e-05  max_mem: 6977M
[04/29 17:54:50] d2.utils.events INFO:  eta: 2 days, 13:27:53  iter: 239  loss_ce: 0.1636  loss_texts: 0.2154  loss_ctrl_points: 0.1266  loss_bd_points: 0.169  loss_ce_0: 0.1896  loss_texts_0: 0.3192  loss_ctrl_points_0: 0.1266  loss_bd_points_0: 0.1752  loss_ce_1: 0.1808  loss_texts_1: 0.2631  loss_ctrl_points_1: 0.132  loss_bd_points_1: 0.1751  loss_ce_2: 0.1734  loss_texts_2: 0.2345  loss_ctrl_points_2: 0.1338  loss_bd_points_2: 0.1744  loss_ce_3: 0.1712  loss_texts_3: 0.2157  loss_ctrl_points_3: 0.1303  loss_bd_points_3: 0.1764  loss_ce_4: 0.1659  loss_texts_4: 0.2148  loss_ctrl_points_4: 0.1282  loss_bd_points_4: 0.1682  loss_ce_enc: 0.1853  loss_bezier_enc: 0.1018  total_loss: 4.642    time: 7.3788  last_time: 7.2462  data_time: 0.0037  last_data_time: 0.0040   lr: 1e-05  max_mem: 6977M
[04/29 17:54:50] d2.utils.events INFO:  eta: 2 days, 13:26:17  iter: 239  loss_ce: 0.1636  loss_texts: 0.2154  loss_ctrl_points: 0.1266  loss_bd_points: 0.169  loss_ce_0: 0.1896  loss_texts_0: 0.3192  loss_ctrl_points_0: 0.1266  loss_bd_points_0: 0.1752  loss_ce_1: 0.1808  loss_texts_1: 0.2631  loss_ctrl_points_1: 0.132  loss_bd_points_1: 0.1751  loss_ce_2: 0.1734  loss_texts_2: 0.2345  loss_ctrl_points_2: 0.1338  loss_bd_points_2: 0.1744  loss_ce_3: 0.1712  loss_texts_3: 0.2157  loss_ctrl_points_3: 0.1303  loss_bd_points_3: 0.1764  loss_ce_4: 0.1659  loss_texts_4: 0.2148  loss_ctrl_points_4: 0.1282  loss_bd_points_4: 0.1682  loss_ce_enc: 0.1853  loss_bezier_enc: 0.1018  total_loss: 4.642    time: 7.3787  last_time: 7.3380  data_time: 0.0037  last_data_time: 0.0040   lr: 1e-05  max_mem: 6977M
[04/29 17:57:17] d2.utils.events INFO:  eta: 2 days, 13:20:00  iter: 259  loss_ce: 0.1472  loss_texts: 0.1955  loss_ctrl_points: 0.1225  loss_bd_points: 0.1774  loss_ce_0: 0.171  loss_texts_0: 0.2951  loss_ctrl_points_0: 0.1221  loss_bd_points_0: 0.1765  loss_ce_1: 0.1594  loss_texts_1: 0.2509  loss_ctrl_points_1: 0.1261  loss_bd_points_1: 0.1796  loss_ce_2: 0.154  loss_texts_2: 0.2218  loss_ctrl_points_2: 0.1219  loss_bd_points_2: 0.1738  loss_ce_3: 0.1546  loss_texts_3: 0.1932  loss_ctrl_points_3: 0.1249  loss_bd_points_3: 0.1743  loss_ce_4: 0.1507  loss_texts_4: 0.1958  loss_ctrl_points_4: 0.1224  loss_bd_points_4: 0.1741  loss_ce_enc: 0.1682  loss_bezier_enc: 0.1049  total_loss: 4.636    time: 7.3733  last_time: 8.2631  data_time: 0.0037  last_data_time: 0.0036   lr: 1e-05  max_mem: 6977M
[04/29 17:57:17] d2.utils.events INFO:  eta: 2 days, 13:20:33  iter: 259  loss_ce: 0.1472  loss_texts: 0.1955  loss_ctrl_points: 0.1225  loss_bd_points: 0.1774  loss_ce_0: 0.171  loss_texts_0: 0.2951  loss_ctrl_points_0: 0.1221  loss_bd_points_0: 0.1765  loss_ce_1: 0.1594  loss_texts_1: 0.2509  loss_ctrl_points_1: 0.1261  loss_bd_points_1: 0.1796  loss_ce_2: 0.154  loss_texts_2: 0.2218  loss_ctrl_points_2: 0.1219  loss_bd_points_2: 0.1738  loss_ce_3: 0.1546  loss_texts_3: 0.1932  loss_ctrl_points_3: 0.1249  loss_bd_points_3: 0.1743  loss_ce_4: 0.1507  loss_texts_4: 0.1958  loss_ctrl_points_4: 0.1224  loss_bd_points_4: 0.1741  loss_ce_enc: 0.1682  loss_bezier_enc: 0.1049  total_loss: 4.636    time: 7.3752  last_time: 8.3546  data_time: 0.0037  last_data_time: 0.0036   lr: 1e-05  max_mem: 6977M
[04/29 17:59:36] d2.utils.events INFO:  eta: 2 days, 12:58:51  iter: 279  loss_ce: 0.161  loss_texts: 0.2293  loss_ctrl_points: 0.1083  loss_bd_points: 0.1397  loss_ce_0: 0.1894  loss_texts_0: 0.34  loss_ctrl_points_0: 0.1215  loss_bd_points_0: 0.1485  loss_ce_1: 0.1758  loss_texts_1: 0.2983  loss_ctrl_points_1: 0.1175  loss_bd_points_1: 0.1426  loss_ce_2: 0.1697  loss_texts_2: 0.2686  loss_ctrl_points_2: 0.1191  loss_bd_points_2: 0.1484  loss_ce_3: 0.1691  loss_texts_3: 0.2457  loss_ctrl_points_3: 0.1132  loss_bd_points_3: 0.1423  loss_ce_4: 0.1674  loss_texts_4: 0.2302  loss_ctrl_points_4: 0.1143  loss_bd_points_4: 0.1422  loss_ce_enc: 0.1846  loss_bezier_enc: 0.1036  total_loss: 4.639    time: 7.3432  last_time: 7.1898  data_time: 0.0038  last_data_time: 0.0037   lr: 1e-05  max_mem: 6977M
[04/29 17:59:36] d2.utils.events INFO:  eta: 2 days, 12:56:54  iter: 279  loss_ce: 0.161  loss_texts: 0.2293  loss_ctrl_points: 0.1083  loss_bd_points: 0.1397  loss_ce_0: 0.1894  loss_texts_0: 0.34  loss_ctrl_points_0: 0.1215  loss_bd_points_0: 0.1485  loss_ce_1: 0.1758  loss_texts_1: 0.2983  loss_ctrl_points_1: 0.1175  loss_bd_points_1: 0.1426  loss_ce_2: 0.1697  loss_texts_2: 0.2686  loss_ctrl_points_2: 0.1191  loss_bd_points_2: 0.1484  loss_ce_3: 0.1691  loss_texts_3: 0.2457  loss_ctrl_points_3: 0.1132  loss_bd_points_3: 0.1423  loss_ce_4: 0.1674  loss_texts_4: 0.2302  loss_ctrl_points_4: 0.1143  loss_bd_points_4: 0.1422  loss_ce_enc: 0.1846  loss_bezier_enc: 0.1036  total_loss: 4.639    time: 7.3430  last_time: 7.2638  data_time: 0.0038  last_data_time: 0.0037   lr: 1e-05  max_mem: 6977M
[04/29 18:01:56] d2.utils.events INFO:  eta: 2 days, 12:48:09  iter: 299  loss_ce: 0.1574  loss_texts: 0.2022  loss_ctrl_points: 0.1263  loss_bd_points: 0.1775  loss_ce_0: 0.176  loss_texts_0: 0.3122  loss_ctrl_points_0: 0.124  loss_bd_points_0: 0.1706  loss_ce_1: 0.1696  loss_texts_1: 0.2632  loss_ctrl_points_1: 0.1287  loss_bd_points_1: 0.1791  loss_ce_2: 0.1641  loss_texts_2: 0.2348  loss_ctrl_points_2: 0.1277  loss_bd_points_2: 0.1778  loss_ce_3: 0.1655  loss_texts_3: 0.2129  loss_ctrl_points_3: 0.1281  loss_bd_points_3: 0.1775  loss_ce_4: 0.1636  loss_texts_4: 0.2021  loss_ctrl_points_4: 0.1279  loss_bd_points_4: 0.1801  loss_ce_enc: 0.1671  loss_bezier_enc: 0.1057  total_loss: 4.486    time: 7.3206  last_time: 6.9844  data_time: 0.0038  last_data_time: 0.0035   lr: 1e-05  max_mem: 6977M
[04/29 18:01:56] d2.utils.events INFO:  eta: 2 days, 12:46:58  iter: 299  loss_ce: 0.1574  loss_texts: 0.2022  loss_ctrl_points: 0.1263  loss_bd_points: 0.1775  loss_ce_0: 0.176  loss_texts_0: 0.3122  loss_ctrl_points_0: 0.124  loss_bd_points_0: 0.1706  loss_ce_1: 0.1696  loss_texts_1: 0.2632  loss_ctrl_points_1: 0.1287  loss_bd_points_1: 0.1791  loss_ce_2: 0.1641  loss_texts_2: 0.2348  loss_ctrl_points_2: 0.1277  loss_bd_points_2: 0.1778  loss_ce_3: 0.1655  loss_texts_3: 0.2129  loss_ctrl_points_3: 0.1281  loss_bd_points_3: 0.1775  loss_ce_4: 0.1636  loss_texts_4: 0.2021  loss_ctrl_points_4: 0.1279  loss_bd_points_4: 0.1801  loss_ce_enc: 0.1671  loss_bezier_enc: 0.1057  total_loss: 4.486    time: 7.3202  last_time: 7.0758  data_time: 0.0038  last_data_time: 0.0035   lr: 1e-05  max_mem: 6977M
[04/29 18:04:12] d2.utils.events INFO:  eta: 2 days, 12:22:56  iter: 319  loss_ce: 0.1557  loss_texts: 0.2205  loss_ctrl_points: 0.1  loss_bd_points: 0.1399  loss_ce_0: 0.183  loss_texts_0: 0.3482  loss_ctrl_points_0: 0.1055  loss_bd_points_0: 0.1607  loss_ce_1: 0.1761  loss_texts_1: 0.2672  loss_ctrl_points_1: 0.09783  loss_bd_points_1: 0.1431  loss_ce_2: 0.1688  loss_texts_2: 0.237  loss_ctrl_points_2: 0.1015  loss_bd_points_2: 0.1447  loss_ce_3: 0.1648  loss_texts_3: 0.217  loss_ctrl_points_3: 0.1009  loss_bd_points_3: 0.1438  loss_ce_4: 0.1608  loss_texts_4: 0.2218  loss_ctrl_points_4: 0.1039  loss_bd_points_4: 0.1393  loss_ce_enc: 0.1693  loss_bezier_enc: 0.1046  total_loss: 4.309    time: 7.2880  last_time: 7.2858  data_time: 0.0038  last_data_time: 0.0039   lr: 1e-05  max_mem: 6977M
[04/29 18:04:12] d2.utils.events INFO:  eta: 2 days, 12:23:18  iter: 319  loss_ce: 0.1557  loss_texts: 0.2205  loss_ctrl_points: 0.1  loss_bd_points: 0.1399  loss_ce_0: 0.183  loss_texts_0: 0.3482  loss_ctrl_points_0: 0.1055  loss_bd_points_0: 0.1607  loss_ce_1: 0.1761  loss_texts_1: 0.2672  loss_ctrl_points_1: 0.09783  loss_bd_points_1: 0.1431  loss_ce_2: 0.1688  loss_texts_2: 0.237  loss_ctrl_points_2: 0.1015  loss_bd_points_2: 0.1447  loss_ce_3: 0.1648  loss_texts_3: 0.217  loss_ctrl_points_3: 0.1009  loss_bd_points_3: 0.1438  loss_ce_4: 0.1608  loss_texts_4: 0.2218  loss_ctrl_points_4: 0.1039  loss_bd_points_4: 0.1393  loss_ce_enc: 0.1693  loss_bezier_enc: 0.1046  total_loss: 4.309    time: 7.2881  last_time: 7.3531  data_time: 0.0038  last_data_time: 0.0039   lr: 1e-05  max_mem: 6977M
[04/29 18:06:28] d2.utils.events INFO:  eta: 2 days, 11:58:45  iter: 339  loss_ce: 0.1525  loss_texts: 0.2187  loss_ctrl_points: 0.148  loss_bd_points: 0.1893  loss_ce_0: 0.1897  loss_texts_0: 0.3336  loss_ctrl_points_0: 0.1423  loss_bd_points_0: 0.1929  loss_ce_1: 0.1688  loss_texts_1: 0.2814  loss_ctrl_points_1: 0.1458  loss_bd_points_1: 0.1861  loss_ce_2: 0.1595  loss_texts_2: 0.2414  loss_ctrl_points_2: 0.1406  loss_bd_points_2: 0.1772  loss_ce_3: 0.154  loss_texts_3: 0.2322  loss_ctrl_points_3: 0.1439  loss_bd_points_3: 0.176  loss_ce_4: 0.1542  loss_texts_4: 0.2235  loss_ctrl_points_4: 0.1482  loss_bd_points_4: 0.1887  loss_ce_enc: 0.1745  loss_bezier_enc: 0.1138  total_loss: 4.819    time: 7.2580  last_time: 7.0464  data_time: 0.0037  last_data_time: 0.0036   lr: 1e-05  max_mem: 6977M
[04/29 18:06:28] d2.utils.events INFO:  eta: 2 days, 11:58:38  iter: 339  loss_ce: 0.1525  loss_texts: 0.2187  loss_ctrl_points: 0.148  loss_bd_points: 0.1893  loss_ce_0: 0.1897  loss_texts_0: 0.3336  loss_ctrl_points_0: 0.1423  loss_bd_points_0: 0.1929  loss_ce_1: 0.1688  loss_texts_1: 0.2814  loss_ctrl_points_1: 0.1458  loss_bd_points_1: 0.1861  loss_ce_2: 0.1595  loss_texts_2: 0.2414  loss_ctrl_points_2: 0.1406  loss_bd_points_2: 0.1772  loss_ce_3: 0.154  loss_texts_3: 0.2322  loss_ctrl_points_3: 0.1439  loss_bd_points_3: 0.176  loss_ce_4: 0.1542  loss_texts_4: 0.2235  loss_ctrl_points_4: 0.1482  loss_bd_points_4: 0.1887  loss_ce_enc: 0.1745  loss_bezier_enc: 0.1138  total_loss: 4.819    time: 7.2578  last_time: 7.1035  data_time: 0.0037  last_data_time: 0.0036   lr: 1e-05  max_mem: 6977M
[04/29 18:08:40] d2.utils.events INFO:  eta: 2 days, 11:37:37  iter: 359  loss_ce: 0.164  loss_texts: 0.1863  loss_ctrl_points: 0.1096  loss_bd_points: 0.1476  loss_ce_0: 0.178  loss_texts_0: 0.2791  loss_ctrl_points_0: 0.1097  loss_bd_points_0: 0.1499  loss_ce_1: 0.1794  loss_texts_1: 0.2343  loss_ctrl_points_1: 0.1103  loss_bd_points_1: 0.1432  loss_ce_2: 0.167  loss_texts_2: 0.2085  loss_ctrl_points_2: 0.1104  loss_bd_points_2: 0.1456  loss_ce_3: 0.1696  loss_texts_3: 0.1969  loss_ctrl_points_3: 0.1119  loss_bd_points_3: 0.1468  loss_ce_4: 0.1649  loss_texts_4: 0.1862  loss_ctrl_points_4: 0.1096  loss_bd_points_4: 0.1464  loss_ce_enc: 0.1645  loss_bezier_enc: 0.1055  total_loss: 4.051    time: 7.2214  last_time: 5.9485  data_time: 0.0037  last_data_time: 0.0037   lr: 1e-05  max_mem: 6977M
[04/29 18:08:40] d2.utils.events INFO:  eta: 2 days, 11:37:30  iter: 359  loss_ce: 0.164  loss_texts: 0.1863  loss_ctrl_points: 0.1096  loss_bd_points: 0.1476  loss_ce_0: 0.178  loss_texts_0: 0.2791  loss_ctrl_points_0: 0.1097  loss_bd_points_0: 0.1499  loss_ce_1: 0.1794  loss_texts_1: 0.2343  loss_ctrl_points_1: 0.1103  loss_bd_points_1: 0.1432  loss_ce_2: 0.167  loss_texts_2: 0.2085  loss_ctrl_points_2: 0.1104  loss_bd_points_2: 0.1456  loss_ce_3: 0.1696  loss_texts_3: 0.1969  loss_ctrl_points_3: 0.1119  loss_bd_points_3: 0.1468  loss_ce_4: 0.1649  loss_texts_4: 0.1862  loss_ctrl_points_4: 0.1096  loss_bd_points_4: 0.1464  loss_ce_enc: 0.1645  loss_bezier_enc: 0.1055  total_loss: 4.051    time: 7.2197  last_time: 6.0156  data_time: 0.0037  last_data_time: 0.0037   lr: 1e-05  max_mem: 6977M
[04/29 18:10:52] d2.utils.events INFO:  eta: 2 days, 11:18:36  iter: 379  loss_ce: 0.1544  loss_texts: 0.2296  loss_ctrl_points: 0.1195  loss_bd_points: 0.1622  loss_ce_0: 0.1842  loss_texts_0: 0.3336  loss_ctrl_points_0: 0.1199  loss_bd_points_0: 0.1708  loss_ce_1: 0.1709  loss_texts_1: 0.3015  loss_ctrl_points_1: 0.1136  loss_bd_points_1: 0.1541  loss_ce_2: 0.1657  loss_texts_2: 0.2669  loss_ctrl_points_2: 0.1146  loss_bd_points_2: 0.1573  loss_ce_3: 0.162  loss_texts_3: 0.2468  loss_ctrl_points_3: 0.1206  loss_bd_points_3: 0.1647  loss_ce_4: 0.1569  loss_texts_4: 0.2374  loss_ctrl_points_4: 0.12  loss_bd_points_4: 0.1632  loss_ce_enc: 0.1752  loss_bezier_enc: 0.1052  total_loss: 4.694    time: 7.1869  last_time: 6.9244  data_time: 0.0036  last_data_time: 0.0037   lr: 1e-05  max_mem: 6977M
[04/29 18:10:52] d2.utils.events INFO:  eta: 2 days, 11:18:25  iter: 379  loss_ce: 0.1544  loss_texts: 0.2296  loss_ctrl_points: 0.1195  loss_bd_points: 0.1622  loss_ce_0: 0.1842  loss_texts_0: 0.3336  loss_ctrl_points_0: 0.1199  loss_bd_points_0: 0.1708  loss_ce_1: 0.1709  loss_texts_1: 0.3015  loss_ctrl_points_1: 0.1136  loss_bd_points_1: 0.1541  loss_ce_2: 0.1657  loss_texts_2: 0.2669  loss_ctrl_points_2: 0.1146  loss_bd_points_2: 0.1573  loss_ce_3: 0.162  loss_texts_3: 0.2468  loss_ctrl_points_3: 0.1206  loss_bd_points_3: 0.1647  loss_ce_4: 0.1569  loss_texts_4: 0.2374  loss_ctrl_points_4: 0.12  loss_bd_points_4: 0.1632  loss_ce_enc: 0.1752  loss_bezier_enc: 0.1052  total_loss: 4.694    time: 7.1867  last_time: 7.0543  data_time: 0.0036  last_data_time: 0.0037   lr: 1e-05  max_mem: 6977M
[04/29 18:13:11] d2.utils.events INFO:  eta: 2 days, 11:11:33  iter: 399  loss_ce: 0.1465  loss_texts: 0.1994  loss_ctrl_points: 0.1236  loss_bd_points: 0.1559  loss_ce_0: 0.1709  loss_texts_0: 0.2912  loss_ctrl_points_0: 0.1165  loss_bd_points_0: 0.1525  loss_ce_1: 0.1754  loss_texts_1: 0.2423  loss_ctrl_points_1: 0.1193  loss_bd_points_1: 0.1436  loss_ce_2: 0.1593  loss_texts_2: 0.2126  loss_ctrl_points_2: 0.1199  loss_bd_points_2: 0.1457  loss_ce_3: 0.1502  loss_texts_3: 0.2085  loss_ctrl_points_3: 0.1181  loss_bd_points_3: 0.1509  loss_ce_4: 0.1492  loss_texts_4: 0.1998  loss_ctrl_points_4: 0.1181  loss_bd_points_4: 0.1546  loss_ce_enc: 0.1574  loss_bezier_enc: 0.1081  total_loss: 4.299    time: 7.1755  last_time: 7.6017  data_time: 0.0038  last_data_time: 0.0036   lr: 1e-05  max_mem: 6977M
[04/29 18:13:11] d2.utils.events INFO:  eta: 2 days, 11:11:40  iter: 399  loss_ce: 0.1465  loss_texts: 0.1994  loss_ctrl_points: 0.1236  loss_bd_points: 0.1559  loss_ce_0: 0.1709  loss_texts_0: 0.2912  loss_ctrl_points_0: 0.1165  loss_bd_points_0: 0.1525  loss_ce_1: 0.1754  loss_texts_1: 0.2423  loss_ctrl_points_1: 0.1193  loss_bd_points_1: 0.1436  loss_ce_2: 0.1593  loss_texts_2: 0.2126  loss_ctrl_points_2: 0.1199  loss_bd_points_2: 0.1457  loss_ce_3: 0.1502  loss_texts_3: 0.2085  loss_ctrl_points_3: 0.1181  loss_bd_points_3: 0.1509  loss_ce_4: 0.1492  loss_texts_4: 0.1998  loss_ctrl_points_4: 0.1181  loss_bd_points_4: 0.1546  loss_ce_enc: 0.1574  loss_bezier_enc: 0.1081  total_loss: 4.299    time: 7.1761  last_time: 7.6811  data_time: 0.0038  last_data_time: 0.0036   lr: 1e-05  max_mem: 6977M
[04/29 18:15:29] d2.utils.events INFO:  eta: 2 days, 11:04:33  iter: 419  loss_ce: 0.1529  loss_texts: 0.1965  loss_ctrl_points: 0.1177  loss_bd_points: 0.1618  loss_ce_0: 0.1873  loss_texts_0: 0.2937  loss_ctrl_points_0: 0.1139  loss_bd_points_0: 0.1549  loss_ce_1: 0.1773  loss_texts_1: 0.2456  loss_ctrl_points_1: 0.114  loss_bd_points_1: 0.1587  loss_ce_2: 0.168  loss_texts_2: 0.2183  loss_ctrl_points_2: 0.1181  loss_bd_points_2: 0.1603  loss_ce_3: 0.1636  loss_texts_3: 0.2088  loss_ctrl_points_3: 0.1192  loss_bd_points_3: 0.1601  loss_ce_4: 0.1554  loss_texts_4: 0.2001  loss_ctrl_points_4: 0.1186  loss_bd_points_4: 0.1617  loss_ce_enc: 0.169  loss_bezier_enc: 0.1088  total_loss: 4.472    time: 7.1622  last_time: 6.5458  data_time: 0.0038  last_data_time: 0.0036   lr: 1e-05  max_mem: 6977M
[04/29 18:15:29] d2.utils.events INFO:  eta: 2 days, 11:04:00  iter: 419  loss_ce: 0.1529  loss_texts: 0.1965  loss_ctrl_points: 0.1177  loss_bd_points: 0.1618  loss_ce_0: 0.1873  loss_texts_0: 0.2937  loss_ctrl_points_0: 0.1139  loss_bd_points_0: 0.1549  loss_ce_1: 0.1773  loss_texts_1: 0.2456  loss_ctrl_points_1: 0.114  loss_bd_points_1: 0.1587  loss_ce_2: 0.168  loss_texts_2: 0.2183  loss_ctrl_points_2: 0.1181  loss_bd_points_2: 0.1603  loss_ce_3: 0.1636  loss_texts_3: 0.2088  loss_ctrl_points_3: 0.1192  loss_bd_points_3: 0.1601  loss_ce_4: 0.1554  loss_texts_4: 0.2001  loss_ctrl_points_4: 0.1186  loss_bd_points_4: 0.1617  loss_ce_enc: 0.169  loss_bezier_enc: 0.1088  total_loss: 4.472    time: 7.1616  last_time: 6.6148  data_time: 0.0038  last_data_time: 0.0036   lr: 1e-05  max_mem: 6977M
[04/29 18:17:39] d2.utils.events INFO:  eta: 2 days, 10:39:06  iter: 439  loss_ce: 0.1485  loss_texts: 0.1828  loss_ctrl_points: 0.1255  loss_bd_points: 0.1655  loss_ce_0: 0.1926  loss_texts_0: 0.3045  loss_ctrl_points_0: 0.1135  loss_bd_points_0: 0.1727  loss_ce_1: 0.1771  loss_texts_1: 0.2495  loss_ctrl_points_1: 0.125  loss_bd_points_1: 0.1731  loss_ce_2: 0.1614  loss_texts_2: 0.2171  loss_ctrl_points_2: 0.1276  loss_bd_points_2: 0.177  loss_ce_3: 0.156  loss_texts_3: 0.2013  loss_ctrl_points_3: 0.1267  loss_bd_points_3: 0.1719  loss_ce_4: 0.1515  loss_texts_4: 0.1855  loss_ctrl_points_4: 0.1261  loss_bd_points_4: 0.1684  loss_ce_enc: 0.1785  loss_bezier_enc: 0.1134  total_loss: 4.333    time: 7.1322  last_time: 7.6568  data_time: 0.0037  last_data_time: 0.0039   lr: 1e-05  max_mem: 6977M
[04/29 18:17:40] d2.utils.events INFO:  eta: 2 days, 10:40:02  iter: 439  loss_ce: 0.1485  loss_texts: 0.1828  loss_ctrl_points: 0.1255  loss_bd_points: 0.1655  loss_ce_0: 0.1926  loss_texts_0: 0.3045  loss_ctrl_points_0: 0.1135  loss_bd_points_0: 0.1727  loss_ce_1: 0.1771  loss_texts_1: 0.2495  loss_ctrl_points_1: 0.125  loss_bd_points_1: 0.1731  loss_ce_2: 0.1614  loss_texts_2: 0.2171  loss_ctrl_points_2: 0.1276  loss_bd_points_2: 0.177  loss_ce_3: 0.156  loss_texts_3: 0.2013  loss_ctrl_points_3: 0.1267  loss_bd_points_3: 0.1719  loss_ce_4: 0.1515  loss_texts_4: 0.1855  loss_ctrl_points_4: 0.1261  loss_bd_points_4: 0.1684  loss_ce_enc: 0.1785  loss_bezier_enc: 0.1134  total_loss: 4.333    time: 7.1329  last_time: 7.7377  data_time: 0.0037  last_data_time: 0.0039   lr: 1e-05  max_mem: 6977M
[04/29 18:20:02] d2.utils.events INFO:  eta: 2 days, 10:38:36  iter: 459  loss_ce: 0.1495  loss_texts: 0.1896  loss_ctrl_points: 0.1031  loss_bd_points: 0.1318  loss_ce_0: 0.1724  loss_texts_0: 0.2921  loss_ctrl_points_0: 0.1069  loss_bd_points_0: 0.1428  loss_ce_1: 0.1647  loss_texts_1: 0.2486  loss_ctrl_points_1: 0.1073  loss_bd_points_1: 0.1358  loss_ce_2: 0.1594  loss_texts_2: 0.2124  loss_ctrl_points_2: 0.1063  loss_bd_points_2: 0.1353  loss_ce_3: 0.1558  loss_texts_3: 0.1984  loss_ctrl_points_3: 0.1039  loss_bd_points_3: 0.1335  loss_ce_4: 0.1555  loss_texts_4: 0.1901  loss_ctrl_points_4: 0.1035  loss_bd_points_4: 0.1319  loss_ce_enc: 0.1578  loss_bezier_enc: 0.1093  total_loss: 4.06    time: 7.1335  last_time: 6.7167  data_time: 0.0038  last_data_time: 0.0038   lr: 1e-05  max_mem: 6977M
[04/29 18:20:02] d2.utils.events INFO:  eta: 2 days, 10:37:39  iter: 459  loss_ce: 0.1495  loss_texts: 0.1896  loss_ctrl_points: 0.1031  loss_bd_points: 0.1318  loss_ce_0: 0.1724  loss_texts_0: 0.2921  loss_ctrl_points_0: 0.1069  loss_bd_points_0: 0.1428  loss_ce_1: 0.1647  loss_texts_1: 0.2486  loss_ctrl_points_1: 0.1073  loss_bd_points_1: 0.1358  loss_ce_2: 0.1594  loss_texts_2: 0.2124  loss_ctrl_points_2: 0.1063  loss_bd_points_2: 0.1353  loss_ce_3: 0.1558  loss_texts_3: 0.1984  loss_ctrl_points_3: 0.1039  loss_bd_points_3: 0.1335  loss_ce_4: 0.1555  loss_texts_4: 0.1901  loss_ctrl_points_4: 0.1035  loss_bd_points_4: 0.1319  loss_ce_enc: 0.1578  loss_bezier_enc: 0.1093  total_loss: 4.06    time: 7.1331  last_time: 6.7769  data_time: 0.0038  last_data_time: 0.0038   lr: 1e-05  max_mem: 6977M
[04/29 18:22:17] d2.utils.events INFO:  eta: 2 days, 10:25:40  iter: 479  loss_ce: 0.1579  loss_texts: 0.1891  loss_ctrl_points: 0.1327  loss_bd_points: 0.18  loss_ce_0: 0.1859  loss_texts_0: 0.3012  loss_ctrl_points_0: 0.1331  loss_bd_points_0: 0.1825  loss_ce_1: 0.1665  loss_texts_1: 0.2442  loss_ctrl_points_1: 0.1292  loss_bd_points_1: 0.1866  loss_ce_2: 0.1584  loss_texts_2: 0.2109  loss_ctrl_points_2: 0.1305  loss_bd_points_2: 0.196  loss_ce_3: 0.1599  loss_texts_3: 0.2004  loss_ctrl_points_3: 0.1278  loss_bd_points_3: 0.1856  loss_ce_4: 0.157  loss_texts_4: 0.1906  loss_ctrl_points_4: 0.1336  loss_bd_points_4: 0.1844  loss_ce_enc: 0.1602  loss_bezier_enc: 0.115  total_loss: 4.623    time: 7.1165  last_time: 7.2536  data_time: 0.0039  last_data_time: 0.0035   lr: 1e-05  max_mem: 6977M
[04/29 18:22:17] d2.utils.events INFO:  eta: 2 days, 10:27:22  iter: 479  loss_ce: 0.1579  loss_texts: 0.1891  loss_ctrl_points: 0.1327  loss_bd_points: 0.18  loss_ce_0: 0.1859  loss_texts_0: 0.3012  loss_ctrl_points_0: 0.1331  loss_bd_points_0: 0.1825  loss_ce_1: 0.1665  loss_texts_1: 0.2442  loss_ctrl_points_1: 0.1292  loss_bd_points_1: 0.1866  loss_ce_2: 0.1584  loss_texts_2: 0.2109  loss_ctrl_points_2: 0.1305  loss_bd_points_2: 0.196  loss_ce_3: 0.1599  loss_texts_3: 0.2004  loss_ctrl_points_3: 0.1278  loss_bd_points_3: 0.1856  loss_ce_4: 0.157  loss_texts_4: 0.1906  loss_ctrl_points_4: 0.1336  loss_bd_points_4: 0.1844  loss_ce_enc: 0.1602  loss_bezier_enc: 0.115  total_loss: 4.623    time: 7.1167  last_time: 7.3050  data_time: 0.0039  last_data_time: 0.0035   lr: 1e-05  max_mem: 6977M
[04/29 18:24:43] d2.utils.events INFO:  eta: 2 days, 10:30:08  iter: 499  loss_ce: 0.1523  loss_texts: 0.2075  loss_ctrl_points: 0.1089  loss_bd_points: 0.1438  loss_ce_0: 0.1665  loss_texts_0: 0.3203  loss_ctrl_points_0: 0.1144  loss_bd_points_0: 0.15  loss_ce_1: 0.1677  loss_texts_1: 0.2685  loss_ctrl_points_1: 0.12  loss_bd_points_1: 0.1506  loss_ce_2: 0.1552  loss_texts_2: 0.2309  loss_ctrl_points_2: 0.1192  loss_bd_points_2: 0.1495  loss_ce_3: 0.1593  loss_texts_3: 0.2121  loss_ctrl_points_3: 0.1197  loss_bd_points_3: 0.1485  loss_ce_4: 0.1577  loss_texts_4: 0.2078  loss_ctrl_points_4: 0.1156  loss_bd_points_4: 0.1454  loss_ce_enc: 0.162  loss_bezier_enc: 0.1014  total_loss: 4.368    time: 7.1239  last_time: 6.9045  data_time: 0.0037  last_data_time: 0.0036   lr: 1e-05  max_mem: 6977M
[04/29 18:24:43] d2.utils.events INFO:  eta: 2 days, 10:28:51  iter: 499  loss_ce: 0.1523  loss_texts: 0.2075  loss_ctrl_points: 0.1089  loss_bd_points: 0.1438  loss_ce_0: 0.1665  loss_texts_0: 0.3203  loss_ctrl_points_0: 0.1144  loss_bd_points_0: 0.15  loss_ce_1: 0.1677  loss_texts_1: 0.2685  loss_ctrl_points_1: 0.12  loss_bd_points_1: 0.1506  loss_ce_2: 0.1552  loss_texts_2: 0.2309  loss_ctrl_points_2: 0.1192  loss_bd_points_2: 0.1495  loss_ce_3: 0.1593  loss_texts_3: 0.2121  loss_ctrl_points_3: 0.1197  loss_bd_points_3: 0.1485  loss_ce_4: 0.1577  loss_texts_4: 0.2078  loss_ctrl_points_4: 0.1156  loss_bd_points_4: 0.1454  loss_ce_enc: 0.162  loss_bezier_enc: 0.1014  total_loss: 4.368    time: 7.1238  last_time: 6.9749  data_time: 0.0037  last_data_time: 0.0036   lr: 1e-05  max_mem: 6977M
[04/29 18:26:58] d2.utils.events INFO:  eta: 2 days, 10:15:22  iter: 519  loss_ce: 0.149  loss_texts: 0.1843  loss_ctrl_points: 0.1208  loss_bd_points: 0.1678  loss_ce_0: 0.1777  loss_texts_0: 0.3069  loss_ctrl_points_0: 0.1173  loss_bd_points_0: 0.1684  loss_ce_1: 0.1649  loss_texts_1: 0.2489  loss_ctrl_points_1: 0.1203  loss_bd_points_1: 0.1673  loss_ce_2: 0.1626  loss_texts_2: 0.2174  loss_ctrl_points_2: 0.12  loss_bd_points_2: 0.1614  loss_ce_3: 0.1549  loss_texts_3: 0.1887  loss_ctrl_points_3: 0.1212  loss_bd_points_3: 0.1687  loss_ce_4: 0.151  loss_texts_4: 0.1832  loss_ctrl_points_4: 0.1206  loss_bd_points_4: 0.1661  loss_ce_enc: 0.1659  loss_bezier_enc: 0.1081  total_loss: 4.401    time: 7.1084  last_time: 6.8744  data_time: 0.0037  last_data_time: 0.0037   lr: 1e-05  max_mem: 6977M
[04/29 18:26:58] d2.utils.events INFO:  eta: 2 days, 10:15:22  iter: 519  loss_ce: 0.149  loss_texts: 0.1843  loss_ctrl_points: 0.1208  loss_bd_points: 0.1678  loss_ce_0: 0.1777  loss_texts_0: 0.3069  loss_ctrl_points_0: 0.1173  loss_bd_points_0: 0.1684  loss_ce_1: 0.1649  loss_texts_1: 0.2489  loss_ctrl_points_1: 0.1203  loss_bd_points_1: 0.1673  loss_ce_2: 0.1626  loss_texts_2: 0.2174  loss_ctrl_points_2: 0.12  loss_bd_points_2: 0.1614  loss_ce_3: 0.1549  loss_texts_3: 0.1887  loss_ctrl_points_3: 0.1212  loss_bd_points_3: 0.1687  loss_ce_4: 0.151  loss_texts_4: 0.1832  loss_ctrl_points_4: 0.1206  loss_bd_points_4: 0.1661  loss_ce_enc: 0.1659  loss_bezier_enc: 0.1081  total_loss: 4.401    time: 7.1083  last_time: 6.9542  data_time: 0.0037  last_data_time: 0.0037   lr: 1e-05  max_mem: 6977M
[04/29 18:29:07] d2.utils.events INFO:  eta: 2 days, 10:11:38  iter: 539  loss_ce: 0.1481  loss_texts: 0.1992  loss_ctrl_points: 0.1174  loss_bd_points: 0.1548  loss_ce_0: 0.1794  loss_texts_0: 0.3184  loss_ctrl_points_0: 0.1186  loss_bd_points_0: 0.1666  loss_ce_1: 0.1614  loss_texts_1: 0.2629  loss_ctrl_points_1: 0.1206  loss_bd_points_1: 0.1651  loss_ce_2: 0.1502  loss_texts_2: 0.2276  loss_ctrl_points_2: 0.1196  loss_bd_points_2: 0.1583  loss_ce_3: 0.1512  loss_texts_3: 0.2087  loss_ctrl_points_3: 0.1203  loss_bd_points_3: 0.1561  loss_ce_4: 0.1516  loss_texts_4: 0.1997  loss_ctrl_points_4: 0.1181  loss_bd_points_4: 0.1567  loss_ce_enc: 0.1654  loss_bezier_enc: 0.1061  total_loss: 4.335    time: 7.0838  last_time: 7.1319  data_time: 0.0037  last_data_time: 0.0036   lr: 1e-05  max_mem: 6977M
[04/29 18:29:07] d2.utils.events INFO:  eta: 2 days, 10:11:55  iter: 539  loss_ce: 0.1481  loss_texts: 0.1992  loss_ctrl_points: 0.1174  loss_bd_points: 0.1548  loss_ce_0: 0.1794  loss_texts_0: 0.3184  loss_ctrl_points_0: 0.1186  loss_bd_points_0: 0.1666  loss_ce_1: 0.1614  loss_texts_1: 0.2629  loss_ctrl_points_1: 0.1206  loss_bd_points_1: 0.1651  loss_ce_2: 0.1502  loss_texts_2: 0.2276  loss_ctrl_points_2: 0.1196  loss_bd_points_2: 0.1583  loss_ce_3: 0.1512  loss_texts_3: 0.2087  loss_ctrl_points_3: 0.1203  loss_bd_points_3: 0.1561  loss_ce_4: 0.1516  loss_texts_4: 0.1997  loss_ctrl_points_4: 0.1181  loss_bd_points_4: 0.1567  loss_ce_enc: 0.1654  loss_bezier_enc: 0.1061  total_loss: 4.335    time: 7.0839  last_time: 7.1866  data_time: 0.0037  last_data_time: 0.0036   lr: 1e-05  max_mem: 6977M
[04/29 18:31:26] d2.utils.events INFO:  eta: 2 days, 9:58:41  iter: 559  loss_ce: 0.1488  loss_texts: 0.2108  loss_ctrl_points: 0.1196  loss_bd_points: 0.1551  loss_ce_0: 0.181  loss_texts_0: 0.3108  loss_ctrl_points_0: 0.1234  loss_bd_points_0: 0.1558  loss_ce_1: 0.1669  loss_texts_1: 0.2585  loss_ctrl_points_1: 0.121  loss_bd_points_1: 0.1544  loss_ce_2: 0.16  loss_texts_2: 0.2301  loss_ctrl_points_2: 0.1186  loss_bd_points_2: 0.1574  loss_ce_3: 0.1567  loss_texts_3: 0.2215  loss_ctrl_points_3: 0.1186  loss_bd_points_3: 0.1539  loss_ce_4: 0.1511  loss_texts_4: 0.215  loss_ctrl_points_4: 0.1217  loss_bd_points_4: 0.1563  loss_ce_enc: 0.1632  loss_bezier_enc: 0.1145  total_loss: 4.513    time: 7.0796  last_time: 6.6227  data_time: 0.0037  last_data_time: 0.0035   lr: 1e-05  max_mem: 6977M
[04/29 18:31:26] d2.utils.events INFO:  eta: 2 days, 9:57:49  iter: 559  loss_ce: 0.1488  loss_texts: 0.2108  loss_ctrl_points: 0.1196  loss_bd_points: 0.1551  loss_ce_0: 0.181  loss_texts_0: 0.3108  loss_ctrl_points_0: 0.1234  loss_bd_points_0: 0.1558  loss_ce_1: 0.1669  loss_texts_1: 0.2585  loss_ctrl_points_1: 0.121  loss_bd_points_1: 0.1544  loss_ce_2: 0.16  loss_texts_2: 0.2301  loss_ctrl_points_2: 0.1186  loss_bd_points_2: 0.1574  loss_ce_3: 0.1567  loss_texts_3: 0.2215  loss_ctrl_points_3: 0.1186  loss_bd_points_3: 0.1539  loss_ce_4: 0.1511  loss_texts_4: 0.215  loss_ctrl_points_4: 0.1217  loss_bd_points_4: 0.1563  loss_ce_enc: 0.1632  loss_bezier_enc: 0.1145  total_loss: 4.513    time: 7.0792  last_time: 6.6871  data_time: 0.0037  last_data_time: 0.0035   lr: 1e-05  max_mem: 6977M
[04/29 18:33:39] d2.utils.events INFO:  eta: 2 days, 9:34:27  iter: 579  loss_ce: 0.1558  loss_texts: 0.2118  loss_ctrl_points: 0.1187  loss_bd_points: 0.1607  loss_ce_0: 0.1809  loss_texts_0: 0.3267  loss_ctrl_points_0: 0.1135  loss_bd_points_0: 0.1716  loss_ce_1: 0.1692  loss_texts_1: 0.2655  loss_ctrl_points_1: 0.1208  loss_bd_points_1: 0.1674  loss_ce_2: 0.1602  loss_texts_2: 0.2307  loss_ctrl_points_2: 0.121  loss_bd_points_2: 0.1658  loss_ce_3: 0.1624  loss_texts_3: 0.2222  loss_ctrl_points_3: 0.1231  loss_bd_points_3: 0.163  loss_ce_4: 0.1598  loss_texts_4: 0.2126  loss_ctrl_points_4: 0.1218  loss_bd_points_4: 0.1587  loss_ce_enc: 0.1655  loss_bezier_enc: 0.1056  total_loss: 4.433    time: 7.0641  last_time: 6.2150  data_time: 0.0038  last_data_time: 0.0039   lr: 1e-05  max_mem: 6977M
[04/29 18:33:39] d2.utils.events INFO:  eta: 2 days, 9:33:21  iter: 579  loss_ce: 0.1558  loss_texts: 0.2118  loss_ctrl_points: 0.1187  loss_bd_points: 0.1607  loss_ce_0: 0.1809  loss_texts_0: 0.3267  loss_ctrl_points_0: 0.1135  loss_bd_points_0: 0.1716  loss_ce_1: 0.1692  loss_texts_1: 0.2655  loss_ctrl_points_1: 0.1208  loss_bd_points_1: 0.1674  loss_ce_2: 0.1602  loss_texts_2: 0.2307  loss_ctrl_points_2: 0.121  loss_bd_points_2: 0.1658  loss_ce_3: 0.1624  loss_texts_3: 0.2222  loss_ctrl_points_3: 0.1231  loss_bd_points_3: 0.163  loss_ce_4: 0.1598  loss_texts_4: 0.2126  loss_ctrl_points_4: 0.1218  loss_bd_points_4: 0.1587  loss_ce_enc: 0.1655  loss_bezier_enc: 0.1056  total_loss: 4.433    time: 7.0634  last_time: 6.2853  data_time: 0.0038  last_data_time: 0.0039   lr: 1e-05  max_mem: 6977M
[04/29 18:35:53] d2.utils.events INFO:  eta: 2 days, 9:06:53  iter: 599  loss_ce: 0.1452  loss_texts: 0.1903  loss_ctrl_points: 0.108  loss_bd_points: 0.1445  loss_ce_0: 0.1764  loss_texts_0: 0.2934  loss_ctrl_points_0: 0.111  loss_bd_points_0: 0.1486  loss_ce_1: 0.1633  loss_texts_1: 0.2428  loss_ctrl_points_1: 0.1088  loss_bd_points_1: 0.1466  loss_ce_2: 0.1597  loss_texts_2: 0.2011  loss_ctrl_points_2: 0.1134  loss_bd_points_2: 0.1488  loss_ce_3: 0.1538  loss_texts_3: 0.1918  loss_ctrl_points_3: 0.1121  loss_bd_points_3: 0.1477  loss_ce_4: 0.1496  loss_texts_4: 0.1913  loss_ctrl_points_4: 0.1084  loss_bd_points_4: 0.1444  loss_ce_enc: 0.1624  loss_bezier_enc: 0.104  total_loss: 4.353    time: 7.0522  last_time: 7.0549  data_time: 0.0037  last_data_time: 0.0039   lr: 1e-05  max_mem: 6977M
[04/29 18:35:53] d2.utils.events INFO:  eta: 2 days, 9:07:09  iter: 599  loss_ce: 0.1452  loss_texts: 0.1903  loss_ctrl_points: 0.108  loss_bd_points: 0.1445  loss_ce_0: 0.1764  loss_texts_0: 0.2934  loss_ctrl_points_0: 0.111  loss_bd_points_0: 0.1486  loss_ce_1: 0.1633  loss_texts_1: 0.2428  loss_ctrl_points_1: 0.1088  loss_bd_points_1: 0.1466  loss_ce_2: 0.1597  loss_texts_2: 0.2011  loss_ctrl_points_2: 0.1134  loss_bd_points_2: 0.1488  loss_ce_3: 0.1538  loss_texts_3: 0.1918  loss_ctrl_points_3: 0.1121  loss_bd_points_3: 0.1477  loss_ce_4: 0.1496  loss_texts_4: 0.1913  loss_ctrl_points_4: 0.1084  loss_bd_points_4: 0.1444  loss_ce_enc: 0.1624  loss_bezier_enc: 0.104  total_loss: 4.353    time: 7.0522  last_time: 7.1151  data_time: 0.0037  last_data_time: 0.0039   lr: 1e-05  max_mem: 6977M
[04/29 18:38:06] d2.utils.events INFO:  eta: 2 days, 8:45:59  iter: 619  loss_ce: 0.1721  loss_texts: 0.2518  loss_ctrl_points: 0.1344  loss_bd_points: 0.1767  loss_ce_0: 0.1944  loss_texts_0: 0.3526  loss_ctrl_points_0: 0.141  loss_bd_points_0: 0.1753  loss_ce_1: 0.1791  loss_texts_1: 0.3125  loss_ctrl_points_1: 0.1424  loss_bd_points_1: 0.1806  loss_ce_2: 0.1766  loss_texts_2: 0.2871  loss_ctrl_points_2: 0.1436  loss_bd_points_2: 0.1832  loss_ce_3: 0.181  loss_texts_3: 0.2707  loss_ctrl_points_3: 0.1377  loss_bd_points_3: 0.1798  loss_ce_4: 0.1755  loss_texts_4: 0.2565  loss_ctrl_points_4: 0.1327  loss_bd_points_4: 0.1762  loss_ce_enc: 0.1787  loss_bezier_enc: 0.1157  total_loss: 5.144    time: 7.0387  last_time: 6.8789  data_time: 0.0037  last_data_time: 0.0037   lr: 1e-05  max_mem: 6977M
[04/29 18:38:06] d2.utils.events INFO:  eta: 2 days, 8:45:34  iter: 619  loss_ce: 0.1721  loss_texts: 0.2518  loss_ctrl_points: 0.1344  loss_bd_points: 0.1767  loss_ce_0: 0.1944  loss_texts_0: 0.3526  loss_ctrl_points_0: 0.141  loss_bd_points_0: 0.1753  loss_ce_1: 0.1791  loss_texts_1: 0.3125  loss_ctrl_points_1: 0.1424  loss_bd_points_1: 0.1806  loss_ce_2: 0.1766  loss_texts_2: 0.2871  loss_ctrl_points_2: 0.1436  loss_bd_points_2: 0.1832  loss_ce_3: 0.181  loss_texts_3: 0.2707  loss_ctrl_points_3: 0.1377  loss_bd_points_3: 0.1798  loss_ce_4: 0.1755  loss_texts_4: 0.2565  loss_ctrl_points_4: 0.1327  loss_bd_points_4: 0.1762  loss_ce_enc: 0.1787  loss_bezier_enc: 0.1157  total_loss: 5.144    time: 7.0386  last_time: 6.9374  data_time: 0.0037  last_data_time: 0.0037   lr: 1e-05  max_mem: 6977M
[04/29 18:40:34] d2.utils.events INFO:  eta: 2 days, 8:40:00  iter: 639  loss_ce: 0.1648  loss_texts: 0.2319  loss_ctrl_points: 0.1237  loss_bd_points: 0.1684  loss_ce_0: 0.1869  loss_texts_0: 0.327  loss_ctrl_points_0: 0.1238  loss_bd_points_0: 0.1762  loss_ce_1: 0.1795  loss_texts_1: 0.2928  loss_ctrl_points_1: 0.1239  loss_bd_points_1: 0.1694  loss_ce_2: 0.1736  loss_texts_2: 0.2599  loss_ctrl_points_2: 0.1239  loss_bd_points_2: 0.1702  loss_ce_3: 0.1669  loss_texts_3: 0.2437  loss_ctrl_points_3: 0.1233  loss_bd_points_3: 0.1702  loss_ce_4: 0.1661  loss_texts_4: 0.2333  loss_ctrl_points_4: 0.1236  loss_bd_points_4: 0.1707  loss_ce_enc: 0.1747  loss_bezier_enc: 0.1112  total_loss: 4.315    time: 7.0495  last_time: 6.5620  data_time: 0.0037  last_data_time: 0.0036   lr: 1e-05  max_mem: 6977M
[04/29 18:40:34] d2.utils.events INFO:  eta: 2 days, 8:39:16  iter: 639  loss_ce: 0.1648  loss_texts: 0.2319  loss_ctrl_points: 0.1237  loss_bd_points: 0.1684  loss_ce_0: 0.1869  loss_texts_0: 0.327  loss_ctrl_points_0: 0.1238  loss_bd_points_0: 0.1762  loss_ce_1: 0.1795  loss_texts_1: 0.2928  loss_ctrl_points_1: 0.1239  loss_bd_points_1: 0.1694  loss_ce_2: 0.1736  loss_texts_2: 0.2599  loss_ctrl_points_2: 0.1239  loss_bd_points_2: 0.1702  loss_ce_3: 0.1669  loss_texts_3: 0.2437  loss_ctrl_points_3: 0.1233  loss_bd_points_3: 0.1702  loss_ce_4: 0.1661  loss_texts_4: 0.2333  loss_ctrl_points_4: 0.1236  loss_bd_points_4: 0.1707  loss_ce_enc: 0.1747  loss_bezier_enc: 0.1112  total_loss: 4.315    time: 7.0491  last_time: 6.6349  data_time: 0.0037  last_data_time: 0.0036   lr: 1e-05  max_mem: 6977M
[04/29 18:42:57] d2.utils.events INFO:  eta: 2 days, 8:33:16  iter: 659  loss_ce: 0.1466  loss_texts: 0.1903  loss_ctrl_points: 0.1141  loss_bd_points: 0.1407  loss_ce_0: 0.175  loss_texts_0: 0.2946  loss_ctrl_points_0: 0.123  loss_bd_points_0: 0.1641  loss_ce_1: 0.1672  loss_texts_1: 0.2525  loss_ctrl_points_1: 0.1181  loss_bd_points_1: 0.1589  loss_ce_2: 0.154  loss_texts_2: 0.2148  loss_ctrl_points_2: 0.1177  loss_bd_points_2: 0.1597  loss_ce_3: 0.1524  loss_texts_3: 0.1946  loss_ctrl_points_3: 0.1159  loss_bd_points_3: 0.1569  loss_ce_4: 0.1479  loss_texts_4: 0.1934  loss_ctrl_points_4: 0.1133  loss_bd_points_4: 0.1405  loss_ce_enc: 0.1646  loss_bezier_enc: 0.1128  total_loss: 4.289    time: 7.0526  last_time: 7.5577  data_time: 0.0038  last_data_time: 0.0038   lr: 1e-05  max_mem: 6977M
[04/29 18:42:57] d2.utils.events INFO:  eta: 2 days, 8:33:16  iter: 659  loss_ce: 0.1466  loss_texts: 0.1903  loss_ctrl_points: 0.1141  loss_bd_points: 0.1407  loss_ce_0: 0.175  loss_texts_0: 0.2946  loss_ctrl_points_0: 0.123  loss_bd_points_0: 0.1641  loss_ce_1: 0.1672  loss_texts_1: 0.2525  loss_ctrl_points_1: 0.1181  loss_bd_points_1: 0.1589  loss_ce_2: 0.154  loss_texts_2: 0.2148  loss_ctrl_points_2: 0.1177  loss_bd_points_2: 0.1597  loss_ce_3: 0.1524  loss_texts_3: 0.1946  loss_ctrl_points_3: 0.1159  loss_bd_points_3: 0.1569  loss_ce_4: 0.1479  loss_texts_4: 0.1934  loss_ctrl_points_4: 0.1133  loss_bd_points_4: 0.1405  loss_ce_enc: 0.1646  loss_bezier_enc: 0.1128  total_loss: 4.289    time: 7.0530  last_time: 7.6110  data_time: 0.0038  last_data_time: 0.0038   lr: 1e-05  max_mem: 6977M
[04/29 18:45:12] d2.utils.events INFO:  eta: 2 days, 8:19:33  iter: 679  loss_ce: 0.1679  loss_texts: 0.2204  loss_ctrl_points: 0.143  loss_bd_points: 0.1883  loss_ce_0: 0.1962  loss_texts_0: 0.3343  loss_ctrl_points_0: 0.1375  loss_bd_points_0: 0.1983  loss_ce_1: 0.187  loss_texts_1: 0.2737  loss_ctrl_points_1: 0.1449  loss_bd_points_1: 0.1941  loss_ce_2: 0.1797  loss_texts_2: 0.2404  loss_ctrl_points_2: 0.1471  loss_bd_points_2: 0.1868  loss_ce_3: 0.1751  loss_texts_3: 0.236  loss_ctrl_points_3: 0.1419  loss_bd_points_3: 0.1894  loss_ce_4: 0.1715  loss_texts_4: 0.2277  loss_ctrl_points_4: 0.1423  loss_bd_points_4: 0.1892  loss_ce_enc: 0.1901  loss_bezier_enc: 0.1074  total_loss: 5.1    time: 7.0449  last_time: 5.5618  data_time: 0.0037  last_data_time: 0.0040   lr: 1e-05  max_mem: 6977M
[04/29 18:45:12] d2.utils.events INFO:  eta: 2 days, 8:19:33  iter: 679  loss_ce: 0.1679  loss_texts: 0.2204  loss_ctrl_points: 0.143  loss_bd_points: 0.1883  loss_ce_0: 0.1962  loss_texts_0: 0.3343  loss_ctrl_points_0: 0.1375  loss_bd_points_0: 0.1983  loss_ce_1: 0.187  loss_texts_1: 0.2737  loss_ctrl_points_1: 0.1449  loss_bd_points_1: 0.1941  loss_ce_2: 0.1797  loss_texts_2: 0.2404  loss_ctrl_points_2: 0.1471  loss_bd_points_2: 0.1868  loss_ce_3: 0.1751  loss_texts_3: 0.236  loss_ctrl_points_3: 0.1419  loss_bd_points_3: 0.1894  loss_ce_4: 0.1715  loss_texts_4: 0.2277  loss_ctrl_points_4: 0.1423  loss_bd_points_4: 0.1892  loss_ce_enc: 0.1901  loss_bezier_enc: 0.1074  total_loss: 5.1    time: 7.0438  last_time: 5.6514  data_time: 0.0037  last_data_time: 0.0040   lr: 1e-05  max_mem: 6977M
[04/29 18:47:25] d2.utils.events INFO:  eta: 2 days, 7:59:27  iter: 699  loss_ce: 0.1458  loss_texts: 0.2255  loss_ctrl_points: 0.1124  loss_bd_points: 0.1545  loss_ce_0: 0.1847  loss_texts_0: 0.3284  loss_ctrl_points_0: 0.1101  loss_bd_points_0: 0.1612  loss_ce_1: 0.1736  loss_texts_1: 0.2771  loss_ctrl_points_1: 0.1147  loss_bd_points_1: 0.1576  loss_ce_2: 0.1596  loss_texts_2: 0.2544  loss_ctrl_points_2: 0.112  loss_bd_points_2: 0.157  loss_ce_3: 0.1532  loss_texts_3: 0.228  loss_ctrl_points_3: 0.1118  loss_bd_points_3: 0.1556  loss_ce_4: 0.1487  loss_texts_4: 0.2149  loss_ctrl_points_4: 0.1121  loss_bd_points_4: 0.1549  loss_ce_enc: 0.1655  loss_bezier_enc: 0.1087  total_loss: 4.423    time: 7.0320  last_time: 6.2128  data_time: 0.0037  last_data_time: 0.0036   lr: 1e-05  max_mem: 6977M
[04/29 18:47:25] d2.utils.events INFO:  eta: 2 days, 7:59:17  iter: 699  loss_ce: 0.1458  loss_texts: 0.2255  loss_ctrl_points: 0.1124  loss_bd_points: 0.1545  loss_ce_0: 0.1847  loss_texts_0: 0.3284  loss_ctrl_points_0: 0.1101  loss_bd_points_0: 0.1612  loss_ce_1: 0.1736  loss_texts_1: 0.2771  loss_ctrl_points_1: 0.1147  loss_bd_points_1: 0.1576  loss_ce_2: 0.1596  loss_texts_2: 0.2544  loss_ctrl_points_2: 0.112  loss_bd_points_2: 0.157  loss_ce_3: 0.1532  loss_texts_3: 0.228  loss_ctrl_points_3: 0.1118  loss_bd_points_3: 0.1556  loss_ce_4: 0.1487  loss_texts_4: 0.2149  loss_ctrl_points_4: 0.1121  loss_bd_points_4: 0.1549  loss_ce_enc: 0.1655  loss_bezier_enc: 0.1087  total_loss: 4.423    time: 7.0314  last_time: 6.2682  data_time: 0.0037  last_data_time: 0.0036   lr: 1e-05  max_mem: 6977M
[04/29 18:49:41] d2.utils.events INFO:  eta: 2 days, 7:30:55  iter: 719  loss_ce: 0.1506  loss_texts: 0.1931  loss_ctrl_points: 0.1119  loss_bd_points: 0.1578  loss_ce_0: 0.1777  loss_texts_0: 0.3077  loss_ctrl_points_0: 0.1234  loss_bd_points_0: 0.1588  loss_ce_1: 0.1741  loss_texts_1: 0.2488  loss_ctrl_points_1: 0.1098  loss_bd_points_1: 0.1484  loss_ce_2: 0.1565  loss_texts_2: 0.2127  loss_ctrl_points_2: 0.1115  loss_bd_points_2: 0.1589  loss_ce_3: 0.1602  loss_texts_3: 0.2039  loss_ctrl_points_3: 0.1144  loss_bd_points_3: 0.1609  loss_ce_4: 0.1532  loss_texts_4: 0.1908  loss_ctrl_points_4: 0.1132  loss_bd_points_4: 0.1582  loss_ce_enc: 0.1681  loss_bezier_enc: 0.1094  total_loss: 4.345    time: 7.0257  last_time: 7.1914  data_time: 0.0037  last_data_time: 0.0041   lr: 1e-05  max_mem: 6977M
[04/29 18:49:41] d2.utils.events INFO:  eta: 2 days, 7:30:55  iter: 719  loss_ce: 0.1506  loss_texts: 0.1931  loss_ctrl_points: 0.1119  loss_bd_points: 0.1578  loss_ce_0: 0.1777  loss_texts_0: 0.3077  loss_ctrl_points_0: 0.1234  loss_bd_points_0: 0.1588  loss_ce_1: 0.1741  loss_texts_1: 0.2488  loss_ctrl_points_1: 0.1098  loss_bd_points_1: 0.1484  loss_ce_2: 0.1565  loss_texts_2: 0.2127  loss_ctrl_points_2: 0.1115  loss_bd_points_2: 0.1589  loss_ce_3: 0.1602  loss_texts_3: 0.2039  loss_ctrl_points_3: 0.1144  loss_bd_points_3: 0.1609  loss_ce_4: 0.1532  loss_texts_4: 0.1908  loss_ctrl_points_4: 0.1132  loss_bd_points_4: 0.1582  loss_ce_enc: 0.1681  loss_bezier_enc: 0.1094  total_loss: 4.345    time: 7.0259  last_time: 7.2431  data_time: 0.0037  last_data_time: 0.0041   lr: 1e-05  max_mem: 6977M
[04/29 18:52:02] d2.utils.events INFO:  eta: 2 days, 7:22:02  iter: 739  loss_ce: 0.1566  loss_texts: 0.2257  loss_ctrl_points: 0.101  loss_bd_points: 0.134  loss_ce_0: 0.1841  loss_texts_0: 0.316  loss_ctrl_points_0: 0.1069  loss_bd_points_0: 0.1381  loss_ce_1: 0.1749  loss_texts_1: 0.2726  loss_ctrl_points_1: 0.1051  loss_bd_points_1: 0.1368  loss_ce_2: 0.1698  loss_texts_2: 0.2475  loss_ctrl_points_2: 0.1061  loss_bd_points_2: 0.1403  loss_ce_3: 0.165  loss_texts_3: 0.2346  loss_ctrl_points_3: 0.1046  loss_bd_points_3: 0.1361  loss_ce_4: 0.1575  loss_texts_4: 0.2214  loss_ctrl_points_4: 0.1022  loss_bd_points_4: 0.1301  loss_ce_enc: 0.1712  loss_bezier_enc: 0.1027  total_loss: 4.275    time: 7.0268  last_time: 6.5909  data_time: 0.0037  last_data_time: 0.0036   lr: 1e-05  max_mem: 6977M
[04/29 18:52:02] d2.utils.events INFO:  eta: 2 days, 7:21:55  iter: 739  loss_ce: 0.1566  loss_texts: 0.2257  loss_ctrl_points: 0.101  loss_bd_points: 0.134  loss_ce_0: 0.1841  loss_texts_0: 0.316  loss_ctrl_points_0: 0.1069  loss_bd_points_0: 0.1381  loss_ce_1: 0.1749  loss_texts_1: 0.2726  loss_ctrl_points_1: 0.1051  loss_bd_points_1: 0.1368  loss_ce_2: 0.1698  loss_texts_2: 0.2475  loss_ctrl_points_2: 0.1061  loss_bd_points_2: 0.1403  loss_ce_3: 0.165  loss_texts_3: 0.2346  loss_ctrl_points_3: 0.1046  loss_bd_points_3: 0.1361  loss_ce_4: 0.1575  loss_texts_4: 0.2214  loss_ctrl_points_4: 0.1022  loss_bd_points_4: 0.1301  loss_ce_enc: 0.1712  loss_bezier_enc: 0.1027  total_loss: 4.275    time: 7.0266  last_time: 6.6557  data_time: 0.0037  last_data_time: 0.0036   lr: 1e-05  max_mem: 6977M
[04/29 18:57:15] detectron2 INFO: Rank of current process: 0. World size: 1
[04/29 18:57:15] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/kylee/DeepSolo_LangPriors/detectron2/detectron2
Compiler                         GCC 9.5
CUDA compiler                    CUDA 11.3
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.12.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 3060 (arch=8.6)
Driver version                   550.120
CUDA_HOME                        /usr
Pillow                           9.0.1
torchvision                      0.13.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[04/29 18:57:15] detectron2 INFO: Command line arguments: Namespace(config_file='configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml', resume=True, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50152', opts=[])
[04/29 18:57:15] detectron2 INFO: Contents of args.config_file=configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml:
_BASE_: "../Base_det.yaml"

MODEL:
  WEIGHTS: "output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth"
  #WEIGHTS: "output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth"
  TRANSFORMER:
    INFERENCE_TH_TEST: 0.2

DATASETS:
  TRAIN: ("ic15_train",)
  TEST: ("ic15_test",)

INPUT:
  MIN_SIZE_TRAIN: (640, 700, 760, 820, 880, 940, 1000)
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MAX_SIZE_TEST: 4000
  CROP:
    ENABLED: False
  ROTATE: False

SOLVER:
  AMP:
    ENABLED: True  # Enable Automatic Mixed Precision
  IMS_PER_BATCH: 2
  BASE_LR: 1e-5
  LR_BACKBONE: 1e-6
  WARMUP_ITERS: 250
  STEPS: (15000, 25000)  # no step
  MAX_ITER: 30000
  GAMMA: 0.1
  CHECKPOINT_PERIOD: 1000

TEST:
  EVAL_PERIOD: 1000
  # 1 - Generic, 2 - Weak, 3 - Strong (for icdar2015)
  LEXICON_TYPE: 3

OUTPUT_DIR: "output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15"

[04/29 18:57:15] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ic15_test
  TRAIN:
  - ic15_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    CROP_INSTANCE: false
    ENABLED: false
    SIZE:
    - 0.1
    - 0.1
    TYPE: relative_range
  FORMAT: RGB
  HFLIP_TRAIN: false
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 4000
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MIN_SIZE_TRAIN:
  - 640
  - 700
  - 760
  - 820
  - 880
  - 940
  - 1000
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  ROTATE: false
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    ANTI_ALIAS: false
    FREEZE_AT: 2
    NAME: build_resnet_backbone
  BASIS_MODULE:
    ANN_SET: coco
    COMMON_STRIDE: 8
    CONVS_DIM: 128
    IN_FEATURES:
    - p3
    - p4
    - p5
    LOSS_ON: false
    LOSS_WEIGHT: 0.3
    NAME: ProtoNet
    NORM: SyncBN
    NUM_BASES: 4
    NUM_CLASSES: 80
    NUM_CONVS: 3
  BATEXT:
    CANONICAL_SIZE: 96
    CONV_DIM: 256
    CUSTOM_DICT: ''
    IN_FEATURES:
    - p2
    - p3
    - p4
    NUM_CHARS: 25
    NUM_CONV: 2
    POOLER_RESOLUTION:
    - 8
    - 32
    POOLER_SCALES:
    - 0.25
    - 0.125
    - 0.0625
    RECOGNITION_LOSS: ctc
    RECOGNIZER: attn
    SAMPLING_RATIO: 1
    USE_AET: false
    USE_COORDCONV: false
    VOC_SIZE: 96
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: false
  META_ARCHITECTURE: TransformerPureDetector
  MOBILENET: false
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_INTERVAL: 1
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  SWIN:
    DROP_PATH_RATE: 0.2
    TYPE: tiny
  TOP_MODULE:
    DIM: 16
    NAME: conv
  TRANSFORMER:
    AUX_LOSS: true
    BOUNDARY_HEAD: true
    CUSTOM_DICT: ''
    DEC_LAYERS: 6
    DEC_N_POINTS: 4
    DIM_FEEDFORWARD: 1024
    DROPOUT: 0.0
    ENABLED: true
    ENC_LAYERS: 6
    ENC_N_POINTS: 4
    HIDDEN_DIM: 256
    INFERENCE_TH_TEST: 0.2
    LOSS:
      AUX_LOSS: true
      BEZIER_CLASS_WEIGHT: 1.0
      BEZIER_COORD_WEIGHT: 1.0
      BEZIER_SAMPLE_POINTS: 25
      BOUNDARY_WEIGHT: 0.5
      FOCAL_ALPHA: 0.25
      FOCAL_GAMMA: 2.0
      POINT_CLASS_WEIGHT: 1.0
      POINT_COORD_WEIGHT: 1.0
      POINT_TEXT_WEIGHT: 0.5
    NHEADS: 8
    NUM_FEATURE_LEVELS: 4
    NUM_POINTS: 25
    NUM_QUERIES: 100
    POSITION_EMBEDDING_SCALE: 6.283185307179586
    TEMPERATURE: 10000
    VOC_SIZE: 37
  ViTAEv2:
    DROP_PATH_RATE: 0.2
    TYPE: vitaev2_s
  WEIGHTS: output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth
OUTPUT_DIR: output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15
SEED: 42
SOLVER:
  AMP:
    ENABLED: true
  BASE_LR: 1.0e-05
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.1
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 2
  LR_BACKBONE: 1.0e-06
  LR_BACKBONE_NAMES:
  - backbone.0
  LR_LINEAR_PROJ_MULT: 1.0
  LR_LINEAR_PROJ_NAMES:
  - reference_points
  - sampling_offsets
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 30000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 15000
  - 25000
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 250
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  LEXICON_TYPE: 3
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/29 18:57:15] detectron2 INFO: Full config saved to output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/config.yaml
[04/29 18:57:17] d2.engine.defaults INFO: Model:
TransformerPureDetector(
  (detection_transformer): DETECTION_TRANSFORMER(
    (backbone): Joiner(
      (0): MaskedBackbone(
        (backbone): ResNet(
          (stem): BasicStem(
            (conv1): Conv2d(
              3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
          )
          (res2): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv1): Conv2d(
                64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
          )
          (res3): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv1): Conv2d(
                256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
          )
          (res4): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
              (conv1): Conv2d(
                512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (4): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (5): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
          )
          (res5): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
              (conv1): Conv2d(
                1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
          )
        )
      )
      (1): PositionalEncoding2D()
    )
    (point_embed): Embedding(2500, 256)
    (transformer): DeformableTransformer(
      (encoder): DeformableTransformerEncoder(
        (layers): ModuleList(
          (0): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (decoder): DeformableCompositeTransformerDecoder(
        (layers): ModuleList(
          (0): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ctrl_point_coord): ModuleList(
          (0): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (1): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (2): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (3): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (4): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (5): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
        )
      )
      (enc_output): Linear(in_features=256, out_features=256, bias=True)
      (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (bezier_coord_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=8, bias=True)
        )
      )
      (bezier_class_embed): Linear(in_features=256, out_features=1, bias=True)
    )
    (input_proj): ModuleList(
      (0): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (1): Sequential(
        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (2): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (3): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (bezier_proposal_coord): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=8, bias=True)
      )
    )
    (bezier_proposal_class): Linear(in_features=256, out_features=1, bias=True)
    (ctrl_point_coord): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
    )
    (ctrl_point_class): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (ctrl_point_text): ModuleList(
      (0): Linear(in_features=256, out_features=38, bias=True)
      (1): Linear(in_features=256, out_features=38, bias=True)
      (2): Linear(in_features=256, out_features=38, bias=True)
      (3): Linear(in_features=256, out_features=38, bias=True)
      (4): Linear(in_features=256, out_features=38, bias=True)
      (5): Linear(in_features=256, out_features=38, bias=True)
    )
    (boundary_offset): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (enc_matcher): BezierHungarianMatcher(
      (bezier_sampler): BezierSampler()
    )
    (dec_matcher): CtrlPointHungarianMatcher()
    (bezier_sampler): BezierSampler()
  )
)
[04/29 18:57:17] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 700, 760, 820, 880, 940, 1000), max_size=2000, sample_style='choice'), RandomFlip()]
[04/29 18:57:17] adet.data.dataset_mapper INFO: Rebuilding the augmentations. The previous augmentations will be overridden.
[04/29 18:57:17] adet.data.detection_utils INFO: Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 700, 760, 820, 880, 940, 1000), max_size=2000, sample_style='choice'), RandomContrast(intensity_min=0.3, intensity_max=1.7), RandomBrightness(intensity_min=0.3, intensity_max=1.7), RandomLighting(scale=1.1394267984578836), RandomSaturation(intensity_min=0.3, intensity_max=1.7)]
[04/29 18:57:17] adet.data.datasets.text INFO: Loaded 1000 images in COCO format from datasets/ic15/train_37voc.json
[04/29 18:57:17] d2.data.build INFO: Removed 0 images with no usable annotations. 979 images left.
[04/29 18:57:17] d2.data.build INFO: Distribution of instances among all 1 categories:
[36m|  category  | #instances   |
|:----------:|:-------------|
|    text    | 4441         |
|            |              |[0m
[04/29 18:57:17] d2.data.build INFO: Using training sampler TrainingSampler
[04/29 18:57:17] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[04/29 18:57:17] d2.data.common INFO: Serializing 979 elements to byte tensors and concatenating them all ...
[04/29 18:57:17] d2.data.common INFO: Serialized dataset takes 6.51 MiB
[04/29 18:57:17] d2.data.build INFO: Making batched data loader with batch_size=2
[04/29 18:57:18] d2.engine.defaults INFO: Model:
TransformerPureDetector(
  (detection_transformer): DETECTION_TRANSFORMER(
    (backbone): Joiner(
      (0): MaskedBackbone(
        (backbone): ResNet(
          (stem): BasicStem(
            (conv1): Conv2d(
              3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
          )
          (res2): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv1): Conv2d(
                64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
          )
          (res3): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv1): Conv2d(
                256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
          )
          (res4): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
              (conv1): Conv2d(
                512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (4): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (5): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
          )
          (res5): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
              (conv1): Conv2d(
                1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
          )
        )
      )
      (1): PositionalEncoding2D()
    )
    (point_embed): Embedding(2500, 256)
    (transformer): DeformableTransformer(
      (encoder): DeformableTransformerEncoder(
        (layers): ModuleList(
          (0): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (decoder): DeformableCompositeTransformerDecoder(
        (layers): ModuleList(
          (0): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ctrl_point_coord): ModuleList(
          (0): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (1): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (2): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (3): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (4): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (5): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
        )
      )
      (enc_output): Linear(in_features=256, out_features=256, bias=True)
      (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (bezier_coord_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=8, bias=True)
        )
      )
      (bezier_class_embed): Linear(in_features=256, out_features=1, bias=True)
    )
    (input_proj): ModuleList(
      (0): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (1): Sequential(
        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (2): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (3): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (bezier_proposal_coord): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=8, bias=True)
      )
    )
    (bezier_proposal_class): Linear(in_features=256, out_features=1, bias=True)
    (ctrl_point_coord): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
    )
    (ctrl_point_class): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (ctrl_point_text): ModuleList(
      (0): Linear(in_features=256, out_features=38, bias=True)
      (1): Linear(in_features=256, out_features=38, bias=True)
      (2): Linear(in_features=256, out_features=38, bias=True)
      (3): Linear(in_features=256, out_features=38, bias=True)
      (4): Linear(in_features=256, out_features=38, bias=True)
      (5): Linear(in_features=256, out_features=38, bias=True)
    )
    (boundary_offset): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (enc_matcher): BezierHungarianMatcher(
      (bezier_sampler): BezierSampler()
    )
    (dec_matcher): CtrlPointHungarianMatcher()
    (bezier_sampler): BezierSampler()
  )
)
[04/29 18:57:18] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 700, 760, 820, 880, 940, 1000), max_size=2000, sample_style='choice'), RandomFlip()]
[04/29 18:57:18] adet.data.dataset_mapper INFO: Rebuilding the augmentations. The previous augmentations will be overridden.
[04/29 18:57:18] adet.data.detection_utils INFO: Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 700, 760, 820, 880, 940, 1000), max_size=2000, sample_style='choice'), RandomContrast(intensity_min=0.3, intensity_max=1.7), RandomBrightness(intensity_min=0.3, intensity_max=1.7), RandomLighting(scale=0.5250107552226669), RandomSaturation(intensity_min=0.3, intensity_max=1.7)]
[04/29 18:57:18] adet.data.datasets.text INFO: Loaded 1000 images in COCO format from datasets/ic15/train_37voc.json
[04/29 18:57:18] d2.data.build INFO: Removed 0 images with no usable annotations. 979 images left.
[04/29 18:57:18] d2.data.build INFO: Using training sampler TrainingSampler
[04/29 18:57:18] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[04/29 18:57:18] d2.data.common INFO: Serializing 979 elements to byte tensors and concatenating them all ...
[04/29 18:57:18] d2.data.common INFO: Serialized dataset takes 6.51 MiB
[04/29 18:57:18] d2.data.build INFO: Making batched data loader with batch_size=2
[04/29 18:57:18] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth ...
[04/29 18:57:18] fvcore.common.checkpoint INFO: [Checkpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth ...
[04/29 18:57:18] adet.trainer INFO: Starting training from iteration 1000
[04/29 18:57:38] detectron2 INFO: Rank of current process: 0. World size: 1
[04/29 18:57:38] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/kylee/DeepSolo_LangPriors/detectron2/detectron2
Compiler                         GCC 9.5
CUDA compiler                    CUDA 11.3
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.12.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 3060 (arch=8.6)
Driver version                   550.120
CUDA_HOME                        /usr
Pillow                           9.0.1
torchvision                      0.13.1+cu113 @/home/kylee/anaconda3/envs/py39_deepsolo_lang_amp/lib/python3.9/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.10.0
-------------------------------  -------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[04/29 18:57:38] detectron2 INFO: Command line arguments: Namespace(config_file='configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml', resume=True, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50152', opts=[])
[04/29 18:57:38] detectron2 INFO: Contents of args.config_file=configs/R_50/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml:
_BASE_: "../Base_det.yaml"

MODEL:
  WEIGHTS: "output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth"
  #WEIGHTS: "output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth"
  TRANSFORMER:
    INFERENCE_TH_TEST: 0.2

DATASETS:
  TRAIN: ("ic15_train",)
  TEST: ("ic15_test",)

INPUT:
  MIN_SIZE_TRAIN: (640, 700, 760, 820, 880, 940, 1000)
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MAX_SIZE_TEST: 4000
  CROP:
    ENABLED: False
  ROTATE: False

SOLVER:
  AMP:
    ENABLED: True  # Enable Automatic Mixed Precision
  IMS_PER_BATCH: 2
  BASE_LR: 2.0e-6
  LR_BACKBONE: 2.0e-7
  WARMUP_ITERS: 250
  STEPS: (100000,)  # no step
  MAX_ITER: 30000
  GAMMA: 0.1
  CHECKPOINT_PERIOD: 1000

TEST:
  EVAL_PERIOD: 1000
  # 1 - Generic, 2 - Weak, 3 - Strong (for icdar2015)
  LEXICON_TYPE: 3

OUTPUT_DIR: "output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15"

[04/29 18:57:38] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_SQRT: true
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ic15_test
  TRAIN:
  - ic15_train
FLOAT32_PRECISION: ''
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    CROP_INSTANCE: false
    ENABLED: false
    SIZE:
    - 0.1
    - 0.1
    TYPE: relative_range
  FORMAT: RGB
  HFLIP_TRAIN: false
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 4000
  MAX_SIZE_TRAIN: 2000
  MIN_SIZE_TEST: 1440
  MIN_SIZE_TRAIN:
  - 640
  - 700
  - 760
  - 820
  - 880
  - 940
  - 1000
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  ROTATE: false
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    ANTI_ALIAS: false
    FREEZE_AT: 2
    NAME: build_resnet_backbone
  BASIS_MODULE:
    ANN_SET: coco
    COMMON_STRIDE: 8
    CONVS_DIM: 128
    IN_FEATURES:
    - p3
    - p4
    - p5
    LOSS_ON: false
    LOSS_WEIGHT: 0.3
    NAME: ProtoNet
    NORM: SyncBN
    NUM_BASES: 4
    NUM_CLASSES: 80
    NUM_CONVS: 3
  BATEXT:
    CANONICAL_SIZE: 96
    CONV_DIM: 256
    CUSTOM_DICT: ''
    IN_FEATURES:
    - p2
    - p3
    - p4
    NUM_CHARS: 25
    NUM_CONV: 2
    POOLER_RESOLUTION:
    - 8
    - 32
    POOLER_SCALES:
    - 0.25
    - 0.125
    - 0.0625
    RECOGNITION_LOSS: ctc
    RECOGNIZER: attn
    SAMPLING_RATIO: 1
    USE_AET: false
    USE_COORDCONV: false
    VOC_SIZE: 96
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: false
  META_ARCHITECTURE: TransformerPureDetector
  MOBILENET: false
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_INTERVAL: 1
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  SWIN:
    DROP_PATH_RATE: 0.2
    TYPE: tiny
  TOP_MODULE:
    DIM: 16
    NAME: conv
  TRANSFORMER:
    AUX_LOSS: true
    BOUNDARY_HEAD: true
    CUSTOM_DICT: ''
    DEC_LAYERS: 6
    DEC_N_POINTS: 4
    DIM_FEEDFORWARD: 1024
    DROPOUT: 0.0
    ENABLED: true
    ENC_LAYERS: 6
    ENC_N_POINTS: 4
    HIDDEN_DIM: 256
    INFERENCE_TH_TEST: 0.2
    LOSS:
      AUX_LOSS: true
      BEZIER_CLASS_WEIGHT: 1.0
      BEZIER_COORD_WEIGHT: 1.0
      BEZIER_SAMPLE_POINTS: 25
      BOUNDARY_WEIGHT: 0.5
      FOCAL_ALPHA: 0.25
      FOCAL_GAMMA: 2.0
      POINT_CLASS_WEIGHT: 1.0
      POINT_COORD_WEIGHT: 1.0
      POINT_TEXT_WEIGHT: 0.5
    NHEADS: 8
    NUM_FEATURE_LEVELS: 4
    NUM_POINTS: 25
    NUM_QUERIES: 100
    POSITION_EMBEDDING_SCALE: 6.283185307179586
    TEMPERATURE: 10000
    VOC_SIZE: 37
  ViTAEv2:
    DROP_PATH_RATE: 0.2
    TYPE: vitaev2_s
  WEIGHTS: output/R50/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth
OUTPUT_DIR: output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15
SEED: 42
SOLVER:
  AMP:
    ENABLED: true
  BASE_LR: 2.0e-06
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.1
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 2
  LR_BACKBONE: 2.0e-07
  LR_BACKBONE_NAMES:
  - backbone.0
  LR_LINEAR_PROJ_MULT: 1.0
  LR_LINEAR_PROJ_NAMES:
  - reference_points
  - sampling_offsets
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 30000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 100000
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 250
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 1000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  LEXICON_TYPE: 3
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/29 18:57:38] detectron2 INFO: Full config saved to output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/config.yaml
[04/29 18:57:40] d2.engine.defaults INFO: Model:
TransformerPureDetector(
  (detection_transformer): DETECTION_TRANSFORMER(
    (backbone): Joiner(
      (0): MaskedBackbone(
        (backbone): ResNet(
          (stem): BasicStem(
            (conv1): Conv2d(
              3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
          )
          (res2): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv1): Conv2d(
                64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
          )
          (res3): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv1): Conv2d(
                256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
          )
          (res4): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
              (conv1): Conv2d(
                512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (4): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (5): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
          )
          (res5): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
              (conv1): Conv2d(
                1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
          )
        )
      )
      (1): PositionalEncoding2D()
    )
    (point_embed): Embedding(2500, 256)
    (transformer): DeformableTransformer(
      (encoder): DeformableTransformerEncoder(
        (layers): ModuleList(
          (0): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (decoder): DeformableCompositeTransformerDecoder(
        (layers): ModuleList(
          (0): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ctrl_point_coord): ModuleList(
          (0): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (1): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (2): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (3): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (4): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (5): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
        )
      )
      (enc_output): Linear(in_features=256, out_features=256, bias=True)
      (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (bezier_coord_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=8, bias=True)
        )
      )
      (bezier_class_embed): Linear(in_features=256, out_features=1, bias=True)
    )
    (input_proj): ModuleList(
      (0): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (1): Sequential(
        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (2): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (3): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (bezier_proposal_coord): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=8, bias=True)
      )
    )
    (bezier_proposal_class): Linear(in_features=256, out_features=1, bias=True)
    (ctrl_point_coord): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
    )
    (ctrl_point_class): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (ctrl_point_text): ModuleList(
      (0): Linear(in_features=256, out_features=38, bias=True)
      (1): Linear(in_features=256, out_features=38, bias=True)
      (2): Linear(in_features=256, out_features=38, bias=True)
      (3): Linear(in_features=256, out_features=38, bias=True)
      (4): Linear(in_features=256, out_features=38, bias=True)
      (5): Linear(in_features=256, out_features=38, bias=True)
    )
    (boundary_offset): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (enc_matcher): BezierHungarianMatcher(
      (bezier_sampler): BezierSampler()
    )
    (dec_matcher): CtrlPointHungarianMatcher()
    (bezier_sampler): BezierSampler()
  )
)
[04/29 18:57:40] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 700, 760, 820, 880, 940, 1000), max_size=2000, sample_style='choice'), RandomFlip()]
[04/29 18:57:40] adet.data.dataset_mapper INFO: Rebuilding the augmentations. The previous augmentations will be overridden.
[04/29 18:57:40] adet.data.detection_utils INFO: Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 700, 760, 820, 880, 940, 1000), max_size=2000, sample_style='choice'), RandomContrast(intensity_min=0.3, intensity_max=1.7), RandomBrightness(intensity_min=0.3, intensity_max=1.7), RandomLighting(scale=1.1394267984578836), RandomSaturation(intensity_min=0.3, intensity_max=1.7)]
[04/29 18:57:40] adet.data.datasets.text INFO: Loaded 1000 images in COCO format from datasets/ic15/train_37voc.json
[04/29 18:57:40] d2.data.build INFO: Removed 0 images with no usable annotations. 979 images left.
[04/29 18:57:40] d2.data.build INFO: Distribution of instances among all 1 categories:
[36m|  category  | #instances   |
|:----------:|:-------------|
|    text    | 4441         |
|            |              |[0m
[04/29 18:57:40] d2.data.build INFO: Using training sampler TrainingSampler
[04/29 18:57:40] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[04/29 18:57:40] d2.data.common INFO: Serializing 979 elements to byte tensors and concatenating them all ...
[04/29 18:57:40] d2.data.common INFO: Serialized dataset takes 6.51 MiB
[04/29 18:57:40] d2.data.build INFO: Making batched data loader with batch_size=2
[04/29 18:57:40] d2.solver.build WARNING: SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.
[04/29 18:57:40] d2.engine.defaults INFO: Model:
TransformerPureDetector(
  (detection_transformer): DETECTION_TRANSFORMER(
    (backbone): Joiner(
      (0): MaskedBackbone(
        (backbone): ResNet(
          (stem): BasicStem(
            (conv1): Conv2d(
              3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
            )
          )
          (res2): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv1): Conv2d(
                64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv2): Conv2d(
                64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
              )
              (conv3): Conv2d(
                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
            )
          )
          (res3): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv1): Conv2d(
                256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv2): Conv2d(
                128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
              )
              (conv3): Conv2d(
                128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
            )
          )
          (res4): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
              (conv1): Conv2d(
                512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (3): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (4): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
            (5): BottleneckBlock(
              (conv1): Conv2d(
                1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv2): Conv2d(
                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
              )
              (conv3): Conv2d(
                256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
              )
            )
          )
          (res5): Sequential(
            (0): BottleneckBlock(
              (shortcut): Conv2d(
                1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
              (conv1): Conv2d(
                1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (1): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
            (2): BottleneckBlock(
              (conv1): Conv2d(
                2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv2): Conv2d(
                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
              )
              (conv3): Conv2d(
                512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
              )
            )
          )
        )
      )
      (1): PositionalEncoding2D()
    )
    (point_embed): Embedding(2500, 256)
    (transformer): DeformableTransformer(
      (encoder): DeformableTransformerEncoder(
        (layers): ModuleList(
          (0): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (decoder): DeformableCompositeTransformerDecoder(
        (layers): ModuleList(
          (0): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ctrl_point_coord): ModuleList(
          (0): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (1): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (2): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (3): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (4): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (5): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
        )
      )
      (enc_output): Linear(in_features=256, out_features=256, bias=True)
      (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (bezier_coord_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=8, bias=True)
        )
      )
      (bezier_class_embed): Linear(in_features=256, out_features=1, bias=True)
    )
    (input_proj): ModuleList(
      (0): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (1): Sequential(
        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (2): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (3): Sequential(
        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (bezier_proposal_coord): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=8, bias=True)
      )
    )
    (bezier_proposal_class): Linear(in_features=256, out_features=1, bias=True)
    (ctrl_point_coord): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
    )
    (ctrl_point_class): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (ctrl_point_text): ModuleList(
      (0): Linear(in_features=256, out_features=38, bias=True)
      (1): Linear(in_features=256, out_features=38, bias=True)
      (2): Linear(in_features=256, out_features=38, bias=True)
      (3): Linear(in_features=256, out_features=38, bias=True)
      (4): Linear(in_features=256, out_features=38, bias=True)
      (5): Linear(in_features=256, out_features=38, bias=True)
    )
    (boundary_offset): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (enc_matcher): BezierHungarianMatcher(
      (bezier_sampler): BezierSampler()
    )
    (dec_matcher): CtrlPointHungarianMatcher()
    (bezier_sampler): BezierSampler()
  )
)
[04/29 18:57:40] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 700, 760, 820, 880, 940, 1000), max_size=2000, sample_style='choice'), RandomFlip()]
[04/29 18:57:40] adet.data.dataset_mapper INFO: Rebuilding the augmentations. The previous augmentations will be overridden.
[04/29 18:57:40] adet.data.detection_utils INFO: Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 700, 760, 820, 880, 940, 1000), max_size=2000, sample_style='choice'), RandomContrast(intensity_min=0.3, intensity_max=1.7), RandomBrightness(intensity_min=0.3, intensity_max=1.7), RandomLighting(scale=0.5250107552226669), RandomSaturation(intensity_min=0.3, intensity_max=1.7)]
[04/29 18:57:40] adet.data.datasets.text INFO: Loaded 1000 images in COCO format from datasets/ic15/train_37voc.json
[04/29 18:57:40] d2.data.build INFO: Removed 0 images with no usable annotations. 979 images left.
[04/29 18:57:41] d2.data.build INFO: Using training sampler TrainingSampler
[04/29 18:57:41] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[04/29 18:57:41] d2.data.common INFO: Serializing 979 elements to byte tensors and concatenating them all ...
[04/29 18:57:41] d2.data.common INFO: Serialized dataset takes 6.51 MiB
[04/29 18:57:41] d2.data.build INFO: Making batched data loader with batch_size=2
[04/29 18:57:41] d2.solver.build WARNING: SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.
[04/29 18:57:41] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth ...
[04/29 18:57:41] fvcore.common.checkpoint INFO: [Checkpointer] Loading from output/R50/150k_tt_mlt_13_15_textocr/finetune/ic15/model_0000999.pth ...
[04/29 18:57:41] adet.trainer INFO: Starting training from iteration 1000
[04/29 19:00:08] d2.utils.events INFO:  eta: 2 days, 12:23:17  iter: 1019  loss_ce: 1.142  loss_texts: 8.316  loss_ctrl_points: 16.51  loss_bd_points: 16.56  loss_ce_0: 1.089  loss_texts_0: 8.635  loss_ctrl_points_0: 16.52  loss_bd_points_0: 16.57  loss_ce_1: 1.003  loss_texts_1: 7.653  loss_ctrl_points_1: 16.55  loss_bd_points_1: 16.59  loss_ce_2: 1.074  loss_texts_2: 8.2  loss_ctrl_points_2: 16.52  loss_bd_points_2: 16.55  loss_ce_3: 1.072  loss_texts_3: 7.958  loss_ctrl_points_3: 16.52  loss_bd_points_3: 16.58  loss_ce_4: 1.22  loss_texts_4: 8.016  loss_ctrl_points_4: 16.49  loss_bd_points_4: 16.54  loss_ce_enc: 1.055  loss_bezier_enc: 0.5691  total_loss: 256.8    time: 7.2985  last_time: 6.4117  data_time: 0.0074  last_data_time: 0.0034   lr: 3.057e-07  max_mem: 6977M
[04/29 19:00:08] d2.utils.events INFO:  eta: 2 days, 11:44:15  iter: 1019  loss_ce: 1.142  loss_texts: 8.316  loss_ctrl_points: 16.51  loss_bd_points: 16.56  loss_ce_0: 1.089  loss_texts_0: 8.635  loss_ctrl_points_0: 16.52  loss_bd_points_0: 16.57  loss_ce_1: 1.003  loss_texts_1: 7.653  loss_ctrl_points_1: 16.55  loss_bd_points_1: 16.59  loss_ce_2: 1.074  loss_texts_2: 8.2  loss_ctrl_points_2: 16.52  loss_bd_points_2: 16.55  loss_ce_3: 1.072  loss_texts_3: 7.958  loss_ctrl_points_3: 16.52  loss_bd_points_3: 16.58  loss_ce_4: 1.22  loss_texts_4: 8.016  loss_ctrl_points_4: 16.49  loss_bd_points_4: 16.54  loss_ce_enc: 1.055  loss_bezier_enc: 0.5691  total_loss: 256.8    time: 7.2767  last_time: 6.5123  data_time: 0.0074  last_data_time: 0.0034   lr: 3.1369e-07  max_mem: 6977M
[04/29 19:02:38] d2.utils.events INFO:  eta: 2 days, 12:20:59  iter: 1039  loss_ce: 1.144  loss_texts: 8.034  loss_ctrl_points: 16.08  loss_bd_points: 16.09  loss_ce_0: 1.092  loss_texts_0: 8.414  loss_ctrl_points_0: 16.13  loss_bd_points_0: 16.15  loss_ce_1: 1.002  loss_texts_1: 7.48  loss_ctrl_points_1: 16.15  loss_bd_points_1: 16.16  loss_ce_2: 1.078  loss_texts_2: 8.012  loss_ctrl_points_2: 16.14  loss_bd_points_2: 16.15  loss_ce_3: 1.074  loss_texts_3: 7.786  loss_ctrl_points_3: 16.08  loss_bd_points_3: 16.1  loss_ce_4: 1.213  loss_texts_4: 7.799  loss_ctrl_points_4: 16.09  loss_bd_points_4: 16.1  loss_ce_enc: 1.05  loss_bezier_enc: 0.5458  total_loss: 252.2    time: 7.3955  last_time: 7.9572  data_time: 0.0035  last_data_time: 0.0035   lr: 6.2538e-07  max_mem: 6977M
[04/29 19:02:39] d2.utils.events INFO:  eta: 2 days, 12:22:12  iter: 1039  loss_ce: 1.144  loss_texts: 8.034  loss_ctrl_points: 16.08  loss_bd_points: 16.09  loss_ce_0: 1.092  loss_texts_0: 8.414  loss_ctrl_points_0: 16.13  loss_bd_points_0: 16.15  loss_ce_1: 1.002  loss_texts_1: 7.48  loss_ctrl_points_1: 16.15  loss_bd_points_1: 16.16  loss_ce_2: 1.078  loss_texts_2: 8.012  loss_ctrl_points_2: 16.14  loss_bd_points_2: 16.15  loss_ce_3: 1.074  loss_texts_3: 7.786  loss_ctrl_points_3: 16.08  loss_bd_points_3: 16.1  loss_ce_4: 1.213  loss_texts_4: 7.799  loss_ctrl_points_4: 16.09  loss_bd_points_4: 16.1  loss_ce_enc: 1.05  loss_bezier_enc: 0.5458  total_loss: 252.2    time: 7.4048  last_time: 8.0983  data_time: 0.0035  last_data_time: 0.0035   lr: 6.3337e-07  max_mem: 6977M
[04/29 19:05:06] d2.utils.events INFO:  eta: 2 days, 11:46:30  iter: 1059  loss_ce: 1.152  loss_texts: 8.185  loss_ctrl_points: 16.54  loss_bd_points: 16.56  loss_ce_0: 1.097  loss_texts_0: 8.49  loss_ctrl_points_0: 16.56  loss_bd_points_0: 16.58  loss_ce_1: 1.005  loss_texts_1: 7.482  loss_ctrl_points_1: 16.6  loss_bd_points_1: 16.61  loss_ce_2: 1.073  loss_texts_2: 8.113  loss_ctrl_points_2: 16.55  loss_bd_points_2: 16.57  loss_ce_3: 1.069  loss_texts_3: 7.864  loss_ctrl_points_3: 16.55  loss_bd_points_3: 16.57  loss_ce_4: 1.232  loss_texts_4: 7.932  loss_ctrl_points_4: 16.53  loss_bd_points_4: 16.55  loss_ce_enc: 1.052  loss_bezier_enc: 0.5832  total_loss: 257.5    time: 7.3836  last_time: 8.0955  data_time: 0.0037  last_data_time: 0.0041   lr: 9.4506e-07  max_mem: 6977M
[04/29 19:05:06] d2.utils.events INFO:  eta: 2 days, 11:59:41  iter: 1059  loss_ce: 1.152  loss_texts: 8.185  loss_ctrl_points: 16.54  loss_bd_points: 16.56  loss_ce_0: 1.097  loss_texts_0: 8.49  loss_ctrl_points_0: 16.56  loss_bd_points_0: 16.58  loss_ce_1: 1.005  loss_texts_1: 7.482  loss_ctrl_points_1: 16.6  loss_bd_points_1: 16.61  loss_ce_2: 1.073  loss_texts_2: 8.113  loss_ctrl_points_2: 16.55  loss_bd_points_2: 16.57  loss_ce_3: 1.069  loss_texts_3: 7.864  loss_ctrl_points_3: 16.55  loss_bd_points_3: 16.57  loss_ce_4: 1.232  loss_texts_4: 7.932  loss_ctrl_points_4: 16.53  loss_bd_points_4: 16.55  loss_ce_enc: 1.052  loss_bezier_enc: 0.5832  total_loss: 257.5    time: 7.3901  last_time: 8.1486  data_time: 0.0037  last_data_time: 0.0041   lr: 9.5305e-07  max_mem: 6977M
[04/29 19:07:36] d2.utils.events INFO:  eta: 2 days, 12:07:30  iter: 1079  loss_ce: 1.157  loss_texts: 8.2  loss_ctrl_points: 16.73  loss_bd_points: 16.77  loss_ce_0: 1.1  loss_texts_0: 8.502  loss_ctrl_points_0: 16.77  loss_bd_points_0: 16.8  loss_ce_1: 1.001  loss_texts_1: 7.618  loss_ctrl_points_1: 16.77  loss_bd_points_1: 16.8  loss_ce_2: 1.073  loss_texts_2: 8.111  loss_ctrl_points_2: 16.74  loss_bd_points_2: 16.78  loss_ce_3: 1.073  loss_texts_3: 7.849  loss_ctrl_points_3: 16.75  loss_bd_points_3: 16.78  loss_ce_4: 1.232  loss_texts_4: 7.96  loss_ctrl_points_4: 16.74  loss_bd_points_4: 16.77  loss_ce_enc: 1.051  loss_bezier_enc: 0.5596  total_loss: 256.6    time: 7.4229  last_time: 7.3276  data_time: 0.0036  last_data_time: 0.0035   lr: 1.2647e-06  max_mem: 6977M
[04/29 19:07:36] d2.utils.events INFO:  eta: 2 days, 11:57:48  iter: 1079  loss_ce: 1.157  loss_texts: 8.2  loss_ctrl_points: 16.73  loss_bd_points: 16.77  loss_ce_0: 1.1  loss_texts_0: 8.502  loss_ctrl_points_0: 16.77  loss_bd_points_0: 16.8  loss_ce_1: 1.001  loss_texts_1: 7.618  loss_ctrl_points_1: 16.77  loss_bd_points_1: 16.8  loss_ce_2: 1.073  loss_texts_2: 8.111  loss_ctrl_points_2: 16.74  loss_bd_points_2: 16.78  loss_ce_3: 1.073  loss_texts_3: 7.849  loss_ctrl_points_3: 16.75  loss_bd_points_3: 16.78  loss_ce_4: 1.232  loss_texts_4: 7.96  loss_ctrl_points_4: 16.74  loss_bd_points_4: 16.77  loss_ce_enc: 1.051  loss_bezier_enc: 0.5596  total_loss: 256.6    time: 7.4227  last_time: 7.3833  data_time: 0.0036  last_data_time: 0.0035   lr: 1.2727e-06  max_mem: 6977M
[04/29 19:10:04] d2.utils.events INFO:  eta: 2 days, 11:45:24  iter: 1099  loss_ce: 1.151  loss_texts: 7.868  loss_ctrl_points: 16.64  loss_bd_points: 16.71  loss_ce_0: 1.099  loss_texts_0: 8.316  loss_ctrl_points_0: 16.67  loss_bd_points_0: 16.73  loss_ce_1: 1.003  loss_texts_1: 7.4  loss_ctrl_points_1: 16.67  loss_bd_points_1: 16.73  loss_ce_2: 1.075  loss_texts_2: 7.921  loss_ctrl_points_2: 16.65  loss_bd_points_2: 16.72  loss_ce_3: 1.07  loss_texts_3: 7.714  loss_ctrl_points_3: 16.65  loss_bd_points_3: 16.72  loss_ce_4: 1.228  loss_texts_4: 7.697  loss_ctrl_points_4: 16.64  loss_bd_points_4: 16.71  loss_ce_enc: 1.053  loss_bezier_enc: 0.5578  total_loss: 258.1    time: 7.4146  last_time: 6.6810  data_time: 0.0036  last_data_time: 0.0031   lr: 1.5844e-06  max_mem: 6977M
[04/29 19:10:04] d2.utils.events INFO:  eta: 2 days, 11:44:03  iter: 1099  loss_ce: 1.151  loss_texts: 7.868  loss_ctrl_points: 16.64  loss_bd_points: 16.71  loss_ce_0: 1.099  loss_texts_0: 8.316  loss_ctrl_points_0: 16.67  loss_bd_points_0: 16.73  loss_ce_1: 1.003  loss_texts_1: 7.4  loss_ctrl_points_1: 16.67  loss_bd_points_1: 16.73  loss_ce_2: 1.075  loss_texts_2: 7.921  loss_ctrl_points_2: 16.65  loss_bd_points_2: 16.72  loss_ce_3: 1.07  loss_texts_3: 7.714  loss_ctrl_points_3: 16.65  loss_bd_points_3: 16.72  loss_ce_4: 1.228  loss_texts_4: 7.697  loss_ctrl_points_4: 16.64  loss_bd_points_4: 16.71  loss_ce_enc: 1.053  loss_bezier_enc: 0.5578  total_loss: 258.1    time: 7.4113  last_time: 6.7717  data_time: 0.0036  last_data_time: 0.0031   lr: 1.5924e-06  max_mem: 6977M
[04/29 19:12:34] d2.utils.events INFO:  eta: 2 days, 11:48:55  iter: 1119  loss_ce: 1.137  loss_texts: 8.116  loss_ctrl_points: 16.07  loss_bd_points: 16.09  loss_ce_0: 1.09  loss_texts_0: 8.448  loss_ctrl_points_0: 16.12  loss_bd_points_0: 16.14  loss_ce_1: 1.003  loss_texts_1: 7.513  loss_ctrl_points_1: 16.11  loss_bd_points_1: 16.13  loss_ce_2: 1.074  loss_texts_2: 8.102  loss_ctrl_points_2: 16.08  loss_bd_points_2: 16.1  loss_ce_3: 1.071  loss_texts_3: 7.79  loss_ctrl_points_3: 16.09  loss_bd_points_3: 16.11  loss_ce_4: 1.216  loss_texts_4: 7.892  loss_ctrl_points_4: 16.07  loss_bd_points_4: 16.09  loss_ce_enc: 1.053  loss_bezier_enc: 0.5914  total_loss: 252.4    time: 7.4279  last_time: 7.4551  data_time: 0.0036  last_data_time: 0.0035   lr: 1.9041e-06  max_mem: 6977M
[04/29 19:12:34] d2.utils.events INFO:  eta: 2 days, 11:49:01  iter: 1119  loss_ce: 1.137  loss_texts: 8.116  loss_ctrl_points: 16.07  loss_bd_points: 16.09  loss_ce_0: 1.09  loss_texts_0: 8.448  loss_ctrl_points_0: 16.12  loss_bd_points_0: 16.14  loss_ce_1: 1.003  loss_texts_1: 7.513  loss_ctrl_points_1: 16.11  loss_bd_points_1: 16.13  loss_ce_2: 1.074  loss_texts_2: 8.102  loss_ctrl_points_2: 16.08  loss_bd_points_2: 16.1  loss_ce_3: 1.071  loss_texts_3: 7.79  loss_ctrl_points_3: 16.09  loss_bd_points_3: 16.11  loss_ce_4: 1.216  loss_texts_4: 7.892  loss_ctrl_points_4: 16.07  loss_bd_points_4: 16.09  loss_ce_enc: 1.053  loss_bezier_enc: 0.5914  total_loss: 252.4    time: 7.4283  last_time: 7.5293  data_time: 0.0036  last_data_time: 0.0035   lr: 1.9121e-06  max_mem: 6977M
[04/29 19:15:02] d2.utils.events INFO:  eta: 2 days, 11:56:22  iter: 1139  loss_ce: 1.154  loss_texts: 8.331  loss_ctrl_points: 16.47  loss_bd_points: 16.5  loss_ce_0: 1.098  loss_texts_0: 8.59  loss_ctrl_points_0: 16.51  loss_bd_points_0: 16.54  loss_ce_1: 1.007  loss_texts_1: 7.714  loss_ctrl_points_1: 16.51  loss_bd_points_1: 16.54  loss_ce_2: 1.078  loss_texts_2: 8.243  loss_ctrl_points_2: 16.49  loss_bd_points_2: 16.52  loss_ce_3: 1.075  loss_texts_3: 7.901  loss_ctrl_points_3: 16.5  loss_bd_points_3: 16.52  loss_ce_4: 1.226  loss_texts_4: 8.02  loss_ctrl_points_4: 16.47  loss_bd_points_4: 16.5  loss_ce_enc: 1.048  loss_bezier_enc: 0.5565  total_loss: 256.1    time: 7.4248  last_time: 7.5338  data_time: 0.0036  last_data_time: 0.0033   lr: 2e-06  max_mem: 6977M
[04/29 19:15:02] d2.utils.events INFO:  eta: 2 days, 11:56:25  iter: 1139  loss_ce: 1.154  loss_texts: 8.331  loss_ctrl_points: 16.47  loss_bd_points: 16.5  loss_ce_0: 1.098  loss_texts_0: 8.59  loss_ctrl_points_0: 16.51  loss_bd_points_0: 16.54  loss_ce_1: 1.007  loss_texts_1: 7.714  loss_ctrl_points_1: 16.51  loss_bd_points_1: 16.54  loss_ce_2: 1.078  loss_texts_2: 8.243  loss_ctrl_points_2: 16.49  loss_bd_points_2: 16.52  loss_ce_3: 1.075  loss_texts_3: 7.901  loss_ctrl_points_3: 16.5  loss_bd_points_3: 16.52  loss_ce_4: 1.226  loss_texts_4: 8.02  loss_ctrl_points_4: 16.47  loss_bd_points_4: 16.5  loss_ce_enc: 1.048  loss_bezier_enc: 0.5565  total_loss: 256.1    time: 7.4254  last_time: 7.6000  data_time: 0.0036  last_data_time: 0.0033   lr: 2e-06  max_mem: 6977M
[04/29 19:17:30] d2.utils.events INFO:  eta: 2 days, 11:59:19  iter: 1159  loss_ce: 1.152  loss_texts: 8.27  loss_ctrl_points: 17.43  loss_bd_points: 17.51  loss_ce_0: 1.098  loss_texts_0: 8.573  loss_ctrl_points_0: 17.45  loss_bd_points_0: 17.52  loss_ce_1: 1.009  loss_texts_1: 7.499  loss_ctrl_points_1: 17.46  loss_bd_points_1: 17.53  loss_ce_2: 1.084  loss_texts_2: 8.168  loss_ctrl_points_2: 17.42  loss_bd_points_2: 17.5  loss_ce_3: 1.077  loss_texts_3: 7.79  loss_ctrl_points_3: 17.44  loss_bd_points_3: 17.52  loss_ce_4: 1.229  loss_texts_4: 7.967  loss_ctrl_points_4: 17.43  loss_bd_points_4: 17.51  loss_ce_enc: 1.056  loss_bezier_enc: 0.5413  total_loss: 264.4    time: 7.4214  last_time: 7.8115  data_time: 0.0036  last_data_time: 0.0037   lr: 2e-06  max_mem: 6977M
[04/29 19:17:31] d2.utils.events INFO:  eta: 2 days, 11:59:24  iter: 1159  loss_ce: 1.152  loss_texts: 8.27  loss_ctrl_points: 17.43  loss_bd_points: 17.51  loss_ce_0: 1.098  loss_texts_0: 8.573  loss_ctrl_points_0: 17.45  loss_bd_points_0: 17.52  loss_ce_1: 1.009  loss_texts_1: 7.499  loss_ctrl_points_1: 17.46  loss_bd_points_1: 17.53  loss_ce_2: 1.084  loss_texts_2: 8.168  loss_ctrl_points_2: 17.42  loss_bd_points_2: 17.5  loss_ce_3: 1.077  loss_texts_3: 7.79  loss_ctrl_points_3: 17.44  loss_bd_points_3: 17.52  loss_ce_4: 1.229  loss_texts_4: 7.967  loss_ctrl_points_4: 17.43  loss_bd_points_4: 17.51  loss_ce_enc: 1.056  loss_bezier_enc: 0.5413  total_loss: 264.4    time: 7.4229  last_time: 7.8816  data_time: 0.0036  last_data_time: 0.0037   lr: 2e-06  max_mem: 6977M
[04/29 19:19:58] d2.utils.events INFO:  eta: 2 days, 11:51:30  iter: 1179  loss_ce: 1.155  loss_texts: 8.254  loss_ctrl_points: 16.17  loss_bd_points: 16.21  loss_ce_0: 1.096  loss_texts_0: 8.655  loss_ctrl_points_0: 16.18  loss_bd_points_0: 16.22  loss_ce_1: 1.007  loss_texts_1: 7.709  loss_ctrl_points_1: 16.18  loss_bd_points_1: 16.22  loss_ce_2: 1.079  loss_texts_2: 8.211  loss_ctrl_points_2: 16.17  loss_bd_points_2: 16.21  loss_ce_3: 1.077  loss_texts_3: 8.104  loss_ctrl_points_3: 16.16  loss_bd_points_3: 16.2  loss_ce_4: 1.225  loss_texts_4: 8.01  loss_ctrl_points_4: 16.16  loss_bd_points_4: 16.2  loss_ce_enc: 1.052  loss_bezier_enc: 0.5666  total_loss: 251.1    time: 7.4214  last_time: 6.5920  data_time: 0.0035  last_data_time: 0.0033   lr: 2e-06  max_mem: 6977M
[04/29 19:19:58] d2.utils.events INFO:  eta: 2 days, 11:51:26  iter: 1179  loss_ce: 1.155  loss_texts: 8.254  loss_ctrl_points: 16.17  loss_bd_points: 16.21  loss_ce_0: 1.096  loss_texts_0: 8.655  loss_ctrl_points_0: 16.18  loss_bd_points_0: 16.22  loss_ce_1: 1.007  loss_texts_1: 7.709  loss_ctrl_points_1: 16.18  loss_bd_points_1: 16.22  loss_ce_2: 1.079  loss_texts_2: 8.211  loss_ctrl_points_2: 16.17  loss_bd_points_2: 16.21  loss_ce_3: 1.077  loss_texts_3: 8.104  loss_ctrl_points_3: 16.16  loss_bd_points_3: 16.2  loss_ce_4: 1.225  loss_texts_4: 8.01  loss_ctrl_points_4: 16.16  loss_bd_points_4: 16.2  loss_ce_enc: 1.052  loss_bezier_enc: 0.5666  total_loss: 251.1    time: 7.4192  last_time: 6.6481  data_time: 0.0035  last_data_time: 0.0033   lr: 2e-06  max_mem: 6977M
[04/29 19:22:33] d2.utils.events INFO:  eta: 2 days, 11:54:20  iter: 1199  loss_ce: 1.152  loss_texts: 8.086  loss_ctrl_points: 16.37  loss_bd_points: 16.39  loss_ce_0: 1.096  loss_texts_0: 8.321  loss_ctrl_points_0: 16.41  loss_bd_points_0: 16.42  loss_ce_1: 1.007  loss_texts_1: 7.457  loss_ctrl_points_1: 16.42  loss_bd_points_1: 16.43  loss_ce_2: 1.078  loss_texts_2: 7.898  loss_ctrl_points_2: 16.38  loss_bd_points_2: 16.4  loss_ce_3: 1.075  loss_texts_3: 7.736  loss_ctrl_points_3: 16.42  loss_bd_points_3: 16.43  loss_ce_4: 1.23  loss_texts_4: 7.858  loss_ctrl_points_4: 16.38  loss_bd_points_4: 16.39  loss_ce_enc: 1.05  loss_bezier_enc: 0.5604  total_loss: 254.3    time: 7.4462  last_time: 8.8801  data_time: 0.0037  last_data_time: 0.0036   lr: 2e-06  max_mem: 6977M
[04/29 19:22:33] d2.utils.events INFO:  eta: 2 days, 11:54:24  iter: 1199  loss_ce: 1.152  loss_texts: 8.086  loss_ctrl_points: 16.37  loss_bd_points: 16.39  loss_ce_0: 1.096  loss_texts_0: 8.321  loss_ctrl_points_0: 16.41  loss_bd_points_0: 16.42  loss_ce_1: 1.007  loss_texts_1: 7.457  loss_ctrl_points_1: 16.42  loss_bd_points_1: 16.43  loss_ce_2: 1.078  loss_texts_2: 7.898  loss_ctrl_points_2: 16.38  loss_bd_points_2: 16.4  loss_ce_3: 1.075  loss_texts_3: 7.736  loss_ctrl_points_3: 16.42  loss_bd_points_3: 16.43  loss_ce_4: 1.23  loss_texts_4: 7.858  loss_ctrl_points_4: 16.38  loss_bd_points_4: 16.39  loss_ce_enc: 1.05  loss_bezier_enc: 0.5604  total_loss: 254.3    time: 7.4501  last_time: 8.9862  data_time: 0.0037  last_data_time: 0.0036   lr: 2e-06  max_mem: 6977M
[04/29 19:25:19] d2.utils.events INFO:  eta: 2 days, 12:05:53  iter: 1219  loss_ce: 1.147  loss_texts: 7.955  loss_ctrl_points: 16.46  loss_bd_points: 16.51  loss_ce_0: 1.092  loss_texts_0: 8.485  loss_ctrl_points_0: 16.5  loss_bd_points_0: 16.56  loss_ce_1: 1.001  loss_texts_1: 7.418  loss_ctrl_points_1: 16.49  loss_bd_points_1: 16.55  loss_ce_2: 1.075  loss_texts_2: 8.058  loss_ctrl_points_2: 16.48  loss_bd_points_2: 16.53  loss_ce_3: 1.068  loss_texts_3: 7.758  loss_ctrl_points_3: 16.48  loss_bd_points_3: 16.51  loss_ce_4: 1.23  loss_texts_4: 7.865  loss_ctrl_points_4: 16.46  loss_bd_points_4: 16.5  loss_ce_enc: 1.058  loss_bezier_enc: 0.5817  total_loss: 255.3    time: 7.5281  last_time: 8.1361  data_time: 0.0038  last_data_time: 0.0042   lr: 2e-06  max_mem: 6977M
[04/29 19:25:19] d2.utils.events INFO:  eta: 2 days, 12:05:59  iter: 1219  loss_ce: 1.147  loss_texts: 7.955  loss_ctrl_points: 16.46  loss_bd_points: 16.51  loss_ce_0: 1.092  loss_texts_0: 8.485  loss_ctrl_points_0: 16.5  loss_bd_points_0: 16.56  loss_ce_1: 1.001  loss_texts_1: 7.418  loss_ctrl_points_1: 16.49  loss_bd_points_1: 16.55  loss_ce_2: 1.075  loss_texts_2: 8.058  loss_ctrl_points_2: 16.48  loss_bd_points_2: 16.53  loss_ce_3: 1.068  loss_texts_3: 7.758  loss_ctrl_points_3: 16.48  loss_bd_points_3: 16.51  loss_ce_4: 1.23  loss_texts_4: 7.865  loss_ctrl_points_4: 16.46  loss_bd_points_4: 16.5  loss_ce_enc: 1.058  loss_bezier_enc: 0.5817  total_loss: 255.3    time: 7.5297  last_time: 8.2106  data_time: 0.0038  last_data_time: 0.0042   lr: 2e-06  max_mem: 6977M
[04/29 19:28:04] d2.utils.events INFO:  eta: 2 days, 12:17:17  iter: 1239  loss_ce: 1.155  loss_texts: 8.149  loss_ctrl_points: 16.57  loss_bd_points: 16.58  loss_ce_0: 1.096  loss_texts_0: 8.557  loss_ctrl_points_0: 16.59  loss_bd_points_0: 16.61  loss_ce_1: 1.007  loss_texts_1: 7.608  loss_ctrl_points_1: 16.59  loss_bd_points_1: 16.61  loss_ce_2: 1.077  loss_texts_2: 8.161  loss_ctrl_points_2: 16.58  loss_bd_points_2: 16.6  loss_ce_3: 1.071  loss_texts_3: 7.983  loss_ctrl_points_3: 16.58  loss_bd_points_3: 16.59  loss_ce_4: 1.223  loss_texts_4: 8.006  loss_ctrl_points_4: 16.56  loss_bd_points_4: 16.58  loss_ce_enc: 1.051  loss_bezier_enc: 0.5616  total_loss: 255.7    time: 7.5872  last_time: 7.9848  data_time: 0.0038  last_data_time: 0.0035   lr: 2e-06  max_mem: 6977M
[04/29 19:28:04] d2.utils.events INFO:  eta: 2 days, 12:17:20  iter: 1239  loss_ce: 1.155  loss_texts: 8.149  loss_ctrl_points: 16.57  loss_bd_points: 16.58  loss_ce_0: 1.096  loss_texts_0: 8.557  loss_ctrl_points_0: 16.59  loss_bd_points_0: 16.61  loss_ce_1: 1.007  loss_texts_1: 7.608  loss_ctrl_points_1: 16.59  loss_bd_points_1: 16.61  loss_ce_2: 1.077  loss_texts_2: 8.161  loss_ctrl_points_2: 16.58  loss_bd_points_2: 16.6  loss_ce_3: 1.071  loss_texts_3: 7.983  loss_ctrl_points_3: 16.58  loss_bd_points_3: 16.59  loss_ce_4: 1.223  loss_texts_4: 8.006  loss_ctrl_points_4: 16.56  loss_bd_points_4: 16.58  loss_ce_enc: 1.051  loss_bezier_enc: 0.5616  total_loss: 255.7    time: 7.5882  last_time: 8.0540  data_time: 0.0038  last_data_time: 0.0035   lr: 2e-06  max_mem: 6977M
[04/29 19:30:46] d2.utils.events INFO:  eta: 2 days, 12:28:59  iter: 1259  loss_ce: 1.156  loss_texts: 8.124  loss_ctrl_points: 16.03  loss_bd_points: 16.06  loss_ce_0: 1.095  loss_texts_0: 8.518  loss_ctrl_points_0: 16.07  loss_bd_points_0: 16.09  loss_ce_1: 1  loss_texts_1: 7.607  loss_ctrl_points_1: 16.08  loss_bd_points_1: 16.11  loss_ce_2: 1.074  loss_texts_2: 8.137  loss_ctrl_points_2: 16.04  loss_bd_points_2: 16.07  loss_ce_3: 1.07  loss_texts_3: 7.85  loss_ctrl_points_3: 16.03  loss_bd_points_3: 16.06  loss_ce_4: 1.23  loss_texts_4: 7.969  loss_ctrl_points_4: 16.02  loss_bd_points_4: 16.05  loss_ce_enc: 1.049  loss_bezier_enc: 0.5571  total_loss: 251.4    time: 7.6236  last_time: 9.1502  data_time: 0.0038  last_data_time: 0.0038   lr: 2e-06  max_mem: 6977M
[04/29 19:30:46] d2.utils.events INFO:  eta: 2 days, 12:29:05  iter: 1259  loss_ce: 1.156  loss_texts: 8.124  loss_ctrl_points: 16.03  loss_bd_points: 16.06  loss_ce_0: 1.095  loss_texts_0: 8.518  loss_ctrl_points_0: 16.07  loss_bd_points_0: 16.09  loss_ce_1: 1  loss_texts_1: 7.607  loss_ctrl_points_1: 16.08  loss_bd_points_1: 16.11  loss_ce_2: 1.074  loss_texts_2: 8.137  loss_ctrl_points_2: 16.04  loss_bd_points_2: 16.07  loss_ce_3: 1.07  loss_texts_3: 7.85  loss_ctrl_points_3: 16.03  loss_bd_points_3: 16.06  loss_ce_4: 1.23  loss_texts_4: 7.969  loss_ctrl_points_4: 16.02  loss_bd_points_4: 16.05  loss_ce_enc: 1.049  loss_bezier_enc: 0.5571  total_loss: 251.4    time: 7.6267  last_time: 9.2074  data_time: 0.0038  last_data_time: 0.0038   lr: 2e-06  max_mem: 6977M
[04/29 19:33:27] d2.utils.events INFO:  eta: 2 days, 12:38:57  iter: 1279  loss_ce: 1.147  loss_texts: 8.136  loss_ctrl_points: 16.54  loss_bd_points: 16.58  loss_ce_0: 1.097  loss_texts_0: 8.501  loss_ctrl_points_0: 16.56  loss_bd_points_0: 16.6  loss_ce_1: 1.003  loss_texts_1: 7.646  loss_ctrl_points_1: 16.59  loss_bd_points_1: 16.63  loss_ce_2: 1.075  loss_texts_2: 8.104  loss_ctrl_points_2: 16.55  loss_bd_points_2: 16.58  loss_ce_3: 1.07  loss_texts_3: 7.842  loss_ctrl_points_3: 16.55  loss_bd_points_3: 16.58  loss_ce_4: 1.228  loss_texts_4: 7.9  loss_ctrl_points_4: 16.53  loss_bd_points_4: 16.57  loss_ce_enc: 1.052  loss_bezier_enc: 0.5653  total_loss: 257.6    time: 7.6581  last_time: 7.9582  data_time: 0.0038  last_data_time: 0.0042   lr: 2e-06  max_mem: 6977M
[04/29 19:33:27] d2.utils.events INFO:  eta: 2 days, 12:39:18  iter: 1279  loss_ce: 1.147  loss_texts: 8.136  loss_ctrl_points: 16.54  loss_bd_points: 16.58  loss_ce_0: 1.097  loss_texts_0: 8.501  loss_ctrl_points_0: 16.56  loss_bd_points_0: 16.6  loss_ce_1: 1.003  loss_texts_1: 7.646  loss_ctrl_points_1: 16.59  loss_bd_points_1: 16.63  loss_ce_2: 1.075  loss_texts_2: 8.104  loss_ctrl_points_2: 16.55  loss_bd_points_2: 16.58  loss_ce_3: 1.07  loss_texts_3: 7.842  loss_ctrl_points_3: 16.55  loss_bd_points_3: 16.58  loss_ce_4: 1.228  loss_texts_4: 7.9  loss_ctrl_points_4: 16.53  loss_bd_points_4: 16.57  loss_ce_enc: 1.052  loss_bezier_enc: 0.5653  total_loss: 257.6    time: 7.6587  last_time: 8.0330  data_time: 0.0038  last_data_time: 0.0042   lr: 2e-06  max_mem: 6977M
[04/29 19:36:12] d2.utils.events INFO:  eta: 2 days, 12:52:50  iter: 1299  loss_ce: 1.149  loss_texts: 8.175  loss_ctrl_points: 17.06  loss_bd_points: 17.07  loss_ce_0: 1.094  loss_texts_0: 8.403  loss_ctrl_points_0: 17.1  loss_bd_points_0: 17.11  loss_ce_1: 1.005  loss_texts_1: 7.554  loss_ctrl_points_1: 17.1  loss_bd_points_1: 17.11  loss_ce_2: 1.081  loss_texts_2: 8.158  loss_ctrl_points_2: 17.09  loss_bd_points_2: 17.1  loss_ce_3: 1.074  loss_texts_3: 7.824  loss_ctrl_points_3: 17.07  loss_bd_points_3: 17.07  loss_ce_4: 1.228  loss_texts_4: 7.875  loss_ctrl_points_4: 17.07  loss_bd_points_4: 17.07  loss_ce_enc: 1.054  loss_bezier_enc: 0.5558  total_loss: 262.9    time: 7.6956  last_time: 8.5002  data_time: 0.0038  last_data_time: 0.0038   lr: 2e-06  max_mem: 6977M
[04/29 19:36:12] d2.utils.events INFO:  eta: 2 days, 12:53:31  iter: 1299  loss_ce: 1.149  loss_texts: 8.175  loss_ctrl_points: 17.06  loss_bd_points: 17.07  loss_ce_0: 1.094  loss_texts_0: 8.403  loss_ctrl_points_0: 17.1  loss_bd_points_0: 17.11  loss_ce_1: 1.005  loss_texts_1: 7.554  loss_ctrl_points_1: 17.1  loss_bd_points_1: 17.11  loss_ce_2: 1.081  loss_texts_2: 8.158  loss_ctrl_points_2: 17.09  loss_bd_points_2: 17.1  loss_ce_3: 1.074  loss_texts_3: 7.824  loss_ctrl_points_3: 17.07  loss_bd_points_3: 17.07  loss_ce_4: 1.228  loss_texts_4: 7.875  loss_ctrl_points_4: 17.07  loss_bd_points_4: 17.07  loss_ce_enc: 1.054  loss_bezier_enc: 0.5558  total_loss: 262.9    time: 7.6970  last_time: 8.5521  data_time: 0.0038  last_data_time: 0.0038   lr: 2e-06  max_mem: 6977M
[04/29 19:38:56] d2.utils.events INFO:  eta: 2 days, 13:09:21  iter: 1319  loss_ce: 1.149  loss_texts: 8.166  loss_ctrl_points: 16.11  loss_bd_points: 16.19  loss_ce_0: 1.095  loss_texts_0: 8.574  loss_ctrl_points_0: 16.14  loss_bd_points_0: 16.22  loss_ce_1: 1.007  loss_texts_1: 7.488  loss_ctrl_points_1: 16.14  loss_bd_points_1: 16.22  loss_ce_2: 1.075  loss_texts_2: 8.16  loss_ctrl_points_2: 16.1  loss_bd_points_2: 16.18  loss_ce_3: 1.073  loss_texts_3: 7.865  loss_ctrl_points_3: 16.11  loss_bd_points_3: 16.18  loss_ce_4: 1.226  loss_texts_4: 8.035  loss_ctrl_points_4: 16.1  loss_bd_points_4: 16.17  loss_ce_enc: 1.053  loss_bezier_enc: 0.5548  total_loss: 248.7    time: 7.7275  last_time: 8.7114  data_time: 0.0038  last_data_time: 0.0034   lr: 2e-06  max_mem: 6977M
[04/29 19:38:56] d2.utils.events INFO:  eta: 2 days, 13:09:30  iter: 1319  loss_ce: 1.149  loss_texts: 8.166  loss_ctrl_points: 16.11  loss_bd_points: 16.19  loss_ce_0: 1.095  loss_texts_0: 8.574  loss_ctrl_points_0: 16.14  loss_bd_points_0: 16.22  loss_ce_1: 1.007  loss_texts_1: 7.488  loss_ctrl_points_1: 16.14  loss_bd_points_1: 16.22  loss_ce_2: 1.075  loss_texts_2: 8.16  loss_ctrl_points_2: 16.1  loss_bd_points_2: 16.18  loss_ce_3: 1.073  loss_texts_3: 7.865  loss_ctrl_points_3: 16.11  loss_bd_points_3: 16.18  loss_ce_4: 1.226  loss_texts_4: 8.035  loss_ctrl_points_4: 16.1  loss_bd_points_4: 16.17  loss_ce_enc: 1.053  loss_bezier_enc: 0.5548  total_loss: 248.7    time: 7.7292  last_time: 8.8096  data_time: 0.0038  last_data_time: 0.0034   lr: 2e-06  max_mem: 6977M
[04/29 19:41:41] d2.utils.events INFO:  eta: 2 days, 13:26:44  iter: 1339  loss_ce: 1.154  loss_texts: 8.034  loss_ctrl_points: 16.37  loss_bd_points: 16.4  loss_ce_0: 1.099  loss_texts_0: 8.398  loss_ctrl_points_0: 16.4  loss_bd_points_0: 16.44  loss_ce_1: 1.006  loss_texts_1: 7.508  loss_ctrl_points_1: 16.4  loss_bd_points_1: 16.43  loss_ce_2: 1.076  loss_texts_2: 8.092  loss_ctrl_points_2: 16.37  loss_bd_points_2: 16.4  loss_ce_3: 1.077  loss_texts_3: 7.834  loss_ctrl_points_3: 16.38  loss_bd_points_3: 16.42  loss_ce_4: 1.221  loss_texts_4: 7.85  loss_ctrl_points_4: 16.37  loss_bd_points_4: 16.4  loss_ce_enc: 1.053  loss_bezier_enc: 0.5521  total_loss: 251.7    time: 7.7575  last_time: 8.6304  data_time: 0.0038  last_data_time: 0.0038   lr: 2e-06  max_mem: 6977M
[04/29 19:41:41] d2.utils.events INFO:  eta: 2 days, 13:28:11  iter: 1339  loss_ce: 1.154  loss_texts: 8.034  loss_ctrl_points: 16.37  loss_bd_points: 16.4  loss_ce_0: 1.099  loss_texts_0: 8.398  loss_ctrl_points_0: 16.4  loss_bd_points_0: 16.44  loss_ce_1: 1.006  loss_texts_1: 7.508  loss_ctrl_points_1: 16.4  loss_bd_points_1: 16.43  loss_ce_2: 1.076  loss_texts_2: 8.092  loss_ctrl_points_2: 16.37  loss_bd_points_2: 16.4  loss_ce_3: 1.077  loss_texts_3: 7.834  loss_ctrl_points_3: 16.38  loss_bd_points_3: 16.42  loss_ce_4: 1.221  loss_texts_4: 7.85  loss_ctrl_points_4: 16.37  loss_bd_points_4: 16.4  loss_ce_enc: 1.053  loss_bezier_enc: 0.5521  total_loss: 251.7    time: 7.7590  last_time: 8.7355  data_time: 0.0038  last_data_time: 0.0038   lr: 2e-06  max_mem: 6977M
[04/29 19:44:19] d2.utils.events INFO:  eta: 2 days, 13:35:12  iter: 1359  loss_ce: 1.155  loss_texts: 8.321  loss_ctrl_points: 15.84  loss_bd_points: 15.88  loss_ce_0: 1.1  loss_texts_0: 8.697  loss_ctrl_points_0: 15.88  loss_bd_points_0: 15.93  loss_ce_1: 1.004  loss_texts_1: 7.78  loss_ctrl_points_1: 15.88  loss_bd_points_1: 15.92  loss_ce_2: 1.073  loss_texts_2: 8.278  loss_ctrl_points_2: 15.85  loss_bd_points_2: 15.89  loss_ce_3: 1.075  loss_texts_3: 8.075  loss_ctrl_points_3: 15.85  loss_bd_points_3: 15.88  loss_ce_4: 1.231  loss_texts_4: 8.088  loss_ctrl_points_4: 15.84  loss_bd_points_4: 15.88  loss_ce_enc: 1.049  loss_bezier_enc: 0.6004  total_loss: 248.9    time: 7.7681  last_time: 7.1881  data_time: 0.0038  last_data_time: 0.0041   lr: 2e-06  max_mem: 6977M
[04/29 19:44:19] d2.utils.events INFO:  eta: 2 days, 13:34:23  iter: 1359  loss_ce: 1.155  loss_texts: 8.321  loss_ctrl_points: 15.84  loss_bd_points: 15.88  loss_ce_0: 1.1  loss_texts_0: 8.697  loss_ctrl_points_0: 15.88  loss_bd_points_0: 15.93  loss_ce_1: 1.004  loss_texts_1: 7.78  loss_ctrl_points_1: 15.88  loss_bd_points_1: 15.92  loss_ce_2: 1.073  loss_texts_2: 8.278  loss_ctrl_points_2: 15.85  loss_bd_points_2: 15.89  loss_ce_3: 1.075  loss_texts_3: 8.075  loss_ctrl_points_3: 15.85  loss_bd_points_3: 15.88  loss_ce_4: 1.231  loss_texts_4: 8.088  loss_ctrl_points_4: 15.84  loss_bd_points_4: 15.88  loss_ce_enc: 1.049  loss_bezier_enc: 0.6004  total_loss: 248.9    time: 7.7674  last_time: 7.2402  data_time: 0.0038  last_data_time: 0.0041   lr: 2e-06  max_mem: 6977M
[04/29 19:46:52] d2.utils.events INFO:  eta: 2 days, 13:24:38  iter: 1379  loss_ce: 1.152  loss_texts: 8.046  loss_ctrl_points: 17.43  loss_bd_points: 17.48  loss_ce_0: 1.098  loss_texts_0: 8.39  loss_ctrl_points_0: 17.47  loss_bd_points_0: 17.52  loss_ce_1: 1.005  loss_texts_1: 7.44  loss_ctrl_points_1: 17.48  loss_bd_points_1: 17.52  loss_ce_2: 1.077  loss_texts_2: 8.008  loss_ctrl_points_2: 17.45  loss_bd_points_2: 17.49  loss_ce_3: 1.075  loss_texts_3: 7.721  loss_ctrl_points_3: 17.45  loss_bd_points_3: 17.49  loss_ce_4: 1.232  loss_texts_4: 7.857  loss_ctrl_points_4: 17.44  loss_bd_points_4: 17.48  loss_ce_enc: 1.053  loss_bezier_enc: 0.564  total_loss: 266.2    time: 7.7579  last_time: 8.5819  data_time: 0.0037  last_data_time: 0.0040   lr: 2e-06  max_mem: 6977M
[04/29 19:46:52] d2.utils.events INFO:  eta: 2 days, 13:27:43  iter: 1379  loss_ce: 1.152  loss_texts: 8.046  loss_ctrl_points: 17.43  loss_bd_points: 17.48  loss_ce_0: 1.098  loss_texts_0: 8.39  loss_ctrl_points_0: 17.47  loss_bd_points_0: 17.52  loss_ce_1: 1.005  loss_texts_1: 7.44  loss_ctrl_points_1: 17.48  loss_bd_points_1: 17.52  loss_ce_2: 1.077  loss_texts_2: 8.008  loss_ctrl_points_2: 17.45  loss_bd_points_2: 17.49  loss_ce_3: 1.075  loss_texts_3: 7.721  loss_ctrl_points_3: 17.45  loss_bd_points_3: 17.49  loss_ce_4: 1.232  loss_texts_4: 7.857  loss_ctrl_points_4: 17.44  loss_bd_points_4: 17.48  loss_ce_enc: 1.053  loss_bezier_enc: 0.564  total_loss: 266.2    time: 7.7590  last_time: 8.6368  data_time: 0.0037  last_data_time: 0.0040   lr: 2e-06  max_mem: 6977M
[04/29 19:49:38] d2.utils.events INFO:  eta: 2 days, 13:41:04  iter: 1399  loss_ce: 1.147  loss_texts: 8.211  loss_ctrl_points: 17.14  loss_bd_points: 17.16  loss_ce_0: 1.096  loss_texts_0: 8.585  loss_ctrl_points_0: 17.18  loss_bd_points_0: 17.2  loss_ce_1: 1.007  loss_texts_1: 7.661  loss_ctrl_points_1: 17.18  loss_bd_points_1: 17.19  loss_ce_2: 1.078  loss_texts_2: 8.211  loss_ctrl_points_2: 17.17  loss_bd_points_2: 17.19  loss_ce_3: 1.073  loss_texts_3: 7.951  loss_ctrl_points_3: 17.15  loss_bd_points_3: 17.17  loss_ce_4: 1.226  loss_texts_4: 7.981  loss_ctrl_points_4: 17.14  loss_bd_points_4: 17.17  loss_ce_enc: 1.051  loss_bezier_enc: 0.5312  total_loss: 263.6    time: 7.7858  last_time: 8.3246  data_time: 0.0039  last_data_time: 0.0036   lr: 2e-06  max_mem: 6977M
[04/29 19:49:38] d2.utils.events INFO:  eta: 2 days, 13:41:23  iter: 1399  loss_ce: 1.147  loss_texts: 8.211  loss_ctrl_points: 17.14  loss_bd_points: 17.16  loss_ce_0: 1.096  loss_texts_0: 8.585  loss_ctrl_points_0: 17.18  loss_bd_points_0: 17.2  loss_ce_1: 1.007  loss_texts_1: 7.661  loss_ctrl_points_1: 17.18  loss_bd_points_1: 17.19  loss_ce_2: 1.078  loss_texts_2: 8.211  loss_ctrl_points_2: 17.17  loss_bd_points_2: 17.19  loss_ce_3: 1.073  loss_texts_3: 7.951  loss_ctrl_points_3: 17.15  loss_bd_points_3: 17.17  loss_ce_4: 1.226  loss_texts_4: 7.981  loss_ctrl_points_4: 17.14  loss_bd_points_4: 17.17  loss_ce_enc: 1.051  loss_bezier_enc: 0.5312  total_loss: 263.6    time: 7.7866  last_time: 8.4023  data_time: 0.0039  last_data_time: 0.0036   lr: 2e-06  max_mem: 6977M
[04/29 19:52:23] d2.utils.events INFO:  eta: 2 days, 13:44:37  iter: 1419  loss_ce: 1.144  loss_texts: 8.021  loss_ctrl_points: 16.37  loss_bd_points: 16.42  loss_ce_0: 1.094  loss_texts_0: 8.324  loss_ctrl_points_0: 16.39  loss_bd_points_0: 16.43  loss_ce_1: 1  loss_texts_1: 7.428  loss_ctrl_points_1: 16.42  loss_bd_points_1: 16.46  loss_ce_2: 1.074  loss_texts_2: 7.998  loss_ctrl_points_2: 16.37  loss_bd_points_2: 16.42  loss_ce_3: 1.071  loss_texts_3: 7.69  loss_ctrl_points_3: 16.37  loss_bd_points_3: 16.41  loss_ce_4: 1.223  loss_texts_4: 7.835  loss_ctrl_points_4: 16.36  loss_bd_points_4: 16.41  loss_ce_enc: 1.055  loss_bezier_enc: 0.5898  total_loss: 252.2    time: 7.8081  last_time: 8.3427  data_time: 0.0038  last_data_time: 0.0038   lr: 2e-06  max_mem: 6977M
[04/29 19:52:23] d2.utils.events INFO:  eta: 2 days, 13:44:44  iter: 1419  loss_ce: 1.144  loss_texts: 8.021  loss_ctrl_points: 16.37  loss_bd_points: 16.42  loss_ce_0: 1.094  loss_texts_0: 8.324  loss_ctrl_points_0: 16.39  loss_bd_points_0: 16.43  loss_ce_1: 1  loss_texts_1: 7.428  loss_ctrl_points_1: 16.42  loss_bd_points_1: 16.46  loss_ce_2: 1.074  loss_texts_2: 7.998  loss_ctrl_points_2: 16.37  loss_bd_points_2: 16.42  loss_ce_3: 1.071  loss_texts_3: 7.69  loss_ctrl_points_3: 16.37  loss_bd_points_3: 16.41  loss_ce_4: 1.223  loss_texts_4: 7.835  loss_ctrl_points_4: 16.36  loss_bd_points_4: 16.41  loss_ce_enc: 1.055  loss_bezier_enc: 0.5898  total_loss: 252.2    time: 7.8089  last_time: 8.4097  data_time: 0.0038  last_data_time: 0.0038   lr: 2e-06  max_mem: 6977M
[04/29 19:55:04] d2.utils.events INFO:  eta: 2 days, 13:43:21  iter: 1439  loss_ce: 1.154  loss_texts: 8.031  loss_ctrl_points: 16.04  loss_bd_points: 16.06  loss_ce_0: 1.1  loss_texts_0: 8.346  loss_ctrl_points_0: 16.05  loss_bd_points_0: 16.07  loss_ce_1: 1.007  loss_texts_1: 7.403  loss_ctrl_points_1: 16.04  loss_bd_points_1: 16.06  loss_ce_2: 1.071  loss_texts_2: 7.865  loss_ctrl_points_2: 16.03  loss_bd_points_2: 16.04  loss_ce_3: 1.071  loss_texts_3: 7.698  loss_ctrl_points_3: 16.05  loss_bd_points_3: 16.08  loss_ce_4: 1.225  loss_texts_4: 7.82  loss_ctrl_points_4: 16.01  loss_bd_points_4: 16.04  loss_ce_enc: 1.05  loss_bezier_enc: 0.5848  total_loss: 249.9    time: 7.8172  last_time: 8.7737  data_time: 0.0038  last_data_time: 0.0039   lr: 2e-06  max_mem: 6977M
[04/29 19:55:04] d2.utils.events INFO:  eta: 2 days, 13:44:02  iter: 1439  loss_ce: 1.154  loss_texts: 8.031  loss_ctrl_points: 16.04  loss_bd_points: 16.06  loss_ce_0: 1.1  loss_texts_0: 8.346  loss_ctrl_points_0: 16.05  loss_bd_points_0: 16.07  loss_ce_1: 1.007  loss_texts_1: 7.403  loss_ctrl_points_1: 16.04  loss_bd_points_1: 16.06  loss_ce_2: 1.071  loss_texts_2: 7.865  loss_ctrl_points_2: 16.03  loss_bd_points_2: 16.04  loss_ce_3: 1.071  loss_texts_3: 7.698  loss_ctrl_points_3: 16.05  loss_bd_points_3: 16.08  loss_ce_4: 1.225  loss_texts_4: 7.82  loss_ctrl_points_4: 16.01  loss_bd_points_4: 16.04  loss_ce_enc: 1.05  loss_bezier_enc: 0.5848  total_loss: 249.9    time: 7.8184  last_time: 8.8741  data_time: 0.0038  last_data_time: 0.0039   lr: 2e-06  max_mem: 6977M
[04/29 19:57:48] d2.utils.events INFO:  eta: 2 days, 13:52:09  iter: 1459  loss_ce: 1.15  loss_texts: 8.069  loss_ctrl_points: 17.05  loss_bd_points: 17.1  loss_ce_0: 1.097  loss_texts_0: 8.457  loss_ctrl_points_0: 17.08  loss_bd_points_0: 17.12  loss_ce_1: 1.005  loss_texts_1: 7.456  loss_ctrl_points_1: 17.09  loss_bd_points_1: 17.13  loss_ce_2: 1.076  loss_texts_2: 8.109  loss_ctrl_points_2: 17.06  loss_bd_points_2: 17.11  loss_ce_3: 1.074  loss_texts_3: 7.88  loss_ctrl_points_3: 17.05  loss_bd_points_3: 17.1  loss_ce_4: 1.235  loss_texts_4: 7.885  loss_ctrl_points_4: 17.05  loss_bd_points_4: 17.09  loss_ce_enc: 1.051  loss_bezier_enc: 0.5666  total_loss: 262.2    time: 7.8363  last_time: 8.2113  data_time: 0.0038  last_data_time: 0.0037   lr: 2e-06  max_mem: 6977M
[04/29 19:57:49] d2.utils.events INFO:  eta: 2 days, 13:52:10  iter: 1459  loss_ce: 1.15  loss_texts: 8.069  loss_ctrl_points: 17.05  loss_bd_points: 17.1  loss_ce_0: 1.097  loss_texts_0: 8.457  loss_ctrl_points_0: 17.08  loss_bd_points_0: 17.12  loss_ce_1: 1.005  loss_texts_1: 7.456  loss_ctrl_points_1: 17.09  loss_bd_points_1: 17.13  loss_ce_2: 1.076  loss_texts_2: 8.109  loss_ctrl_points_2: 17.06  loss_bd_points_2: 17.11  loss_ce_3: 1.074  loss_texts_3: 7.88  loss_ctrl_points_3: 17.05  loss_bd_points_3: 17.1  loss_ce_4: 1.235  loss_texts_4: 7.885  loss_ctrl_points_4: 17.05  loss_bd_points_4: 17.09  loss_ce_enc: 1.051  loss_bezier_enc: 0.5666  total_loss: 262.2    time: 7.8368  last_time: 8.2962  data_time: 0.0038  last_data_time: 0.0037   lr: 2e-06  max_mem: 6977M
[04/29 20:00:35] d2.utils.events INFO:  eta: 2 days, 14:08:37  iter: 1479  loss_ce: 1.144  loss_texts: 8.334  loss_ctrl_points: 17.12  loss_bd_points: 17.16  loss_ce_0: 1.093  loss_texts_0: 8.588  loss_ctrl_points_0: 17.16  loss_bd_points_0: 17.21  loss_ce_1: 1.006  loss_texts_1: 7.678  loss_ctrl_points_1: 17.17  loss_bd_points_1: 17.22  loss_ce_2: 1.074  loss_texts_2: 8.118  loss_ctrl_points_2: 17.16  loss_bd_points_2: 17.21  loss_ce_3: 1.077  loss_texts_3: 7.962  loss_ctrl_points_3: 17.15  loss_bd_points_3: 17.2  loss_ce_4: 1.228  loss_texts_4: 8.014  loss_ctrl_points_4: 17.12  loss_bd_points_4: 17.17  loss_ce_enc: 1.054  loss_bezier_enc: 0.5585  total_loss: 261.7    time: 7.8571  last_time: 8.1916  data_time: 0.0038  last_data_time: 0.0037   lr: 2e-06  max_mem: 6977M
[04/29 20:00:35] d2.utils.events INFO:  eta: 2 days, 14:08:44  iter: 1479  loss_ce: 1.144  loss_texts: 8.334  loss_ctrl_points: 17.12  loss_bd_points: 17.16  loss_ce_0: 1.093  loss_texts_0: 8.588  loss_ctrl_points_0: 17.16  loss_bd_points_0: 17.21  loss_ce_1: 1.006  loss_texts_1: 7.678  loss_ctrl_points_1: 17.17  loss_bd_points_1: 17.22  loss_ce_2: 1.074  loss_texts_2: 8.118  loss_ctrl_points_2: 17.16  loss_bd_points_2: 17.21  loss_ce_3: 1.077  loss_texts_3: 7.962  loss_ctrl_points_3: 17.15  loss_bd_points_3: 17.2  loss_ce_4: 1.228  loss_texts_4: 8.014  loss_ctrl_points_4: 17.12  loss_bd_points_4: 17.17  loss_ce_enc: 1.054  loss_bezier_enc: 0.5585  total_loss: 261.7    time: 7.8576  last_time: 8.2615  data_time: 0.0038  last_data_time: 0.0037   lr: 2e-06  max_mem: 6977M
[04/29 20:03:22] d2.utils.events INFO:  eta: 2 days, 14:18:04  iter: 1499  loss_ce: 1.157  loss_texts: 8.206  loss_ctrl_points: 16.43  loss_bd_points: 16.47  loss_ce_0: 1.099  loss_texts_0: 8.505  loss_ctrl_points_0: 16.45  loss_bd_points_0: 16.51  loss_ce_1: 1  loss_texts_1: 7.432  loss_ctrl_points_1: 16.45  loss_bd_points_1: 16.5  loss_ce_2: 1.072  loss_texts_2: 8.168  loss_ctrl_points_2: 16.44  loss_bd_points_2: 16.49  loss_ce_3: 1.072  loss_texts_3: 7.819  loss_ctrl_points_3: 16.43  loss_bd_points_3: 16.48  loss_ce_4: 1.237  loss_texts_4: 7.926  loss_ctrl_points_4: 16.42  loss_bd_points_4: 16.47  loss_ce_enc: 1.05  loss_bezier_enc: 0.5746  total_loss: 253.2    time: 7.8763  last_time: 7.9412  data_time: 0.0038  last_data_time: 0.0037   lr: 2e-06  max_mem: 6977M
[04/29 20:03:22] d2.utils.events INFO:  eta: 2 days, 14:18:10  iter: 1499  loss_ce: 1.157  loss_texts: 8.206  loss_ctrl_points: 16.43  loss_bd_points: 16.47  loss_ce_0: 1.099  loss_texts_0: 8.505  loss_ctrl_points_0: 16.45  loss_bd_points_0: 16.51  loss_ce_1: 1  loss_texts_1: 7.432  loss_ctrl_points_1: 16.45  loss_bd_points_1: 16.5  loss_ce_2: 1.072  loss_texts_2: 8.168  loss_ctrl_points_2: 16.44  loss_bd_points_2: 16.49  loss_ce_3: 1.072  loss_texts_3: 7.819  loss_ctrl_points_3: 16.43  loss_bd_points_3: 16.48  loss_ce_4: 1.237  loss_texts_4: 7.926  loss_ctrl_points_4: 16.42  loss_bd_points_4: 16.47  loss_ce_enc: 1.05  loss_bezier_enc: 0.5746  total_loss: 253.2    time: 7.8764  last_time: 8.0087  data_time: 0.0038  last_data_time: 0.0037   lr: 2e-06  max_mem: 6977M
[04/29 20:06:00] d2.utils.events INFO:  eta: 2 days, 14:30:13  iter: 1519  loss_ce: 1.16  loss_texts: 8  loss_ctrl_points: 15.97  loss_bd_points: 16.02  loss_ce_0: 1.102  loss_texts_0: 8.379  loss_ctrl_points_0: 16.02  loss_bd_points_0: 16.06  loss_ce_1: 1.007  loss_texts_1: 7.491  loss_ctrl_points_1: 16.02  loss_bd_points_1: 16.05  loss_ce_2: 1.075  loss_texts_2: 8.051  loss_ctrl_points_2: 15.99  loss_bd_points_2: 16.03  loss_ce_3: 1.07  loss_texts_3: 7.764  loss_ctrl_points_3: 15.98  loss_bd_points_3: 16.02  loss_ce_4: 1.226  loss_texts_4: 7.818  loss_ctrl_points_4: 15.98  loss_bd_points_4: 16.02  loss_ce_enc: 1.052  loss_bezier_enc: 0.5433  total_loss: 247.6    time: 7.8780  last_time: 7.8321  data_time: 0.0038  last_data_time: 0.0034   lr: 2e-06  max_mem: 6977M
[04/29 20:06:00] d2.utils.events INFO:  eta: 2 days, 14:30:13  iter: 1519  loss_ce: 1.16  loss_texts: 8  loss_ctrl_points: 15.97  loss_bd_points: 16.02  loss_ce_0: 1.102  loss_texts_0: 8.379  loss_ctrl_points_0: 16.02  loss_bd_points_0: 16.06  loss_ce_1: 1.007  loss_texts_1: 7.491  loss_ctrl_points_1: 16.02  loss_bd_points_1: 16.05  loss_ce_2: 1.075  loss_texts_2: 8.051  loss_ctrl_points_2: 15.99  loss_bd_points_2: 16.03  loss_ce_3: 1.07  loss_texts_3: 7.764  loss_ctrl_points_3: 15.98  loss_bd_points_3: 16.02  loss_ce_4: 1.226  loss_texts_4: 7.818  loss_ctrl_points_4: 15.98  loss_bd_points_4: 16.02  loss_ce_enc: 1.052  loss_bezier_enc: 0.5433  total_loss: 247.6    time: 7.8780  last_time: 7.8996  data_time: 0.0038  last_data_time: 0.0034   lr: 2e-06  max_mem: 6977M
[04/29 20:08:26] d2.utils.events INFO:  eta: 2 days, 14:27:11  iter: 1539  loss_ce: 1.162  loss_texts: 8.115  loss_ctrl_points: 15.89  loss_bd_points: 15.91  loss_ce_0: 1.104  loss_texts_0: 8.467  loss_ctrl_points_0: 15.92  loss_bd_points_0: 15.94  loss_ce_1: 1.01  loss_texts_1: 7.494  loss_ctrl_points_1: 15.92  loss_bd_points_1: 15.94  loss_ce_2: 1.079  loss_texts_2: 8.074  loss_ctrl_points_2: 15.94  loss_bd_points_2: 15.96  loss_ce_3: 1.075  loss_texts_3: 7.846  loss_ctrl_points_3: 15.92  loss_bd_points_3: 15.94  loss_ce_4: 1.237  loss_texts_4: 7.872  loss_ctrl_points_4: 15.89  loss_bd_points_4: 15.91  loss_ce_enc: 1.051  loss_bezier_enc: 0.5596  total_loss: 248.9    time: 7.8559  last_time: 8.0843  data_time: 0.0037  last_data_time: 0.0037   lr: 2e-06  max_mem: 6977M
[04/29 20:08:26] d2.utils.events INFO:  eta: 2 days, 14:27:11  iter: 1539  loss_ce: 1.162  loss_texts: 8.115  loss_ctrl_points: 15.89  loss_bd_points: 15.91  loss_ce_0: 1.104  loss_texts_0: 8.467  loss_ctrl_points_0: 15.92  loss_bd_points_0: 15.94  loss_ce_1: 1.01  loss_texts_1: 7.494  loss_ctrl_points_1: 15.92  loss_bd_points_1: 15.94  loss_ce_2: 1.079  loss_texts_2: 8.074  loss_ctrl_points_2: 15.94  loss_bd_points_2: 15.96  loss_ce_3: 1.075  loss_texts_3: 7.846  loss_ctrl_points_3: 15.92  loss_bd_points_3: 15.94  loss_ce_4: 1.237  loss_texts_4: 7.872  loss_ctrl_points_4: 15.89  loss_bd_points_4: 15.91  loss_ce_enc: 1.051  loss_bezier_enc: 0.5596  total_loss: 248.9    time: 7.8562  last_time: 8.1408  data_time: 0.0037  last_data_time: 0.0037   lr: 2e-06  max_mem: 6977M
[04/29 20:10:50] d2.utils.events INFO:  eta: 2 days, 14:23:51  iter: 1559  loss_ce: 1.146  loss_texts: 8.058  loss_ctrl_points: 16.5  loss_bd_points: 16.55  loss_ce_0: 1.095  loss_texts_0: 8.329  loss_ctrl_points_0: 16.53  loss_bd_points_0: 16.58  loss_ce_1: 1  loss_texts_1: 7.35  loss_ctrl_points_1: 16.54  loss_bd_points_1: 16.58  loss_ce_2: 1.075  loss_texts_2: 8.047  loss_ctrl_points_2: 16.51  loss_bd_points_2: 16.55  loss_ce_3: 1.072  loss_texts_3: 7.73  loss_ctrl_points_3: 16.5  loss_bd_points_3: 16.54  loss_ce_4: 1.227  loss_texts_4: 7.79  loss_ctrl_points_4: 16.49  loss_bd_points_4: 16.54  loss_ce_enc: 1.052  loss_bezier_enc: 0.5381  total_loss: 254.1    time: 7.8332  last_time: 6.7130  data_time: 0.0037  last_data_time: 0.0035   lr: 2e-06  max_mem: 6977M
[04/29 20:10:50] d2.utils.events INFO:  eta: 2 days, 14:23:11  iter: 1559  loss_ce: 1.146  loss_texts: 8.058  loss_ctrl_points: 16.5  loss_bd_points: 16.55  loss_ce_0: 1.095  loss_texts_0: 8.329  loss_ctrl_points_0: 16.53  loss_bd_points_0: 16.58  loss_ce_1: 1  loss_texts_1: 7.35  loss_ctrl_points_1: 16.54  loss_bd_points_1: 16.58  loss_ce_2: 1.075  loss_texts_2: 8.047  loss_ctrl_points_2: 16.51  loss_bd_points_2: 16.55  loss_ce_3: 1.072  loss_texts_3: 7.73  loss_ctrl_points_3: 16.5  loss_bd_points_3: 16.54  loss_ce_4: 1.227  loss_texts_4: 7.79  loss_ctrl_points_4: 16.49  loss_bd_points_4: 16.54  loss_ce_enc: 1.052  loss_bezier_enc: 0.5381  total_loss: 254.1    time: 7.8323  last_time: 6.7987  data_time: 0.0037  last_data_time: 0.0035   lr: 2e-06  max_mem: 6977M
[04/29 20:13:15] d2.utils.events INFO:  eta: 2 days, 14:13:25  iter: 1579  loss_ce: 1.145  loss_texts: 8.166  loss_ctrl_points: 16.43  loss_bd_points: 16.48  loss_ce_0: 1.093  loss_texts_0: 8.524  loss_ctrl_points_0: 16.46  loss_bd_points_0: 16.51  loss_ce_1: 1.005  loss_texts_1: 7.598  loss_ctrl_points_1: 16.47  loss_bd_points_1: 16.51  loss_ce_2: 1.076  loss_texts_2: 8.094  loss_ctrl_points_2: 16.44  loss_bd_points_2: 16.49  loss_ce_3: 1.075  loss_texts_3: 7.822  loss_ctrl_points_3: 16.44  loss_bd_points_3: 16.48  loss_ce_4: 1.225  loss_texts_4: 7.871  loss_ctrl_points_4: 16.43  loss_bd_points_4: 16.48  loss_ce_enc: 1.05  loss_bezier_enc: 0.5833  total_loss: 253.7    time: 7.8118  last_time: 6.8064  data_time: 0.0037  last_data_time: 0.0036   lr: 2e-06  max_mem: 6977M
[04/29 20:13:15] d2.utils.events INFO:  eta: 2 days, 14:13:25  iter: 1579  loss_ce: 1.145  loss_texts: 8.166  loss_ctrl_points: 16.43  loss_bd_points: 16.48  loss_ce_0: 1.093  loss_texts_0: 8.524  loss_ctrl_points_0: 16.46  loss_bd_points_0: 16.51  loss_ce_1: 1.005  loss_texts_1: 7.598  loss_ctrl_points_1: 16.47  loss_bd_points_1: 16.51  loss_ce_2: 1.076  loss_texts_2: 8.094  loss_ctrl_points_2: 16.44  loss_bd_points_2: 16.49  loss_ce_3: 1.075  loss_texts_3: 7.822  loss_ctrl_points_3: 16.44  loss_bd_points_3: 16.48  loss_ce_4: 1.225  loss_texts_4: 7.871  loss_ctrl_points_4: 16.43  loss_bd_points_4: 16.48  loss_ce_enc: 1.05  loss_bezier_enc: 0.5833  total_loss: 253.7    time: 7.8111  last_time: 7.0210  data_time: 0.0037  last_data_time: 0.0036   lr: 2e-06  max_mem: 6977M
